---
title: "APM Student Solutions (Ch 3 & 4)"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

## General Strategies

### Exercise 3.1

```{r ex3.1, message=FALSE, warning=FALSE}
require(mlbench)
data(Glass)
str(Glass)

# (a) visualizations
pairs(Glass[,-10])

require(tidyverse)
require(ggplot2)

Glass[, -10] %>% gather() %>% head()  # check
ggplot(gather(Glass[, -10]), aes(value)) +
  geom_histogram(bins = 10) +
  facet_wrap(~key, scales = "free_x")

require(corrplot)
correlations <- cor(Glass[, -10])
correlations
corrplot(correlations, order = "hclust")

# (b) outliers / skewness
# there seems to be an outlier for K and possibly for Fe
require(e1071)
apply(Glass[, -10], 2, skewness)  # definitely skewed

# (c) relevant transformations
# apply box cox to skewed predictors
require(caret)
RITrans <- BoxCoxTrans(Glass$RI)
RITrans
predict(RITrans, head(Glass$RI))

```

### Exercise 3.2

```{r ex3.2, message=FALSE, warning=FALSE}
require(mlbench)
data("Soybean")
str(Soybean)

# (a) category frequencies
require(caret)
nearZeroVar(Soybean, saveMetrics = TRUE)
sapply(Soybean[,nearZeroVar(Soybean)], table)

# (b) missing data patterns
require(mice)
md.pattern(Soybean)  # 562 of 683 have values for all cols
# missing values seem to happen for entire categories, i.e. all leaf cols
md.pattern(Soybean) %>% colnames  # col names from most to least filled out

# (c) handle missing values 
soy_data <- Soybean[, -nearZeroVar(Soybean)]  # drop 3 cols
preProcValues <- preProcess(soy_data, method = c("knnImpute"))  # doesn't work bc of categorical vars?
soy_data_imputed <- mice(soy_data, method = "pmm", seed = 500)
```

### Exercise 3.3

```{r ex3.3, message=FALSE, warning=FALSE}
# (a) category frequencies
require(caret)
data(BloodBrain)

# (b) degenerate distributions
# assume this means dist with near zero variance
require(magrittr)
sapply(bbbDescr[,nearZeroVar(bbbDescr)], table)
sapply(bbbDescr[,nearZeroVar(bbbDescr)], table) %>% names()

# (c) correlations
correlations <- cor(bbbDescr)
highCorr <- findCorrelation(correlations, cutoff = 0.75)
length(highCorr)  # yes, strong correlations
# use PCA or remove highly correlated predictors
# almost half the vars are highly correlated
# yes, will impact number of predictors for modeling
```

### Exercise 4.1

__(a)__ Goal is to classify music into 6 categories and we have 12,495 samples with 191 predictors. The reponse categories are not balanced (mostly classical, metal and blues are rarer). For a larger dataset like this, computational efficiency is more important. Maybe go with a 10-fold CV. Because response variable is not balanced, may need to use stratified random sampling.

__(b)__ 

1. set seed
2. randomly sample training vs test set
3. create folds using stratified random sampling

### Exercise 4.2

__(a)__ 165 samples and 1,107 predictors. The response is highly skewed and predictors are sparse (15.5% are present) and highly correlated. Because sample size is small, should use 10-fold CV. This has good bias and variance properties. 

### Exercise 4.3

__(b)__ 

```{r ex4.3, message=FALSE, warning=FALSE}
mean_r2 <- c(0.444, 0.5, 0.533, 0.545, 0.542, 0.537, 0.534, 0.534, 0.52, 0.507)
tolerance <- (mean_r2 - max(mean_r2))/max(mean_r2)
tolerance
```

For 10% tolerance, select 2 components.

__(c) & (d)__ If goal is to optimize R^2, then choose the model that has the best R^2 model using bootstrapping. Book recommends to run the complex models first. Then, run simple models and pick the simple model that can get close to the complex model results. Otherwise, models can overfit, be too computationally expensive, and/or not be understood.

### Exercise 4.4

```{r ex4.4, message=FALSE, warning=FALSE}
require(AppliedPredictiveModeling)
require(caret)
data(oil)
str(oilType)
table(oilType)

# (a) sample in base R
set.seed(1)
table(sample(oilType, 60))
set.seed(2)
table(sample(oilType, 60))
set.seed(3)
table(sample(oilType, 60))
set.seed(4)
table(sample(oilType, 60))
set.seed(5)
table(sample(oilType, 60))

# (b) stratified sampling with caret
sample_caret_ID <- createDataPartition(oilType, p = 60/96, list = FALSE)
table(oilType[sample_caret_ID])  # pretty similar to base sample

# (c) resampling for small sample size
# could try a 10-fold CV

# (d) binom.test confidence interval
binom.test(16, 20)  # if 16 out of 20 correct on test set (80% accuracy)
# confidence interval can give a sense of uncertainty
0.943 - 0.563  # width of confidence interval
test_accuracy <- 0.8
diff(binom.test(20 * test_accuracy, 20)[["conf.int"]])
diff(binom.test(40 * test_accuracy, 40)[["conf.int"]])
diff(binom.test(60 * test_accuracy, 60)[["conf.int"]])
diff(binom.test(80 * test_accuracy, 80)[["conf.int"]])
diff(binom.test(100 * test_accuracy, 100)[["conf.int"]])
diff(binom.test(1000 * test_accuracy, 1000)[["conf.int"]])
```
