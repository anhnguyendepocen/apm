---
title: "Applied Predictive Modeling (2013) by Kjell Johnson and Max Kuhn -- Student Solutions"
output: html_document
---

## General Strategies

### Exercise 3.1

```{r, message=FALSE}
library(mlbench)
data(Glass)
str(Glass)

# (a) visualizations
pairs(Glass[,-10])

library(tidyverse)
library(ggplot2)

Glass[, -10] %>% gather() %>% head()  # check
ggplot(gather(Glass[, -10]), aes(value)) +
  geom_histogram(bins = 10) +
  facet_wrap(~key, scales = "free_x")

library(corrplot)
correlations <- cor(Glass[, -10])
correlations
corrplot(correlations, order = "hclust")

# (b) outliers / skewness
# there seems to be an outlier for K and possibly for Fe
library(e1071)
apply(Glass[, -10], 2, skewness)  # definitely skewed

# (c) relevant transformations
# apply box cox to skewed predictors
library(caret)
RITrans <- BoxCoxTrans(Glass$RI)
RITrans
predict(RITrans, head(Glass$RI))

```

### Exercise 3.2

```{r, message=FALSE}
library(mlbench)
data("Soybean")
str(Soybean)

# (a) category frequencies
library(caret)
nearZeroVar(Soybean, saveMetrics = TRUE)
sapply(Soybean[,nearZeroVar(Soybean)], table)

# (b) missing data patterns
library(mice)
md.pattern(Soybean)  # 562 of 683 have values for all cols
# missing values seem to happen for entire categories, i.e. all leaf cols
md.pattern(Soybean) %>% colnames  # col names from most to least filled out

# (c) handle missing values 
soy_data <- Soybean[, -nearZeroVar(Soybean)]  # drop 3 cols
preProcValues <- preProcess(soy_data, method = c("knnImpute"))  # doesn't work bc of categorical vars?
soy_data_imputed <- mice(soy_data, method = "pmm", seed = 500)
```

### Exercise 3.3

```{r, message=FALSE}
# (a) category frequencies
library(caret)
data(BloodBrain)

# (b) degenerate distributions
# assume this means dist with near zero variance
library(magrittr)
sapply(bbbDescr[,nearZeroVar(bbbDescr)], table)
sapply(bbbDescr[,nearZeroVar(bbbDescr)], table) %>% names()

# (c) correlations
correlations <- cor(bbbDescr)
highCorr <- findCorrelation(correlations, cutoff = 0.75)
length(highCorr)  # yes, strong correlations
# use PCA or remove highly correlated predictors
# almost half the vars are highly correlated
# yes, will impact number of predictors for modeling
```

### Exercise 4.1

__(a)__ Goal is to classify music into 6 categories and we have 12,495 samples with 191 predictors. The reponse categories are not balanced (mostly classical, metal and blues are rarer). For a larger dataset like this, computational efficiency is more important. Maybe go with a 10-fold CV. Because response variable is not balanced, may need to use stratified random sampling.

__(b)__ 

1. set seed
2. randomly sample training vs test set
3. create folds using stratified random sampling

### Exercise 4.2

__(a)__ 165 samples and 1,107 predictors. The response is highly skewed and predictors are sparse (15.5% are present) and highly correlated. Because sample size is small, should use 10-fold CV. This has good bias and variance properties. 

### Exercise 4.3

__(b)__ 

```{r, message=FALSE, warning=FALSE}
mean_r2 <- c(0.444, 0.5, 0.533, 0.545, 0.542, 0.537, 0.534, 0.534, 0.52, 0.507)
tolerance <- (mean_r2 - max(mean_r2))/max(mean_r2)
tolerance
```

For 10% tolerance, select 2 components.

__(c) & (d)__ If goal is to optimize R^2, then choose the model that has the best R^2 model using bootstrapping. Book recommends to run the complex models first. Then, run simple models and pick the simple model that can get close to the complex model results. Otherwise, models can overfit, be too computationally expensive, and/or not be understood.

### Exercise 4.4

```{r, message=FALSE, warning=FALSE}

```

## Regression Models

### Exercise 6

```{r, message=FALSE, warning=FALSE}

```

### Exercise 7

```{r, message=FALSE, warning=FALSE}

```

### Exercise 8

```{r, message=FALSE, warning=FALSE}

```

## Classification Models

### Exercise 12

```{r, message=FALSE, warning=FALSE}

```

### Exercise 13

```{r, message=FALSE, warning=FALSE}

```

### Exercise 14

```{r, message=FALSE, warning=FALSE}

```

### Exercise 16

```{r, message=FALSE, warning=FALSE}

```

## Other Considerations

### Exercise 18

```{r, message=FALSE, warning=FALSE}

```

### Exercise 19

```{r, message=FALSE, warning=FALSE}

```

### Exercise 20

```{r, message=FALSE, warning=FALSE}

```