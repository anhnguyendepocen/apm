---
title: "APM Notes"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

## Introduction

### Chapter 1: Introduction

Data characteristics to evaluate

* Response
    * categorical vs. continuous
    * balanced/symmetric
    * unbalanced/skewed
    * independent
* Predictors
    * continuous
    * count
    * categorical
    * correlated/associated
    * different scales
    * missing values
    * sparse
    
    
## General Strategies
    
### Chapter 2: A Short Tour of the Predictive Modeling Process

n/a


### Chapter 3: Data Pre-Processing

* __Centering__: subtract average value so that predictors have 0 mean
* __Scaling__: divide by standard deviation so predictors all have sd of 1

For __skewness__, general rule of thumb is if ratio of highest value to lowest value is greater than 20, there is significant skewness. If using the skewness statistic, a symmetric distribution will have a skewness value close to 0, right skewed distributions have a positive statistic and left skewed distributions have a negative statistic.

Transforming data with log, square root, or inverse could remove skewness. 

__Box-Cox Transformation__

This can be applied independently to each predictor that contains values > 0.

$$x^{*} = \left\{\begin{matrix}
(x^{\lambda-1})/\lambda, &  \lambda \neq 0 \\ 
log(x), & \lambda = 0 \end{matrix}\right.$$

__Outliers__

* Some methods like tree-based and SVM are not sensitive to outliers.
* Spatial sign transformation projects predictors onto a multidimensional sphere (making all samples the same distance from the center). Need to center and scale data first, and has to be applied on entire group of predictors.

__PCA__

* principal components are uncorrelated
* scree plot shows total variance explained by # of components
* scale of components tend to get smaller bc they account for less and less of the variation
* PCA can be used to look at loadings by "channels".

__Missing Values__

* understand why data is missing first
* is pattern of missing data related to outcome?
* censored data is not missing data since something is known about its value
* can impute missing values with K nearest neighbors

__Removing Variables__

* rule of thumb for near-zero variance predictors
    * franction of unique values over sample size is low (<10%)
    * ratio of most frequent to second most frequent value is large (>20)
* variance inflation factor (VIF) can be used to identify collinearity
    * limited to linear regression and requires more samples than predictors
* could remove min # of predictors so that pairwise correlations are below some threshold (e.g. 0.75)


### Chapter 4: Over-Fitting and Model Tuning

* __apparent__ performance is using training set to predict model performance
* __tuning parameters__ are model params that can't be estimated with formulas (number of neighbors in a KNN classification model)

__Data Splitting__

* when samples are not large, may need to avoid test set
    * test set needs to be sufficiently large to have power/precision
* stratified random sampling applies random sampling within groups to account for the outcome
* maximum dissimilarity sampling splits based on predictors

__Resampling Techniques__

* k-fold cross-validation
    * usually k is set to 5 or 10, but no formal rule
    * larger k has smaller bias (difference b/w estimated and true model performance), but more computationally demanding
    * k-fold CV generally has higher variance compared to other methods (problem for small datasets)
    * bias for small values of k (2 or 3) is about the same as the bootstrap, but with larger variance
* stratified random sampling applied to k-fold CV selects k partitions so that folds are balanced with respect to the outcome
* leave-one-out cross-validation (LOOCV) can be thought of as a special case of k-fold where k equals to number of samples
    * has a closed form solution for linear regression models
* repeated training/test splits, also called leave-group-out CV or monte carlo CV
    * rule of thumb for group size is 75-80%
    * unlike k-fold CV, samples can be in multiple hold out sets, and repitions are usually larger (50-200)
* bootstrap samples data with replacement to create subsets that are the same size as total samples
    * samples that are not selected are the "out of bag" samples, used like a hold out test set
    * tend to have less uncertainty than k-fold CV
    * bias can be problematic for small datasets
    * 632 method: 0.632 x bootstrap estimate + 0.368 x apparent error rate
      * 63.2% of data points are represented at least once in the bootstrap sample
      * this method reduces bias, but can be unstable for smaller datasets
      * there's also a 632+ method
      
__Choosing Tuning Parameters__

* in general, favor simpler models
    * choosing tuning params based on numically optimal value can still lead to overfitting
* one-standard error method starts with numerically optimal param and picks the simplest model within one sd
* tolerance is (x-optimal)/optimal and can be used if deciding a certain % loss in performance is acceptable

__Choosing Resampling Methods__

* if sample size is small, recommend 10-fold CV
    * good bias/variance properties
    * low computational costs
* if goal is model selection instead of best indicator of performance, recommend a bootstrap method
    * low variance
* if sample size is large, differences between resampling methods are smaller
    * computational efficiency is more important
    
__Choosing Between Models__

1. start with several models that are less interpretable and most flexible (boosted trees, SVM)
2. investigate simpler models that are less opaque (MARS, PLS, GAM, naive Bayes)
3. use the simplest model that reasonably approximates performance of complex models

A __paired t-test__ can be used to evaluate if there are statistically significant differences in model performance.


## Regression Models

### Chapter 5: Measuring Performance in Regression Models

__Quantitative Performance Measures__

* RMSE is most common
* $R^{2}$ is the coefficient of determination
    * proportion of information in data explained by model
    * simplest version squares the correlation coefficient b/w observed and predicted
    * cautions
        * this reflects correlation, not accuracy
        * is dependent on variation in outcome (in denominator)
            * $R^{2}=1-RMSE/(Sample Variance)$ so result is worse if variance is low and responses with large variance may have very good $R^{2}$ results
* rank correlation (Spearman) can be used for models used to rank new samples 
    * calculated as the correlation coefficent between the ranks of observed and predicted

__Variance-Bias Trade-off__

Expected value of MSE is equal to irreducible noise plus squared bias plus model variance.

* Model bias is how close model form can reflect true relationship between predictors and outcome
* Model variance reflects how much model parameters will change if predictors change
    * collinearity can increase variance

$$E[MSE] = \sigma^{2} + (Model Bias)^{2} + Model Variance$$


### Chapter 6: Linear Regression and Its Cousins

* ordinary linear regression finds parameter estimates that minimize bias
    * objective is to minimize sum of squared errors
* ridge, lasso, and elastic net find estimates that have lower variance

Benefits of linear regression type models

* predictor changes are easiliy interpretable
* relationship among predictors is easily interpretable
* mathematical nature allows for standard error computations
    * can assess statistical significance of each predictor, but need to make assumptions about residual distribution
    
Drawbacks of linear regression type models

* solution may not be linear in parameters
* sensitive to outliers bc SSE is minimized
    * can use MAE or Huber function instead (Huber uses squared residuals under a threshold and simple difference above threshold)
    
__Linear Regression__

$$\hat{\beta} = (X^{T}X)^{-1}X^{T}y$$

* inverse of $(X^{T}X)$ exists when:
    * no predictor can be determined from a combo of other predictors
        * removing pairwise correlated predictors does not eliminate predictors that are functions of 2+ predictors
        * use variance inflation factor to diagnose multicollinearity
    * number of samples is greater than number of predictors
    
__Partial Least Squares__

* pre-processing predictors with PCA before regression is called PCR
    * drawback is that PCA does not consider response, so if variability of predictors is unrelated to variability of response then regression will not be good
        * PLS is recommended when there are correlated predictors and want a linear regression type model
* PLS finds components that maximally summarize variation of predictors while also requiring components to have max correlation with response
    * iterative process of calculating weights (w), scores (t), and loadings (p)
        * w: relationship b/w predictor and response, signals predictor importance in VIP (variable importance in the projection)
        * t: predictor orthongonally projected onto direction
        * p: correlation between scores (t) and original predictors
        * at end of iteration, current estimates are subtracted from predictors and responses
    * can be thought of like supervised dimension reduction
    * predictors should be centered and scaled before PLS, similar to PCA
    * has one tuning parameter: number of components
    * squared VIP values sum to number of predictors
        * general rule of thumb is if VIP > 1 then predictor has predictive info for response
        * predictors with small PLS regression coef and small VIP are likely not important for model
    * algo has been improved upon many times, though other models are still recommended for nonlinear relationships b/w predictors and response instead of augmenting PLS
* in practice, PCR and PLS produce models with similar performance but PCR needs to retain more components

__Penalized Models__

* OLS coef are unbiased and has lowest variance of unbiased linear models
* common that small increase in bias will lower var significantly, thus reducing MSE
    * correlation can lead to large variance
    * biased models that deal w collinearity better can have better MSE than OLS
* one way to create biased model is by using penalty
    * since penalty is applied to coefficients, they should be on the same scale so predictors should be centered and scaled
    * when model overfits or coef are inflated bc of collinearity, can add penalty to SSE if estimates are large
    * ridge regression adds penalty to the sum of squared regression parameters
        * L2: second order penalty
    * lasso (least absolute shrinkage and selection) adds penalty to abs value of regression parameters
        * L1: first order penalty
        * also does feature selection bc some betas are set to 0, unliked ridge
    * LARS (least angle regression) includes lasso and similar models
    * elastic net combines ridge and lasso penalties (different lambdas)
        * combines effective regularization of ridge and feature selection of lasso
        * might more effectively deal w groups of correlated vars
        

### Chapter 7: Nonlinear Regression Models

* linear models can handle non linearity but must know nature of nonlinearity to manually adjust

__Neural Networks__

* hidden layer is a linear combo of predictors, but typically transformed by nonlinear function (i.e. sigmoidal)
* output is a linear combo of hidden layer units
* coef are unlikely to present coherent info bc there are no constraints to defining the linear combo (unlike PLS)
* total params estimates are $H(P+1)+H+1$
* challenging optimization problem bc of number of params
    * back propagation is an efficient method of using derivatives, but soln is often not a global one
    * tendency to overfit
        * can address with early stopping method to halt when error rate increases, however, errors have uncertainty
        * can also use regularization (weight decay) and add penalty to coefficients (__lambda is usually b/w 0 and 0.1__)
        * bc models can be unstable (finds local optimal solution), several models can be averaged, this has a large positive fefect on neural networks 

__Multivariate Adaptive Regression Splines (MARS)__

* creates new features from one or two predictors by finding a breakpoint for which one side is 0 until breakpoint and the other is 0 after breakpoint
    * each predictor is evaluated for breakpoint by minimized error
        * additional features are evaluated on top of prior features until a stopping point is reached
        * hinge functions are typically written as $h(x-a)$ (nonzero when $x>a$) and $h(a-x)$ (nonzero when $x<a$)
        * this wouldn't be done for binary predictors
    * becomes a piecewise linear model
* after creating full set of features, features are pruned if they don't significantly reduce error rate using leave one out CV approximation
    * process does not proceed backwards from feature creation
* second degree MARS does the same search for single predictor breakpoints and then searches for new cuts to couple with original features
    * in practice, have seen second degree or higher MARS produce have instabilities where a few samples predictions are wildly inaccurate
        * this has not been observed with additive MARS models
* two tuning parameters: degree of features added, number of retained terms
* advantages of using MARS
    * automatically does feature selection
        * model equation is independent of predictors that are not involved with any final model features
    * very interpretable
    * requires little pre-processing
        * MARS is okay if there are correlated variables, but strong correlations will hurt interpretability
    
__Support Vector Machines__

* drawback of minimizing SSE is that models are sensitive to outliers
    * alternative is to use Huber function, where residuals are squared for small values else use absolute values
    * SVM is similar in that user selects a threshold and residuals within threshold do not contribute to error and above threshold contribute a linear scale amount
        * squared residuals are not used, so less sensitive to outliers
        * if threshold is high, outliers determine the regression
* SVM has to fit number of parameters on scale of sample size
    * regularization helps over-parameterization
    * training data is required to make new predictions
        * when training data is large, SVM can be time consuming, unless many samples have $\alpha=0$ due to cost penalty
* cost penalty is applied to residuals, lower cost leads to underfitting
* SVM can have different kernels: linear, polynomial, radial, hyperbolic
    * using kernel functions can introduce new parameters
        * for radial kernel, can estimate scale parameter with shortcut
    * in practice radial kernel does quite well, unless true relationship is linear
* in practice, cost parameter provides more flexibility for model tuning, so recommend fixing epsilon and tuning over kernel parameters
* predictors should be centered and scaled bc they enter model as sum of cross products
* relevance vector machine is a bayesian analog to SVM

__K-Nearest Neighbors__

* user defines distance measure
    * Euclidean distance (straight line) is most common
    * Minkowski distance is a generalization of Euclidean distance (handles higher dimensions)
* predictors should be centered and scaled to prevent predictors with larger scales biasing results
* distance computation requires non-missing values
    * can exclude missing values or impute with naive estimator (e.g. mean)
* ideal K can be selection through resampling
    * small K's overfit and large K's underfit
* problems include computation time (distance calc requires entire sample to be in memory) and disconnect between local structure and predictive ability of KNN (irrelevant or noisy predictors need to be removed in pre-processing)


### Chapter 8: Regression Trees and Rule-Based Models



### Chapter 9: A Summary of Solubility Models



### Chapter 10: Case Study: Compressive Strength of Concrete Mixtures



## Classification Models

### Chapter 11: Measuring Performance in Classification Models



### Chapter 12: Discriminant Analysis and Other Linear Classification Models



### Chapter 13: Nonlinear Classification Models



### Chapter 14: Classification Trees and Rule-Based Models



### Chapter 15: A Summary of Grant Application Models



### Chapter 16: Remedies of Severe Class Imbalance



### Chapter 17: Case Study: Job Scheduling



## Other Considerations

### Chapter 18: Measuring Predictor Importance



### Chapter 19: An Introduction to Feature Selection



### Chapter 20: Factors That Can Affect Model Performance


