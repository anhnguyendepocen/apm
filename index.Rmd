---
title: "APM Notes"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

## Introduction

### Chapter 1: Introduction

Data characteristics to evaluate

* Response
    * categorical vs. continuous
    * balanced/symmetric
    * unbalanced/skewed
    * independent
* Predictors
    * continuous
    * count
    * categorical
    * correlated/associated
    * different scales
    * missing values
    * sparse
    
    
## General Strategies
    
### Chapter 2: A Short Tour of the Predictive Modeling Process

n/a


### Chapter 3: Data Pre-Processing

* __Centering__: subtract average value so that predictors have 0 mean
* __Scaling__: divide by standard deviation so predictors all have sd of 1

For __skewness__, general rule of thumb is if ratio of highest value to lowest value is greater than 20, there is significant skewness. If using the skewness statistic, a symmetric distribution will have a skewness value close to 0, right skewed distributions have a positive statistic and left skewed distributions have a negative statistic.

Transforming data with log, square root, or inverse could remove skewness. 

__Box-Cox Transformation__

This can be applied independently to each predictor that contains values > 0.

$$x^{*} = \left\{\begin{matrix}
(x^{\lambda-1})/\lambda, &  \lambda \neq 0 \\ 
log(x), & \lambda = 0 \end{matrix}\right.$$

__Outliers__

* Some methods like tree-based and SVM are not sensitive to outliers.
* Spatial sign transformation projects predictors onto a multidimensional sphere (making all samples the same distance from the center). Need to center and scale data first, and has to be applied on entire group of predictors.

__PCA__

* principal components are uncorrelated
* scree plot shows total variance explained by # of components
* scale of components tend to get smaller bc they account for less and less of the variation
* PCA can be used to look at loadings by "channels".

__Missing Values__

* understand why data is missing first
* is pattern of missing data related to outcome?
* censored data is not missing data since something is known about its value
* can impute missing values with K nearest neighbors

__Removing Variables__

* rule of thumb for near-zero variance predictors
    * franction of unique values over sample size is low (<10%)
    * ratio of most frequent to second most frequent value is large (>20)
* variance inflation factor (VIF) can be used to identify collinearity
    * limited to linear regression and requires more samples than predictors
* could remove min # of predictors so that pairwise correlations are below some threshold (e.g. 0.75)


### Chapter 4: Over-Fitting and Model Tuning

* __apparent__ performance is using training set to predict model performance
* __tuning parameters__ are model params that can't be estimated with formulas (number of neighbors in a KNN classification model)

__Data Splitting__

* when samples are not large, may need to avoid test set
    * test set needs to be sufficiently large to have power/precision
* stratified random sampling applies random sampling within groups to account for the outcome
* maximum dissimilarity sampling splits based on predictors

__Resampling Techniques__

* k-fold cross-validation
    * usually k is set to 5 or 10, but no formal rule
    * larger k has smaller bias (difference b/w estimated and true model performance), but more computationally demanding
    * k-fold CV generally has higher variance compared to other methods (problem for small datasets)
    * bias for small values of k (2 or 3) is about the same as the bootstrap, but with larger variance
* stratified random sampling applied to k-fold CV selects k partitions so that folds are balanced with respect to the outcome
* leave-one-out cross-validation (LOOCV) can be thought of as a special case of k-fold where k equals to number of samples
    * has a closed form solution for linear regression models
* repeated training/test splits, also called leave-group-out CV or monte carlo CV
    * rule of thumb for group size is 75-80%
    * unlike k-fold CV, samples can be in multiple hold out sets, and repitions are usually larger (50-200)
* bootstrap samples data with replacement to create subsets that are the same size as total samples
    * samples that are not selected are the "out of bag" samples, used like a hold out test set
    * tend to have less uncertainty than k-fold CV
    * bias can be problematic for small datasets
    * 632 method: 0.632 x bootstrap estimate + 0.368 x apparent error rate
      * 63.2% of data points are represented at least once in the bootstrap sample
      * this method reduces bias, but can be unstable for smaller datasets
      * there's also a 632+ method
      
__Choosing Tuning Parameters__

* in general, favor simpler models
    * choosing tuning params based on numically optimal value can still lead to overfitting
* one-standard error method starts with numerically optimal param and picks the simplest model within one sd
* tolerance is (x-optimal)/optimal and can be used if deciding a certain % loss in performance is acceptable

__Choosing Resampling Methods__

* if sample size is small, recommend 10-fold CV
    * good bias/variance properties
    * low computational costs
* if goal is model selection instead of best indicator of performance, recommend a bootstrap method
    * low variance
* if sample size is large, differences between resampling methods are smaller
    * computational efficiency is more important
    
__Choosing Between Models__

1. start with several models that are less interpretable and most flexible (boosted trees, SVM)
2. investigate simpler models that are less opaque (MARS, PLS, GAM, naive Bayes)
3. use the simplest model that reasonably approximates performance of complex models

A __paired t-test__ can be used to evaluate if there are statistically significant differences in model performance.


## Regression Models

### Chapter 5: Measuring Performance in Regression Models

__Quantitative Performance Measures__

* RMSE is most common
* $R^{2}$ is the coefficient of determination
    * proportion of information in data explained by model
    * simplest version squares the correlation coefficient b/w observed and predicted
    * cautions
        * this reflects correlation, not accuracy
        * is dependent on variation in outcome (in denominator)
            * $R^{2}=1-RMSE/(Sample Variance)$ so result is worse if variance is low and responses with large variance may have very good $R^{2}$ results
* rank correlation (Spearman) can be used for models used to rank new samples 
    * calculated as the correlation coefficent between the ranks of observed and predicted

__Variance-Bias Trade-off__

Expected value of MSE is equal to irreducible noise plus squared bias plus model variance.

* Model bias is how close model form can reflect true relationship between predictors and outcome
* Model variance reflects how much model parameters will change if predictors change
    * collinearity can increase variance

$$E[MSE] = \sigma^{2} + (Model Bias)^{2} + Model Variance$$


### Chapter 6: Linear Regression and Its Cousins

* ordinary linear regression finds parameter estimates that minimize bias
    * objective is to minimize sum of squared errors
* ridge, lasso, and elastic net find estimates that have lower variance

Benefits of linear regression type models

* predictor changes are easiliy interpretable
* relationship among predictors is easily interpretable
* mathematical nature allows for standard error computations
    * can assess statistical significance of each predictor, but need to make assumptions about residual distribution
    
Drawbacks of linear regression type models

* solution may not be linear in parameters
* sensitive to outliers bc SSE is minimized
    * can use MAE or Huber function instead (Huber uses squared residuals under a threshold and simple difference above threshold)
    
__Linear Regression__

$$\hat{\beta} = (X^{T}X)^{-1}X^{T}y$$

* inverse of $(X^{T}X)$ exists when:
    * no predictor can be determined from a combo of other predictors
        * removing pairwise correlated predictors does not eliminate predictors that are functions of 2+ predictors
        * use variance inflation factor to diagnose multicollinearity
    * number of samples is greater than number of predictors
    
__Partial Least Squares__

* pre-processing predictors with PCA before regression is called PCR
    * drawback is that PCA does not consider response, so if variability of predictors is unrelated to variability of response then regression will not be good
        * PLS is recommended when there are correlated predictors and want a linear regression type model
* PLS finds components that maximally summarize variation of predictors while also requiring components to have max correlation with response
    * iterative process of calculating weights (w), scores (t), and loadings (p)
        * w: relationship b/w predictor and response, signals predictor importance in VIP (variable importance in the projection)
        * t: predictor orthongonally projected onto direction
        * p: correlation between scores (t) and original predictors
        * at end of iteration, current estimates are subtracted from predictors and responses
    * can be thought of like supervised dimension reduction
    * predictors should be centered and scaled before PLS, similar to PCA
    * has one tuning parameter: number of components
    * squared VIP values sum to number of predictors
        * general rule of thumb is if VIP > 1 then predictor has predictive info for response
        * predictors with small PLS regression coef and small VIP are likely not important for model
    * algo has been improved upon many times, though other models are still recommended for nonlinear relationships b/w predictors and response instead of augmenting PLS
* in practice, PCR and PLS produce models with similar performance but PCR needs to retain more components

__Penalized Models__

* OLS coef are unbiased and has lowest variance of unbiased linear models
* common that small increase in bias will lower var significantly, thus reducing MSE
    * correlation can lead to large variance
    * biased models that deal w collinearity better can have better MSE than OLS
* one way to create biased model is by using penalty
    * since penalty is applied to coefficients, they should be on the same scale so predictors should be centered and scaled
    * when model overfits or coef are inflated bc of collinearity, can add penalty to SSE if estimates are large
    * ridge regression adds penalty to the sum of squared regression parameters
        * L2: second order penalty
    * lasso (least absolute shrinkage and selection) adds penalty to abs value of regression parameters
        * L1: first order penalty
        * also does feature selection bc some betas are set to 0, unliked ridge
    * LARS (least angle regression) includes lasso and similar models
    * elastic net combines ridge and lasso penalties (different lambdas)
        * combines effective regularization of ridge and feature selection of lasso
        * might more effectively deal w groups of correlated vars
        

### Chapter 7: Nonlinear Regression Models

* linear models can handle non linearity but must know nature of nonlinearity to manually adjust

__Neural Networks__

* hidden layer is a linear combo of predictors, but typically transformed by nonlinear function (i.e. sigmoidal)
* output is a linear combo of hidden layer units
* coef are unlikely to present coherent info bc there are no constraints to defining the linear combo (unlike PLS)
* total params estimates are $H(P+1)+H+1$
* challenging optimization problem bc of number of params
    * back propagation is an efficient method of using derivatives, but soln is often not a global one
    * tendency to overfit
        * can address with early stopping method to halt when error rate increases, however, errors have uncertainty
        * can also use regularization (weight decay) and add penalty to coefficients (__lambda is usually b/w 0 and 0.1__)
        * bc models can be unstable (finds local optimal solution), several models can be averaged, this has a large positive fefect on neural networks 

__Multivariate Adaptive Regression Splines (MARS)__

* creates new features from one or two predictors by finding a breakpoint for which one side is 0 until breakpoint and the other is 0 after breakpoint
    * each predictor is evaluated for breakpoint by minimized error
        * additional features are evaluated on top of prior features until a stopping point is reached
        * hinge functions are typically written as $h(x-a)$ (nonzero when $x>a$) and $h(a-x)$ (nonzero when $x<a$)
        * this wouldn't be done for binary predictors
    * becomes a piecewise linear model
* after creating full set of features, features are pruned if they don't significantly reduce error rate using leave one out CV approximation
    * process does not proceed backwards from feature creation
* second degree MARS does the same search for single predictor breakpoints and then searches for new cuts to couple with original features
    * in practice, have seen second degree or higher MARS produce have instabilities where a few samples predictions are wildly inaccurate
        * this has not been observed with additive MARS models
* two tuning parameters: degree of features added, number of retained terms
* advantages of using MARS
    * automatically does feature selection
        * model equation is independent of predictors that are not involved with any final model features
    * very interpretable
    * requires little pre-processing
        * MARS is okay if there are correlated variables, but strong correlations will hurt interpretability
    
__Support Vector Machines__

* drawback of minimizing SSE is that models are sensitive to outliers
    * alternative is to use Huber function, where residuals are squared for small values else use absolute values
    * SVM is similar in that user selects a threshold and residuals within threshold do not contribute to error and above threshold contribute a linear scale amount
        * squared residuals are not used, so less sensitive to outliers
        * if threshold is high, outliers determine the regression
* SVM has to fit number of parameters on scale of sample size
    * regularization helps over-parameterization
    * training data is required to make new predictions
        * when training data is large, SVM can be time consuming, unless many samples have $\alpha=0$ due to cost penalty
* cost penalty is applied to residuals, lower cost leads to underfitting
* SVM can have different kernels: linear, polynomial, radial, hyperbolic
    * using kernel functions can introduce new parameters
        * for radial kernel, can estimate scale parameter with shortcut
    * in practice radial kernel does quite well, unless true relationship is linear
* in practice, cost parameter provides more flexibility for model tuning, so recommend fixing epsilon and tuning over kernel parameters
* predictors should be centered and scaled bc they enter model as sum of cross products
* relevance vector machine is a bayesian analog to SVM

__K-Nearest Neighbors__

* user defines distance measure
    * Euclidean distance (straight line) is most common
    * Minkowski distance is a generalization of Euclidean distance (handles higher dimensions)
* predictors should be centered and scaled to prevent predictors with larger scales biasing results
* distance computation requires non-missing values
    * can exclude missing values or impute with naive estimator (e.g. mean)
* ideal K can be selection through resampling
    * small K's overfit and large K's underfit
* problems include computation time (distance calc requires entire sample to be in memory) and disconnect between local structure and predictive ability of KNN (irrelevant or noisy predictors need to be removed in pre-processing)


### Chapter 8: Regression Trees and Rule-Based Models

* advantages
    * do not need as much pre-processing
        * can effectively handle predictors that are sparse, skewed, continuous, categorical, etc. 
    * do not need to specify model form
    * can handle missing data
    * implicitly conducts feature selection
    * when tree is small, easy to interpret and compute
* disadvantages
    * model instability - slight changes in data can drastically change model so model is not as interpretable (for single trees)
        * strongly correlated vars are also an issue
    * not optimal predictive performance bc of defined rectangular space
    * finite terminal nodes results in finite predicted results
    * predictors with more distinct values are implicitly favored (selection bias)
* ensemble models mitigate disadvantages

__Basic Regression Trees__

* CART (classification and regression tree)
    * starts with entire data set, finds split for each predictor to minimize sum of squares error
    * repeats for each split group
    * also called recursive partitioning
    * tree growing step happens until number of samples in the final splits fall below some threshold
        * missing data is ignored, surrogate splits are evaluated
    * tree is then pruned back to a smaller depth
        * one method is cost-complexity tuning
            * finds tree with smallest error rate with penalization for number of nodes
            * number of nodes is multiplied by complexity parameter
                * can select tuning parameter with cross validation and one-standard error rule
    * variable importance can be measured by keeping track of reduction of optimization criteria for each predictor
* GUIDE (generalized, unbiased, interaction detection and estimation)
    * addresses bias issue by decoupling predictor selection (uses hypotheses testing) and split value
* Conditional Inference Trees
    * addresses bias by using hypothesis testing for predection selection and determining split values
    * uses 1 minus p-value as significance threshold
        * allows for comparison of predictors on different scales
        * multiple comparison corrections can be applied to reduce bias and reduce false-positives from conduction a large number of hypothesis tests
    * no need to prune bc more splits result in smaller samples and thus higher p-values
    * p-val is not directly related to model performance so still need to choose tree based on performance
    
__Regression Model Trees__

* terminal nodes can use a function besides average in group (models in the leaves)
* Weka package includes a "rational reconstruction" implementation of M5
    * initial split starts the same as simple regression tree
        * split is found through exhaustive search across all predictors and training samples, but expected reduction in node error rate is used (measures reduction in standard deviation)
    * linear model is fit within partition using the split variable (and preceding ones) to minimize error instead of SD
    * repeat tree growing process until error rate does not improve or not enough samples to continue
        * fully grown tree will have a linear model for every node
    * prune tree using adjusted error rate
        * observed vs predicted error is multiplied by penalty $(n^{*}+p)/(n^{*}-p)$
            * $n^{*}$ is number of training set data points used in linear model
            * $p$ is number of parameters
    * smoothing or recursive shrinking metho is used when predicting new samples
        * linear models along path are combined from the bottom up
            * child model prediction is weighted using training sample size and parent uses a default weighting of 15
* collinearity can still lead to model instability, smoothing helps, but removing correlated vars can have a measurable drop in performance

__Rule-Based Models__

* a rule is a distinct path through a tree (all the way to the end)
* number of samples affected by a rule is called its coverage
* nodes can be redundancies (same split used in different places)
* separate and conquer strategy creates many trees (unsmoothed) and keeps rule with largest coverage, then repeats process on samples that are not in coverage area until all samples are covered 

__Bagged Trees__

* bagging, short for bootstrap aggregation, is a general approach that uses bootstrapping with regression/classification models to construct an ensemble
    * generate m bootstrapped samples of original training data to train unpruned trees of set max depth
    * individual m model predictions are averaged to give the bagged model prediction
    * often see exponential decrease in prediction improvement from increasing m
        * most improvement is seen from small number of trees (m<10)
        * if performance is still not good at m=50, should try a different method (random forest, boosting)
* advantages of bagged models
    * reduces prediction variance through aggregation process (more important for unstable models)
        * changing the sequence of bootstrapped samples should not result in a significantly different end prediction
    * better prediction accuracy (related to predictions being more stable), esp for trees
        * bagging stable, low variance models (linear regression, MARS) will result is smaller accuracy improvements
    * provides internal estimate of predictive performance that correlates well with CV or test set estimates
        * because some (out-of-bag) samples are left out when constructing each tree, sort of like CV
        * average of out-of-bag performance metrics (out-of-bag estimates) can be used to gauge ensemble performance
* disadvantages
    * computational costs and memory requirements (as m increases)
        * parallel computing can help with both computation time and memory (each tree is fit independently)
    * much less interpretable than unbagged model
        * can't obtain any convenient rules like from single tree
        * can still get variable importance (see random forest section)

__Random Forests__

* bagged trees are not completely independent bc all predictors are used for each tree
    * if sample size is large enough, trees might not be that different (tree correlation)
    * to reduce this correlation, randomness can be added by sampling top predictors
* algo for random forests
    * create m bootstrapped samples
    * train tree on each sample
        * at each split randomly select k (or m_try) from the original predictors to evaluate for splits
    * predictions are averages across trees
* m_try parameter
    * default m_try is recommended to be 1/3 of total predictors
    * bc of computational intensity, recommend tuning grid to have 5 values of m_try evenly spaced between 2 and P (total)
* number of trees
    * rf is protected from overfitting so increasing trees just increases computation time
    * recommend using at least 1000 trees, add more trees until CV performance levels off
* advantages
    * more efficient than bagging (bc a subset of predictors are evaluated for each tree), though more trees are required for rf than bagging
    * computation can be parallel, so more efficient than boosting
    * protected from overfitting
* disadvantages
    * rf can underfit when data is not noisy
    * impossible to gain understading of relationship b/w predictors and response
* rf tuning parameters in practice usually does not have too much impact on performance
* using out of bag error rate for evaluation will drastic reduce time needed, compared to CV
* variable importance values
    * between predictor correlations can have significant impact on values
        * uninformative predictors with high correlation to informative predictors can have higher ranking
        * dilution effect (including two important variables that are highly correlated will halve their importance)
    * m_try parameter also has a serious effect on importance values

__Boosting__

* gradient boosting machines (gbm): given a loss function (squared error) and weak learner (regression trees), find an additive model that minimizes loss function
    * initialize with best guess (mean of response), another model is fit to residuals, repeat until user specified number of iterations is hit
    * trees make a great base learner bc
        * can be made weak learners by restricting depth
        * can be additive
        * can be generated quickly
    * if trees are used, tuning parameters will be depth of tree (interaction depth) and number of iterations
* susceptible to overfitting by finding local optima instead of global best split, despit weak learner
    * remedy is to apply regularization (shrinkage) by only adding a fraction (learning rate $\lambda$) of current predicted value
        * small learning rates increase computation time and memory demands
        * usually use learning rate of 0.01 or 0.1
    * can also add a bagging step by only using a random sample (bagging fraction) for each tree (stochastic gradient boosting)
        * increases accuracy and reduces computation time
        * suggest default bagging fraction of 0.5, but this can also be tuned
* computation time for boosting is usually greater than rf bc it can't be made into a parallel process (each tree depends on prior trees)
* variable importance value rankings can drop off more steeply than rf because of tree dependency (structure correlation can lead to same predictors being selected across trees)

__Cubist__

* differs from previous models in three ways
    * technique used for linear model smoothing, creating rules, pruning
    * optional boosting (committees)
    * predictions can be adjusted using nearby training data
    

## Classification Models

### Chapter 11: Measuring Performance in Classification Models

__Class Predictions__

* most classification models generate predicted class probs
    * probably estimate of classes can suggest confidence
    * neural nets and PLS do not generate probabilities (not b/w 0 and 1, or sum to 1)
        * can transform predictions into probability-like values (e.g. softmax transformation)
* calibration plot can assess quality of class probs (obs vs pred probabilities)
    * x = midpoint of predicted bin
    * y = observed
    * want line to be 45 degrees
    * bayes rules and sigmoidal approach can be used to calibrate
* plotting true outcome against probability can also be used for calibration
    * for two classes, histograms can be uniform or skewed
    * for three or more classes, can use heat map
* equivocal (or indeterminate) zones are set to ignore predictions on the border of classes
    * equivocal rate should be reported along with performance of samples outside of the equivocal zone

__Evaluating Predicted Classes__

* confusion matrix is commonly used to measure performance
* simplest metric is overall accuracy
    * most straightforward, but has disadvantages
        * does not distinguish type of error
            * not all errors are equal
        * not great for data with severely imbalanced classes
* kappa statistic (or cohen's kappa) accounts for accuracy generated from chance
    * $Kappa = \frac{O-E}{1-E}$
    * O is observed accuracy
    * E is expected accuracy
    * takes values from -1 to 1, think of like correlation
    * when there are more than two classes and there is a natural ordering to the classes, weighted kappa can be used to penalize errors further from true result
* two class problems
    * sensitivity = true positive / positives = true positive rate
    * specificity = true negatives / negatives
    * false positive rate = 1 - specificity
    * holding accuracy constant, there is usually a tradeoff between sensitivity and specificity
        * ROC (receiver operating characteristic) curve helps evaluate the tradeoffs
        * a single measure can be used J = sensitivity + specificity - 1
    * unconditional rates can be calculated as:
        * PPV (positive prevalance value)
        * NPV (negative prevalance value)
        * prevalance is the frequency of event, but hard to quantify and can vary across predictors (and can also be dynamic, i.e. amount of spam changes based on algos)
* besides accuracy, models can have other primary goals
    * reduce cost, improve custumer satisfaction
    * probability cost function (PCF) accounts for different costs and expected class frequencies
        * calculates portion of cost associated with fast positives
        * normalized expected cost $NEC = PCF \times (1-TP) + (1-PCF) \times FP$
        
__Evaluating Class Probabilities__

* ROC curves plot sensitivity against 1-specificity
    * created by evaluating class probs against different thresholds
    * better models will maximize AUC
    * pro: insensitive to class imbalances
    * con: only defined for two class problems and obscures information
        * model that maximizes area may not be best for area of interest
* lift charts
    * data vis tool for assessing model predictions
    * ranked predictions are compared against actual
    * higher slope = more lift = better model
    

### Chapter 12: Discriminant Analysis and Other Linear Classification Models



### Chapter 13: Nonlinear Classification Models



### Chapter 14: Classification Trees and Rule-Based Models



### Chapter 15: A Summary of Grant Application Models



### Chapter 16: Remedies of Severe Class Imbalance



### Chapter 17: Case Study: Job Scheduling



## Other Considerations

### Chapter 18: Measuring Predictor Importance



### Chapter 19: An Introduction to Feature Selection



### Chapter 20: Factors That Can Affect Model Performance


