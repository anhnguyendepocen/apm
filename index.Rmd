---
title: "Applied Predictive Modeling (2013) by Kjell Johnson and Max Kuhn -- Notes"
output: html_document
---

## Introduction

### Chapter 1

Data characteristics to evaluate

* Response
    * categorical vs. continuous
    * balanced/symmetric
    * unbalanced/skewed
    * independent
* Predictors
    * continuous
    * count
    * categorical
    * correlated/associated
    * different scales
    * missing values
    * sparse
    
## General Strategies
    
### Chapter 2

n/a

### Chapter 3

* __Centering__: subtract average value so that predictors have 0 mean
* __Scaling__: divide by standard deviation so predictors all have sd of 1

For __skewness__, general rule of thumb is if ratio of highest value to lowest value is greater than 20, there is significant skewness. If using the skewness statistic, a symmetric distribution will have a skewness value close to 0, right skewed distributions have a positive statistic and left skewed distributions have a negative statistic.

Transforming data with log, square root, or inverse could remove skewness. 

```{r, message=FALSE}
library(AppliedPredictiveModeling)
library(e1071)
data("segmentationOriginal")
#apply(segmentationOriginal[, -(1:3)], 2, function(x) skewness(x))
# can't match the 2.39 from book
```

__Box-Cox Transformation__

This can be applied independently to each predictor that contains values > 0.

$$x^{*} = \left\{\begin{matrix}
(x^{\lambda-1})/\lambda, &  \lambda \neq 0 \\ 
log(x), & \lambda = 0 \end{matrix}\right.$$

__Outliers__

* Some methods like tree-based and SVM are not sensitive to outliers.
* Spatial sign transformation projects predictors onto a multidimensional sphere (making all samples the same distance from the center). Need to center and scale data first, and has to be applied on entire group of predictors.

__PCA__

* principal components are uncorrelated
* scree plot shows total variance explained by # of components
* scale of components tend to get smaller bc they account for less and less of the variation
* PCA can be used to look at loadings by "channels".

__Missing Values__

* understand why data is missing first
* is pattern of missing data related to outcome?
* censored data is not missing data since something is known about its value
* can impute missing values with K nearest neighbors

__Removing Variables__

* rule of thumb for near-zero variance predictors
    * franction of unique values over sample size is low (<10%)
    * ratio of most frequent to second most frequent value is large (>20)
* variance inflation factor (VIF) can be used to identify collinearity
    * limited to linear regression and requires more samples than predictors
* could remove min # of predictors so that pairwise correlations are below some threshold (e.g. 0.75)

```{r, message=FALSE}
library(AppliedPredictiveModeling)
library(e1071)
library(caret)
library(corrplot)
data("segmentationOriginal")
segData <- subset(segmentationOriginal, Case == "Train")
cellID <- segData$Cell
class <- segData$Class
case <- segData$Case
segData <- segData[, -(1:3)]  # drop cols

statusColNum <- grep("Status", names(segData))  # binary versions of predictors
segData <- segData[, -statusColNum]
skewValues <- apply(segData, 2, skewness)
head(skewValues)

# Box Cox Transform
Ch1AreaTrans <- BoxCoxTrans(segData$AreaCh1)
Ch1AreaTrans
head(segData$AreaCh1)
predict(Ch1AreaTrans, head(segData$AreaCh1))

# PCA using base R
pcaObject <- prcomp(segData,
                    center = TRUE, scale. = TRUE)
percentVariance <- pcaObject$sdev^2/sum(pcaObject$sdev^2)*100
percentVariance[1:3]  # pct var explained
head(pcaObject$x[, 1:5])  # transformed object
head(pcaObject$rotation[, 1:3])  # loadings

# pre-processing with caret
trans <- preProcess(segData,
                    method = c("BoxCox", "center", "scale", "pca"))
trans
transformed <- predict(trans, segData)
head(transformed)
nearZeroVar(segData)

correlations <- cor(segData)
correlations[1:4, 1:4]
#corrplot(correlations, order = "hclust")

highCorr <- findCorrelation(correlations, cutoff = 0.75)
length(highCorr)
head(highCorr)
filteredSegData <- segData[, -highCorr]
```

### Chapter 4

* __apparent__ performance is using training set to predict model performance
* __tuning parameters__ are model params that can't be estimated with formulas (number of neighbors in a KNN classification model)

__Data Splitting__

* when samples are not large, may need to avoid test set
    * test set needs to be sufficiently large to have power/precision
* stratified random sampling applies random sampling within groups to account for the outcome
* maximum dissimilarity sampling splits based on predictors

__Resampling Techniques__

* k-fold cross-validation
    * usually k is set to 5 or 10, but no formal rule
    * larger k has smaller bias (difference b/w estimated and true model performance), but more computationally demanding
    * k-fold CV generally has higher variance compared to other methods (problem for small datasets)
    * bias for small values of k (2 or 3) is about the same as the bootstrap, but with larger variance
* stratified random sampling applied to k-fold CV selects k partitions so that folds are balanced with respect to the outcome
* leave-one-out cross-validation (LOOCV) can be thought of as a special case of k-fold where k equals to number of samples
    * has a closed form solution for linear regression models
* repeated training/test splits, also called leave-group-out CV or monte carlo CV
    * rule of thumb for group size is 75-80%
    * unlike k-fold CV, samples can be in multiple hold out sets, and repitions are usually larger (50-200)
* bootstrap samples data with replacement to create subsets that are the same size as total samples
    * samples that are not selected are the "out of bag" samples, used like a hold out test set
    * tend to have less uncertainty than k-fold CV
    * bias can be problematic for small datasets
    * 632 method: 0.632 x bootstrap estimate + 0.368 x apparent error rate
      * 63.2% of data points are represented at least once in the bootstrap sample
      * this method reduces bias, but can be unstable for smaller datasets
      * there's also a 632+ method
      
__Choosing Tuning Parameters__

* in general, favor simpler models
    * choosing tuning params based on numically optimal value can still lead to overfitting
* one-standard error method starts with numerically optimal param and picks the simplest model within one sd
* tolerance is (x-optimal)/optimal and can be used if deciding a certain % loss in performance is acceptable

__Choosing Resampling Methods__

* if sample size is small, recommend 10-fold CV
    * good bias/variance properties
    * low computational costs
* if goal is model selection instead of best indicator of performance, recommend a bootstrap method
    * low variance
* if sample size is large, differences between resampling methods are smaller
    * computational efficiency is more important
    
__Choosing Between Models__

1. start with several models that are less interpretable and most flexible (boosted trees, SVM)
2. investigate simpler models that are less opaque (MARS, PLS, GAM, naive Bayes)
3. use the simplest model that reasonably approximates performance of complex models

A __paired t-test__ can be used to evaluate if there are statistically significant differences in model performance.

```{r, message=FALSE, warning=FALSE}
rm(list=ls())
library(AppliedPredictiveModeling)
library(caret)
#library(Design)  # replaced with rms
library(rms)
library(e1071)
library(ipred)
library(MASS)

data(twoClassData)
str(predictors)
str(classes)

# data splitting
set.seed(1)
trainingRows <- createDataPartition(classes, p = 0.8, list = FALSE)
head(trainingRows)
trainPredictors <- predictors[trainingRows,]
trainClasses <- classes[trainingRows]  # not a dataframe, just vector
testPredictors <- predictors[-trainingRows,]
testClasses <- classes[-trainingRows]

str(trainPredictors)
str(testPredictors)
# caret fn 'maxdissim' generates test set using max dissimilarity sampling

# resampling
# - repeated training/test splits
set.seed(1)
repeatedSplits <- createDataPartition(trainClasses, p = 0.8, times = 3)
str(repeatedSplits)
# - k-fold CV
set.seed(1)
cvSplits <- createFolds(trainClasses, k = 10, returnTrain = TRUE)
str(cvSplits)
cvPredictors1 <- trainPredictors[cvSplits[[1]],]
cvClasses1 <- trainClasses[cvSplits[[1]]]
nrow(cvPredictors1)/nrow(trainPredictors)
# - bootstrapping: createResamples
# - repeated CV: createMultiFolds

# model building
# using formula interface w large data can by unnecessarily slow
# - using knn3 from caret
trainPredictors <- as.matrix(trainPredictors)
knnFit <- knn3(x = trainPredictors, y = trainClasses, k = 5)
knnFit
testPredictions <- predict(knnFit, newdata = testPredictors, type = "class")
head(testPredictions)

# determining tuning parameters
rm(list=ls())
data("GermanCredit")
set.seed(1056)
trainingRows <- createDataPartition(GermanCredit$Class, p = 0.8, list = FALSE)
GermanCreditTrain <- GermanCredit[trainingRows, -(1:9)]  # drop first 9 cols
dropZeroVar <- nearZeroVar(GermanCreditTrain)
GermanCreditTrain <- GermanCreditTrain[, -dropZeroVar]
GermanCreditTest <- GermanCredit[-trainingRows, -(1:9)]  # drop first 9 cols
GermanCreditTest <- GermanCreditTest[, -dropZeroVar]
svmFit <- train(Class ~.,
                data = GermanCreditTrain,
                method = "svmRadial",
                preProc = c("center", "scale"),
                tuneLength = 10,  # evals 2^(-2) to 2^7
                trControl = trainControl(method = "repeatedcv", repeats = 5)) 
svmFit
plot(svmFit, scales = list(x = list(log = 2)))
#predictedProbs <- predict(svmFit, newdata = GermanCreditTest, type = "prob")  # see errata

# between model comparison
set.seed(1056)
logisticReg <- train(Class ~.,
                     data = GermanCreditTrain,
                     method = "glm",
                     trControl = trainControl(method = "repeatedcv", repeats =5))
logisticReg  # need to remove extra cols for one hot encoding? not going to bother for notes
resamp <- resamples(list(SVM = svmFit, Logistic = logisticReg))
summary(resamp)
?xyplot.resamples
modelDifferences <- diff(resamp)
summary(modelDifferences)
# small values for SVM mean small p vals so possible that models are really different
```


## Regression Models

### Chapter 5



### Chapter 6



### Chapter 7



### Chapter 8



### Chapter 9



### Chapter 10



## Classification Models

### Chapter 11



### Chapter 12



### Chapter 13



### Chapter 14



### Chapter 15



### Chapter 16



### Chapter 17



## Other Considerations

### Chapter 18



### Chapter 19



### Chapter 20


