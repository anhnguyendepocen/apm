<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>APM Computation</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 45px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 50px;
  margin-top: -50px;
}

.section h2 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h3 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h4 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h5 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h6 {
  padding-top: 50px;
  margin-top: -50px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Applied Predictive Modeling (2013)</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Notes</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Computation
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="general-strategies.html">General Strategies</a>
    </li>
    <li>
      <a href="regression-models.html">Regression Models</a>
    </li>
    <li>
      <a href="classification-models.html">Classification Models</a>
    </li>
    <li>
      <a href="other-considerations.html">Other Considerations</a>
    </li>
  </ul>
</li>
<li>
  <a href="solutions.html">Solutions</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">APM Computation</h1>

</div>


<div id="regression-models" class="section level2">
<h2>Regression Models</h2>
<div id="chapter-5-measuring-performance-in-regression-models" class="section level3">
<h3>Chapter 5: Measuring Performance in Regression Models</h3>
<pre class="r"><code>rm(list=ls())
observed &lt;-  c(0.22, 0.83,-0.12, 0.89,-0.23,-1.30,-0.15,-1.4,
               0.62, 0.99,-0.18, 0.32, 0.34,-0.30, 0.04,-0.87,
               0.55,-1.30,-1.15, 0.20)
predicted &lt;- c(0.24, 0.78,-0.66, 0.53, 0.70,-0.75,-0.41,-0.43,
               0.49, 0.79,-1.19, 0.06, 0.75,-0.07, 0.43,-0.42,
              -0.25,-0.64,-1.26,-0.07)
residualValues &lt;- observed - predicted
summary(residualValues)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -0.9700 -0.4200  0.0800 -0.0310  0.2625  1.0100</code></pre>
<pre class="r"><code># observed versus predicted
axisRange &lt;- extendrange(c(observed, predicted))
plot(observed, predicted, ylim = axisRange, xlim = axisRange)
abline(0, 1, col = &quot;darkgrey&quot;, lty = 2)</code></pre>
<p><img src="regression-models_files/figure-html/ch5-1.png" width="672" /></p>
<pre class="r"><code># predicted versus residuals
plot(predicted, residualValues, ylab = &quot;residual&quot;)
abline(h = 0, col = &quot;darkgrey&quot;, lty = 2)

# quantitative model performance measures
library(caret)</code></pre>
<p><img src="regression-models_files/figure-html/ch5-2.png" width="672" /></p>
<pre class="r"><code>R2(predicted, observed)</code></pre>
<pre><code>## [1] 0.5170123</code></pre>
<pre class="r"><code>RMSE(predicted, observed)</code></pre>
<pre><code>## [1] 0.5234883</code></pre>
<pre class="r"><code>cor(predicted, observed)  # base R simple correlation</code></pre>
<pre><code>## [1] 0.7190357</code></pre>
<pre class="r"><code>cor(predicted, observed)^2  # match R^2</code></pre>
<pre><code>## [1] 0.5170123</code></pre>
<pre class="r"><code>cor(predicted, observed, method = &quot;spearman&quot;)  # rank correlation</code></pre>
<pre><code>## [1] 0.7554552</code></pre>
</div>
<div id="chapter-6-linear-regression-and-its-cousins" class="section level3">
<h3>Chapter 6: Linear Regression and Its Cousins</h3>
<pre class="r"><code>rm(list = ls())
require(AppliedPredictiveModeling)
require(elasticnet)
require(lars)
require(caret)
require(MASS)
require(pls)
require(stats)
data(solubility)
ls(pattern = &quot;^sol&quot;)  # obj beginning w &quot;sol&quot;</code></pre>
<pre><code>## [1] &quot;solTestX&quot;       &quot;solTestXtrans&quot;  &quot;solTestY&quot;       &quot;solTrainX&quot;     
## [5] &quot;solTrainXtrans&quot; &quot;solTrainY&quot;</code></pre>
<pre class="r"><code>trainingData &lt;- solTrainXtrans
trainingData$Solubility &lt;- solTrainY

### Ordinary Linear Regression ###

lmFitAllPredictors &lt;- lm(Solubility ~., data = trainingData)
str(summary(lmFitAllPredictors))  # training results</code></pre>
<pre><code>## List of 11
##  $ call         : language lm(formula = Solubility ~ ., data = trainingData)
##  $ terms        :Classes &#39;terms&#39;, &#39;formula&#39;  language Solubility ~ FP001 + FP002 + FP003 + FP004 + FP005 + FP006 + FP007 +      FP008 + FP009 + FP010 + FP011 + FP012 +| __truncated__ ...
##   .. ..- attr(*, &quot;variables&quot;)= language list(Solubility, FP001, FP002, FP003, FP004, FP005, FP006, FP007,      FP008, FP009, FP010, FP011, FP012, FP013, | __truncated__ ...
##   .. ..- attr(*, &quot;factors&quot;)= int [1:229, 1:228] 0 1 0 0 0 0 0 0 0 0 ...
##   .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. .. .. ..$ : chr [1:229] &quot;Solubility&quot; &quot;FP001&quot; &quot;FP002&quot; &quot;FP003&quot; ...
##   .. .. .. ..$ : chr [1:228] &quot;FP001&quot; &quot;FP002&quot; &quot;FP003&quot; &quot;FP004&quot; ...
##   .. ..- attr(*, &quot;term.labels&quot;)= chr [1:228] &quot;FP001&quot; &quot;FP002&quot; &quot;FP003&quot; &quot;FP004&quot; ...
##   .. ..- attr(*, &quot;order&quot;)= int [1:228] 1 1 1 1 1 1 1 1 1 1 ...
##   .. ..- attr(*, &quot;intercept&quot;)= int 1
##   .. ..- attr(*, &quot;response&quot;)= int 1
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; 
##   .. ..- attr(*, &quot;predvars&quot;)= language list(Solubility, FP001, FP002, FP003, FP004, FP005, FP006, FP007,      FP008, FP009, FP010, FP011, FP012, FP013, | __truncated__ ...
##   .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:229] &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ...
##   .. .. ..- attr(*, &quot;names&quot;)= chr [1:229] &quot;Solubility&quot; &quot;FP001&quot; &quot;FP002&quot; &quot;FP003&quot; ...
##  $ residuals    : Named num [1:951] -0.36722 0.62243 -0.47199 -0.33254 -0.00967 ...
##   ..- attr(*, &quot;names&quot;)= chr [1:951] &quot;661&quot; &quot;662&quot; &quot;663&quot; &quot;665&quot; ...
##  $ coefficients : num [1:229, 1:4] 2.4307 0.3594 0.1456 -0.0397 -0.3049 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:229] &quot;(Intercept)&quot; &quot;FP001&quot; &quot;FP002&quot; &quot;FP003&quot; ...
##   .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Std. Error&quot; &quot;t value&quot; &quot;Pr(&gt;|t|)&quot;
##  $ aliased      : Named logi [1:229] FALSE FALSE FALSE FALSE FALSE FALSE ...
##   ..- attr(*, &quot;names&quot;)= chr [1:229] &quot;(Intercept)&quot; &quot;FP001&quot; &quot;FP002&quot; &quot;FP003&quot; ...
##  $ sigma        : num 0.552
##  $ df           : int [1:3] 229 722 229
##  $ r.squared    : num 0.945
##  $ adj.r.squared: num 0.927
##  $ fstatistic   : Named num [1:3] 54 228 722
##   ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;value&quot; &quot;numdf&quot; &quot;dendf&quot;
##  $ cov.unscaled : num [1:229, 1:229] 15.3197 -0.0678 -0.0537 0.0504 -0.022 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:229] &quot;(Intercept)&quot; &quot;FP001&quot; &quot;FP002&quot; &quot;FP003&quot; ...
##   .. ..$ : chr [1:229] &quot;(Intercept)&quot; &quot;FP001&quot; &quot;FP002&quot; &quot;FP003&quot; ...
##  - attr(*, &quot;class&quot;)= chr &quot;summary.lm&quot;</code></pre>
<pre class="r"><code>lmPred1 &lt;- predict(lmFitAllPredictors, solTestXtrans)
head(lmPred1)</code></pre>
<pre><code>##          20          21          23          25          28          31 
##  0.99370933  0.06834627 -0.69877632  0.84796356 -0.16578324  1.40815083</code></pre>
<pre class="r"><code>lmValues1 &lt;- data.frame(obs = solTestY, pred = lmPred1)
defaultSummary(lmValues1)  # caret metrics</code></pre>
<pre><code>##      RMSE  Rsquared       MAE 
## 0.7455802 0.8722236 0.5497605</code></pre>
<pre class="r"><code># using robust lm from MASS (uses Huber approach)
ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 10)
set.seed(100)  
lmFit1 &lt;- train(x = solTrainXtrans, y = solTrainY,
                method = &quot;lm&quot;, trControl = ctrl)
lmFit1</code></pre>
<pre><code>## Linear Regression 
## 
## 951 samples
## 228 predictors
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 856, 856, 855, 855, 857, 856, ... 
## Resampling results:
## 
##   RMSE       Rsquared   MAE      
##   0.6926348  0.8872058  0.5199216
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<pre class="r"><code>xyplot(solTrainY ~ predict(lmFit1),
       # plot points and use background grid
       type = c(&quot;p&quot;, &quot;g&quot;),
       xlab = &quot;Predicted&quot;, ylab = &quot;Observed&quot;)</code></pre>
<p><img src="regression-models_files/figure-html/ch6-1.png" width="672" /></p>
<pre class="r"><code>xyplot(resid(lmFit1) ~ predict(lmFit1),
       type = c(&quot;p&quot;, &quot;g&quot;),
       xlab = &quot;Predicted&quot;, ylab = &quot;Residuals&quot;)</code></pre>
<p><img src="regression-models_files/figure-html/ch6-2.png" width="672" /></p>
<pre class="r"><code># using train function
corThresh &lt;- 0.9
tooHigh &lt;- findCorrelation(cor(solTrainXtrans), corThresh)
corrPred &lt;- names(solTrainXtrans)[tooHigh]
trainXfiltered &lt;- solTrainXtrans[, -tooHigh]
testXfiltered &lt;- solTestXtrans[, -tooHigh]
set.seed(100)
lmFiltered &lt;- train(trainXfiltered, solTrainY, method = &quot;lm&quot;, trControl = ctrl)
lmFiltered</code></pre>
<pre><code>## Linear Regression 
## 
## 951 samples
## 190 predictors
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 856, 856, 855, 855, 857, 856, ... 
## Resampling results:
## 
##   RMSE       Rsquared   MAE      
##   0.6980579  0.8857871  0.5251033
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<pre class="r"><code>rlmPCA &lt;- train(trainXfiltered, solTrainY, method = &quot;rlm&quot;, preProcess = &quot;pca&quot;, trControl = ctrl)
rlmPCA</code></pre>
<pre><code>## Robust Linear Model 
## 
## 951 samples
## 190 predictors
## 
## Pre-processing: principal component signal extraction (190),
##  centered (190), scaled (190) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 856, 855, 857, 855, 855, 855, ... 
## Resampling results across tuning parameters:
## 
##   intercept  psi           RMSE       Rsquared   MAE      
##   FALSE      psi.huber     2.8428697  0.8365759  2.7190689
##   FALSE      psi.hampel    2.8430314  0.8366099  2.7192498
##   FALSE      psi.bisquare  2.8427605  0.8367128  2.7191205
##    TRUE      psi.huber     0.8332064  0.8352457  0.6342326
##    TRUE      psi.hampel    0.8306776  0.8361875  0.6353316
##    TRUE      psi.bisquare  0.8395935  0.8328240  0.6376391
## 
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were intercept = TRUE and psi
##  = psi.hampel.</code></pre>
<pre class="r"><code>### Partial Least Squares ###
plsFit &lt;- plsr(Solubility ~., data = trainingData)
predict(plsFit, solTestXtrans[1:5,], ncomp = 1:2)</code></pre>
<pre><code>## , , 1 comps
## 
##    Solubility
## 20  -1.789335
## 21  -1.427551
## 23  -2.268798
## 25  -2.269782
## 28  -1.867960
## 
## , , 2 comps
## 
##    Solubility
## 20  0.2520469
## 21  0.3555028
## 23 -1.8795338
## 25 -0.6848584
## 28 -1.5531552</code></pre>
<pre class="r"><code># using train function
plsTune &lt;- train(solTrainXtrans, solTrainY,
                 method = &quot;pls&quot;, 
                 tuneLength = 20,  # default tuning grid evals 1:tuneLength
                 trControl = ctrl,
                 preProcess = c(&quot;center&quot;, &quot;scale&quot;))
plsTune</code></pre>
<pre><code>## Partial Least Squares 
## 
## 951 samples
## 228 predictors
## 
## Pre-processing: centered (228), scaled (228) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 855, 856, 856, 857, 855, 856, ... 
## Resampling results across tuning parameters:
## 
##   ncomp  RMSE       Rsquared   MAE      
##    1     1.2837126  0.6035247  0.9922825
##    2     1.0547201  0.7324842  0.8319901
##    3     0.9357512  0.7900099  0.7242684
##    4     0.8616608  0.8211019  0.6722070
##    5     0.8160783  0.8387881  0.6361127
##    6     0.7822121  0.8532419  0.6045357
##    7     0.7563839  0.8627598  0.5768689
##    8     0.7445818  0.8666266  0.5698316
##    9     0.7319242  0.8710991  0.5593103
##   10     0.7165426  0.8766836  0.5474919
##   11     0.7114162  0.8786968  0.5428781
##   12     0.7016554  0.8820784  0.5353093
##   13     0.7005284  0.8826371  0.5331785
##   14     0.7016078  0.8824373  0.5350061
##   15     0.6973150  0.8839006  0.5303442
##   16     0.6941679  0.8850301  0.5307692
##   17     0.6941309  0.8850671  0.5328551
##   18     0.6902336  0.8862730  0.5310625
##   19     0.6929507  0.8853791  0.5320262
##   20     0.6948920  0.8847706  0.5335884
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was ncomp = 18.</code></pre>
<pre class="r"><code>### Penalized Regression Models ###

# ridge; lm.ridge from MASS or enet from elasticnet
ridgeModel &lt;- enet(x = as.matrix(solTrainXtrans), y = solTrainY, 
                   lambda = 0.001)  # this is ridge penalty
ridgePred &lt;- predict(ridgeModel, newx = as.matrix(solTestXtrans),
                     s = 1, mode = &quot;fraction&quot;,  # s=1 is full solution. lasso lamba=0 so this is ridge regression
                     type = &quot;fit&quot;)
head(ridgePred$fit)</code></pre>
<pre><code>##          20          21          23          25          28          31 
##  0.96795590  0.06918538 -0.54365077  0.96072014 -0.03594693  1.59284535</code></pre>
<pre class="r"><code># defining tuning grid
ridgeGrid &lt;- data.frame(.lambda = seq(0, 0.1, length = 15))
set.seed(100)
ridgeRegFit &lt;- train(solTrainXtrans, solTrainY,
                     method = &quot;ridge&quot;,
                     tuneGrid = ridgeGrid,
                     trControl = ctrl,
                     preProcess = c(&quot;center&quot;, &quot;scale&quot;))
ridgeRegFit</code></pre>
<pre><code>## Ridge Regression 
## 
## 951 samples
## 228 predictors
## 
## Pre-processing: centered (228), scaled (228) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 856, 856, 855, 855, 857, 856, ... 
## Resampling results across tuning parameters:
## 
##   lambda       RMSE       Rsquared   MAE      
##   0.000000000  0.6923558  0.8872977  0.5194817
##   0.007142857  0.6842051  0.8901855  0.5180204
##   0.014285714  0.6782572  0.8924345  0.5135023
##   0.021428571  0.6763196  0.8933364  0.5129646
##   0.028571429  0.6761659  0.8936611  0.5137609
##   0.035714286  0.6770285  0.8936769  0.5150076
##   0.042857143  0.6785555  0.8935075  0.5169778
##   0.050000000  0.6805575  0.8932196  0.5190373
##   0.057142857  0.6829220  0.8928530  0.5213703
##   0.064285714  0.6855755  0.8924331  0.5238093
##   0.071428571  0.6884742  0.8919761  0.5263585
##   0.078571429  0.6915802  0.8914943  0.5290529
##   0.085714286  0.6948706  0.8909958  0.5318508
##   0.092857143  0.6983276  0.8904864  0.5347012
##   0.100000000  0.7019378  0.8899703  0.5375828
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was lambda = 0.02857143.</code></pre>
<pre class="r"><code># lasso: lars from lars or enet from elasticnet or glmnet
enetModel &lt;- enet(x = as.matrix(solTrainXtrans), y = solTrainY,
                  lambda = 0.01, normalize = TRUE)  # normalize does center and scale
enetPred &lt;- predict(enetModel, newx = as.matrix(solTestXtrans),
                    s = 0.1, mode = &quot;fraction&quot;,
                    type = &quot;fit&quot;)
names(enetPred)</code></pre>
<pre><code>## [1] &quot;s&quot;        &quot;fraction&quot; &quot;mode&quot;     &quot;fit&quot;</code></pre>
<pre class="r"><code>head(enetPred$fit)</code></pre>
<pre><code>##          20          21          23          25          28          31 
## -0.60186178 -0.42226814 -1.20465564 -1.23652963 -1.25023517 -0.05587631</code></pre>
<pre class="r"><code>enetCoef &lt;- predict(enetModel, newx = as.matrix(solTestXtrans),
                    s = 0.1, mode = &quot;fraction&quot;,
                    type = &quot;coefficients&quot;)

tail(enetCoef$coefficients)</code></pre>
<pre><code>##       NumChlorine        NumHalogen          NumRings HydrophilicFactor 
##        0.00000000        0.00000000        0.00000000        0.12678967 
##      SurfaceArea1      SurfaceArea2 
##        0.09035596        0.00000000</code></pre>
<pre class="r"><code># using train function for lasso
enetGrid &lt;- expand.grid(.lambda = c(0, 0.01, 0.1),
                        .fraction = seq(0.05, 1, length = 20))
set.seed(100)
enetTune &lt;- train(solTrainXtrans, solTrainY,
                  method = &quot;enet&quot;,
                  tuneGrid = enetGrid,
                  trControl = ctrl,
                  preProcess = c(&quot;center&quot;, &quot;scale&quot;))
plot(enetTune)</code></pre>
<p><img src="regression-models_files/figure-html/ch6-3.png" width="672" /></p>
</div>
<div id="chapter-7-nonlinear-regression-models" class="section level3">
<h3>Chapter 7: Nonlinear Regression Models</h3>
<pre class="r"><code>rm(list = ls())
require(AppliedPredictiveModeling)
require(caret)
require(earth)
require(kernlab)
require(nnet)

### Neural Networks ###

# using nnet
# nnetFit &lt;- nnet(predictors, outcome, 
#                 size = 5,
#                 decay = 0.01,
#                 linout = TRUE,
#                 trace = FALSE, 
#                 maxit = 500,
#                 MaxNwts = 5 * (ncol(predictors) + 1) + 5 + 1)
# nnetAvg &lt;- avNNet(predictors, outcome, 
#                 size = 5,
#                 decay = 0.01,
#                 linout = TRUE,
#                 trace = FALSE, 
#                 maxit = 500,
#                 MaxNwts = 5 * (ncol(predictors) + 1) + 5 + 1)
# predict(nnetFit, newData)
# predict(nnetAvg, newData)

# using train function, method = &quot;nnet&quot; or method = &quot;avNNet&quot;
data(solubility)
tooHigh &lt;- findCorrelation(cor(solTrainXtrans), cutoff = 0.75)
trainXnnet &lt;- solTrainXtrans[, -tooHigh]
testXnnet &lt;- solTestXtrans[, -tooHigh]
nnetGrid &lt;- expand.grid(.decay = c(0, 0.01, 0.1),
                        .size = c(1:10),
                        .bag = FALSE)
ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 10)
set.seed(100)
# nnetTune &lt;- train(trainXnnet, solTrainY,  # takes a long time to run
#                   method = &quot;avNNet&quot;,
#                   tuneGrid = nnetGrid,
#                   trControl = ctrl,
#                   preProcess = c(&quot;center&quot;, &quot;scale&quot;),
#                   linout = TRUE,
#                   trace = FALSE,
#                   MaxNWts = 10 * (ncol(trainXnnet) + 1) + 10 + 1,
#                   maxit = 500)

### Multivariate Adaptive Regression Splines ###

# using earth package
marsFit &lt;- earth(solTrainXtrans, solTrainY)
marsFit</code></pre>
<pre><code>## Selected 40 of 47 terms, and 31 of 228 predictors
## Termination condition: RSq changed by less than 0.001 at 47 terms
## Importance: NumNonHAtoms, SurfaceArea2, MolWeight, SurfaceArea1, ...
## Number of terms at each degree of interaction: 1 39 (additive model)
## GCV 0.3873018    RSS 309.672    GRSq 0.9076346    RSq 0.9221793</code></pre>
<pre class="r"><code>summary(marsFit)  # more details</code></pre>
<pre><code>## Call: earth(x=solTrainXtrans, y=solTrainY)
## 
##                                 coefficients
## (Intercept)                        -4.455949
## FP002                               0.733904
## FP003                              -0.203502
## FP059                              -0.613495
## FP065                              -0.278610
## FP075                               0.295269
## FP083                              -0.563202
## FP085                              -0.193880
## FP099                               0.337591
## FP111                              -0.428246
## FP135                               0.405277
## FP142                               0.397119
## FP154                              -0.597650
## FP172                              -0.527362
## FP176                               0.294774
## FP188                               0.407757
## FP202                               0.279390
## FP204                              -0.336720
## FP207                               0.424005
## h(MolWeight-5.77157)               -1.964915
## h(5.94458-MolWeight)                0.686131
## h(2.99573-NumNonHAtoms)             2.755144
## h(NumNonHAtoms-2.99573)            -3.517132
## h(2.57858-NumNonHBonds)            -0.647484
## h(0.79877-NumMultBonds)            -0.501790
## h(2.19722-NumRotBonds)              0.120247
## h(NumRotBonds-2.19722)             -2.709055
## h(0.941208-NumDblBonds)             0.501773
## h(2.48491-NumAromaticBonds)         0.492052
## h(NumAromaticBonds-2.48491)        -2.811877
## h(0.584815-NumNitrogen)            -1.593633
## h(NumNitrogen-0.584815)             9.162309
## h(1.38629-NumOxygen)               -0.730829
## h(NumOxygen-1.38629)                3.358142
## h(NumChlorine-0.46875)            -51.880014
## h(-0.816625-HydrophilicFactor)     -0.374411
## h(HydrophilicFactor- -0.816625)     0.196559
## h(SurfaceArea1-1.9554)              0.148552
## h(4.66178-SurfaceArea2)            -0.158472
## h(SurfaceArea2-4.66178)            -0.168509
## 
## Selected 40 of 47 terms, and 31 of 228 predictors
## Termination condition: RSq changed by less than 0.001 at 47 terms
## Importance: NumNonHAtoms, SurfaceArea2, MolWeight, SurfaceArea1, ...
## Number of terms at each degree of interaction: 1 39 (additive model)
## GCV 0.3873018    RSS 309.672    GRSq 0.9076346    RSq 0.9221793</code></pre>
<pre class="r"><code>plotmo(marsFit)</code></pre>
<pre><code>##  plotmo grid:    FP001 FP002 FP003 FP004 FP005 FP006 FP007 FP008 FP009
##                      0     1     0     1     1     0     0     0     0
##  FP010 FP011 FP012 FP013 FP014 FP015 FP016 FP017 FP018 FP019 FP020 FP021
##      0     0     0     0     0     1     0     0     0     0     0     0
##  FP022 FP023 FP024 FP025 FP026 FP027 FP028 FP029 FP030 FP031 FP032 FP033
##      0     0     0     0     0     0     0     0     0     0     0     0
##  FP034 FP035 FP036 FP037 FP038 FP039 FP040 FP041 FP042 FP043 FP044 FP045
##      0     0     0     0     0     0     0     0     0     0     0     0
##  FP046 FP047 FP048 FP049 FP050 FP051 FP052 FP053 FP054 FP055 FP056 FP057
##      0     0     0     0     0     0     0     0     0     0     0     0
##  FP058 FP059 FP060 FP061 FP062 FP063 FP064 FP065 FP066 FP067 FP068 FP069
##      0     0     0     0     0     0     0     1     1     0     0     0
##  FP070 FP071 FP072 FP073 FP074 FP075 FP076 FP077 FP078 FP079 FP080 FP081
##      0     0     1     0     0     0     0     0     0     1     0     0
##  FP082 FP083 FP084 FP085 FP086 FP087 FP088 FP089 FP090 FP091 FP092 FP093
##      1     0     0     0     0     1     0     0     0     0     0     0
##  FP094 FP095 FP096 FP097 FP098 FP099 FP100 FP101 FP102 FP103 FP104 FP105
##      0     0     0     0     0     0     0     0     0     0     0     0
##  FP106 FP107 FP108 FP109 FP110 FP111 FP112 FP113 FP114 FP115 FP116 FP117
##      0     0     0     0     0     0     0     0     0     0     0     0
##  FP118 FP119 FP120 FP121 FP122 FP123 FP124 FP125 FP126 FP127 FP128 FP129
##      0     0     0     0     0     0     0     0     0     0     0     0
##  FP130 FP131 FP132 FP133 FP134 FP135 FP136 FP137 FP138 FP139 FP140 FP141
##      0     0     0     0     0     0     0     0     0     0     0     0
##  FP142 FP143 FP144 FP145 FP146 FP147 FP148 FP149 FP150 FP151 FP152 FP153
##      0     0     0     0     0     0     0     0     0     0     0     0
##  FP154 FP155 FP156 FP157 FP158 FP159 FP160 FP161 FP162 FP163 FP164 FP165
##      0     0     0     0     0     0     0     0     0     0     1     0
##  FP166 FP167 FP168 FP169 FP170 FP171 FP172 FP173 FP174 FP175 FP176 FP177
##      0     0     1     0     0     0     0     0     0     0     0     0
##  FP178 FP179 FP180 FP181 FP182 FP183 FP184 FP185 FP186 FP187 FP188 FP189
##      0     0     0     0     0     0     0     0     0     0     0     0
##  FP190 FP191 FP192 FP193 FP194 FP195 FP196 FP197 FP198 FP199 FP200 FP201
##      0     0     0     0     0     0     0     0     0     0     0     0
##  FP202 FP203 FP204 FP205 FP206 FP207 FP208 MolWeight NumAtoms NumNonHAtoms
##      0     0     0     0     0     0     0  5.194234 3.135494     2.564949
##  NumBonds NumNonHBonds NumMultBonds NumRotBonds NumDblBonds
##  3.178054     3.351388     2.944766    1.098612   0.5670767
##  NumAromaticBonds NumHydrogen NumCarbon NumNitrogen NumOxygen NumSulfer
##           1.94591    3.691453  3.317541           0 0.6931472         0
##  NumChlorine NumHalogen  NumRings HydrophilicFactor SurfaceArea1
##            0          0 0.6931472        -0.3630242      7.25813
##  SurfaceArea2
##      7.759912</code></pre>
<p><img src="regression-models_files/figure-html/ch7-1.png" width="672" /></p>
<pre class="r"><code>evimp(marsFit)</code></pre>
<pre><code>##                   nsubsets   gcv    rss
## NumNonHAtoms            38  90.0   90.0
## SurfaceArea2            38  90.0   90.0
## MolWeight               37 100.0&gt; 100.0&gt;
## SurfaceArea1            37  57.3   58.4
## FP142                   35  45.2   46.7
## FP204                   34  40.2   41.9
## NumAromaticBonds        33  38.3   40.0
## FP172                   32  36.1   37.9
## NumOxygen               30  31.3   33.3
## NumNonHBonds            29  29.1   31.2
## FP135                   28  26.9   29.1
## FP059                   27  25.0   27.3
## NumNitrogen             26  23.4   25.8
## FP083                   25  23.1   25.4
## FP154                   24  21.8   24.1
## FP002                   22  19.6   22.0
## NumChlorine             19  17.6   19.9
## FP207                   18  16.3   18.6
## FP202                   17  15.4   17.8
## NumRotBonds             16  14.6   17.0
## NumDblBonds             15  13.8   16.1
## FP188                   14  13.4   15.6
## FP003                   13  13.0   15.0
## FP099                   11  11.5   13.4
## FP111                   10  10.6   12.6
## NumMultBonds             8   8.6   10.6
## HydrophilicFactor        7   8.0    9.8
## FP065                    6   6.9    8.7
## FP085                    5   5.7    7.6
## FP075                    4   4.6    6.5
## FP176                    3   3.5    5.3</code></pre>
<pre class="r"><code># using train function
marsGrid &lt;- expand.grid(.degree = 1:2, .nprune = 2:38)
set.seed(100)
# marsTuned &lt;- train(solTrainXtrans, solTrainY,  # also takes a long time to run
#                    method = &quot;earth&quot;,
#                    tuneGrid = marsGrid,
#                    trControl = trainControl(method = &quot;cv&quot;))
# marsSTuned
# head(predict(marsTuned, solTestXtrans))
# varImp(marsTuned)

### Support Vector Machines ###

# using kernlab
trainingData &lt;- solTrainXtrans
trainingData$Solubility &lt;- solTrainY
svmFit &lt;- ksvm(Solubility ~., data = trainingData,
               kernel =&quot;rbfdot&quot;, kpar = &quot;automatic&quot;,
               C = 1, epsilon = 0.1)
svmFit</code></pre>
<pre><code>## Support Vector Machine object of class &quot;ksvm&quot; 
## 
## SV type: eps-svr  (regression) 
##  parameter : epsilon = 0.1  cost C = 1 
## 
## Gaussian Radial Basis kernel function. 
##  Hyperparameter : sigma =  0.00276299791797444 
## 
## Number of Support Vectors : 669 
## 
## Objective Function Value : -125.1328 
## Training error : 0.041482</code></pre>
<pre class="r"><code># using train function
# svmRTuned &lt;- train(solTrainXtrans, solTrainY,
#                    method = &quot;svmRadial&quot;,
#                    preProcess = c(&quot;center&quot;, &quot;scale&quot;),
#                    tuneLength = 14,  # 2^-2 to 2^11 cost values
#                    trControl = trainControl(method = &quot;cv&quot;))
# svmRTuned
# svmRTuned$finalModel  

### K-Nearest Neighbors ###

# using the train function from caret
knnDescr &lt;- solTrainXtrans[, -nearZeroVar(solTrainXtrans)]  # remove sparse and unbalanced fingerprints
set.seed(100)
knnTune &lt;- train(knnDescr,
                 solTrainY,
                 method = &quot;knn&quot;,
                 preProcess = c(&quot;center&quot;, &quot;scale&quot;),
                 tuneGrid = data.frame(.k = 1:20),
                 trControl = trainControl(method = &quot;cv&quot;))
# new data will be auto centered and scaled when using knn model object for prediction
knnTune</code></pre>
<pre><code>## k-Nearest Neighbors 
## 
## 951 samples
## 225 predictors
## 
## Pre-processing: centered (225), scaled (225) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 856, 856, 855, 855, 857, 856, ... 
## Resampling results across tuning parameters:
## 
##   k   RMSE      Rsquared   MAE      
##    1  1.250232  0.6611312  0.9165221
##    2  1.103952  0.7197662  0.7988127
##    3  1.065688  0.7350824  0.7903996
##    4  1.043534  0.7449116  0.7860586
##    5  1.049529  0.7409384  0.7889923
##    6  1.058417  0.7359120  0.7986353
##    7  1.060938  0.7331595  0.7980003
##    8  1.055874  0.7358010  0.7974424
##    9  1.065884  0.7311823  0.8082185
##   10  1.070847  0.7286249  0.8134830
##   11  1.069596  0.7302802  0.8124852
##   12  1.076953  0.7269775  0.8202945
##   13  1.088453  0.7204797  0.8318907
##   14  1.093359  0.7181335  0.8380720
##   15  1.100073  0.7149682  0.8461514
##   16  1.104751  0.7127331  0.8533856
##   17  1.109370  0.7101825  0.8581988
##   18  1.119957  0.7059283  0.8669092
##   19  1.127248  0.7024141  0.8742935
##   20  1.132476  0.6998827  0.8761286
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was k = 4.</code></pre>
</div>
<div id="chapter-8-regression-trees-and-rule-based-models" class="section level3">
<h3>Chapter 8: Regression Trees and Rule-Based Models</h3>
<pre class="r"><code>rm(list = ls())
library(caret)
library(Cubist)
library(gbm)
library(ipred)
library(party)
library(partykit)
library(randomForest)
library(rpart)
library(RWeka)
library(AppliedPredictiveModeling)
data(solubility)
trainingData &lt;- solTrainXtrans
trainingData$Solubility &lt;- solTrainY

### Single Trees ###

# rpart uses CART
set.seed(100)
rpartTune &lt;- train(solTrainXtrans, solTrainY,
                   method = &quot;rpart2&quot;,  # use rpart to tune over complexity parameter, rpart2 tune over maxdepth
                   tuneLength = 10,
                   trControl = trainControl(method= &quot;cv&quot;))
plot(rpartTune)</code></pre>
<p><img src="regression-models_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="r"><code># ctree in party uses conditional inference
# use ctree_control to tune parameters

### Model Trees ###

# use method = &quot;M5&quot; in caret or RWeka
# set.seed(100)
# m5Tune &lt;- train(solTrainXtrans, solTrainY,
#                 method = &quot;M5&quot;,
#                 trControl = trainControl(method = &quot;cv&quot;),
#                 control =  Weka_control(M = 10))
# plot(m5Tune)

### Bagged Trees ###

# ipred: bagging, ipredbagg
# RWeka: Baggin
# caret: method = bag
# party: cforest set mtry equal to total number of predictors

### Random Forest ###

# randomForest
# can&#39;t use with missing data
rfModel &lt;- randomForest(solTrainXtrans, solTrainY,
                        importance = TRUE,
                        ntrees = 1000)
head(varImp(rfModel))  # caret wrapper for variable importance</code></pre>
<pre><code>##        Overall
## FP001 4.344181
## FP002 3.916990
## FP003 5.420408
## FP004 5.674667
## FP005 4.445544
## FP006 5.573001</code></pre>
<pre class="r"><code>### Boosted Trees ###

# gbm 
gbmModel &lt;- gbm.fit(solTrainXtrans, solTrainY, distribution = &quot;gaussian&quot;)</code></pre>
<pre><code>## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        4.1811             nan     0.0010    0.0031
##      2        4.1783             nan     0.0010    0.0029
##      3        4.1750             nan     0.0010    0.0029
##      4        4.1720             nan     0.0010    0.0032
##      5        4.1689             nan     0.0010    0.0031
##      6        4.1658             nan     0.0010    0.0030
##      7        4.1628             nan     0.0010    0.0029
##      8        4.1598             nan     0.0010    0.0027
##      9        4.1568             nan     0.0010    0.0031
##     10        4.1537             nan     0.0010    0.0030
##     20        4.1243             nan     0.0010    0.0030
##     40        4.0654             nan     0.0010    0.0029
##     60        4.0092             nan     0.0010    0.0027
##     80        3.9550             nan     0.0010    0.0026
##    100        3.9016             nan     0.0010    0.0026</code></pre>
<pre class="r"><code># use gaussian distribution for continuous response

# tuning with caret
# gbmGrid &lt;- expand.grid(.interaction.depth = seq(1, 7, by = 2),
#                        .n.trees = seq(100, 1000, by = 50),
#                        .shrinkage = c(0.01, 0.1))  # need to add n.minobsinnode
# set.seed(100)
# gbmTune &lt;- train(solTrainXtrans, solTrainY,
#                  method = &quot;gbm&quot;,
#                  tuneGrid = gbmGrid,
#                  verbose = FALSE)

### Cubist ###

cubistMod &lt;- cubist(solTrainXtrans, solTrainY)
# predict(cubistMod, solTestXtrans)  # use neighbors agument to pass 0-9 integer to adjust predictions
summary(cubistMod)</code></pre>
<pre><code>## 
## Call:
## cubist.default(x = solTrainXtrans, y = solTrainY)
## 
## 
## Cubist [Release 2.07 GPL Edition]  Mon Jun 25 09:16:43 2018
## ---------------------------------
## 
##     Target attribute `outcome&#39;
## 
## Read 951 cases (229 attributes) from undefined.data
## 
## Model:
## 
##   Rule 1: [52 cases, mean -5.752, range -11.62 to -2.09, est err 0.603]
## 
##     if
##  MolWeight &gt; 5.890234
##  SurfaceArea2 &lt;= 13.23537
##     then
##  outcome = 8.148 - 5.77 NumNonHBonds - 10.79 NumAtoms + 8 NumNonHAtoms
##            + 6.31 NumBonds + 0.92 NumHydrogen + 0.24 SurfaceArea1
##            + 0.67 NumCarbon - 0.34 NumMultBonds - 1.06 MolWeight
##            + 0.8 NumRings + 0.28 NumAromaticBonds + 0.74 FP142
##            + 0.042 SurfaceArea2 - 0.39 FP172 + 0.22 NumOxygen
##            - 0.7 NumHalogen - 0.26 FP065 + 0.36 FP173 - 0.28 FP094
##            + 0.24 FP066 + 0.28 FP102 + 0.24 FP092 - 0.26 NumDblBonds
##            - 0.22 FP081 + 0.31 FP053 - 0.18 FP003 + 0.2 FP088 + 0.2 FP101
##            + 0.19 FP099 + 0.2 FP116 + 0.25 FP147 + 0.16 FP046
##            - 0.17 FP085 + 0.14 FP082 + 0.13 FP071 + 0.13 FP080
## 
##   Rule 2: [24 cases, mean -3.943, range -6.7 to -0.5, est err 0.795]
## 
##     if
##  MolWeight &gt; 5.890234
##  SurfaceArea2 &gt; 13.23537
##     then
##  outcome = 12.334 - 3.92 MolWeight + 0.386 SurfaceArea1
##            + 0.21 SurfaceArea2 - 1.63 NumAtoms - 0.83 NumNonHBonds
##            + 1.32 NumNonHAtoms + 0.91 NumBonds + 0.13 NumHydrogen
##            + 0.1 NumCarbon - 0.05 NumMultBonds + 0.12 NumRings
##            + 0.04 NumAromaticBonds + 0.11 FP142 - 0.1 NumHalogen
## 
##   Rule 3: [375 cases, mean -3.107, range -8.8 to 1.11, est err 0.497]
## 
##     if
##  MolWeight &lt;= 5.890234
##  HydrophilicFactor &lt;= -0.6934879
##     then
##  outcome = 11.608 - 24.37 NumAtoms + 17.57 NumNonHAtoms
##            - 8.82 NumNonHBonds + 15.61 NumBonds + 2.11 NumHydrogen
##            - 2.06 MolWeight + 0.217 SurfaceArea1 + 0.83 HydrophilicFactor
##            + 0.62 NumRings + 0.75 FP102 - 0.17 NumMultBonds
##            - 0.29 NumRotBonds - 0.61 FP141 + 0.48 FP116 + 0.53 FP173
##            + 0.36 FP168 + 0.13 NumAromaticBonds - 0.8 NumChlorine
##            + 0.28 FP080 + 0.13 NumCarbon + 0.35 FP142 - 0.5 NumHalogen
##            + 0.2 FP046 - 0.018 SurfaceArea2 + 0.18 FP092 + 0.12 NumOxygen
##            - 0.13 FP065 + 0.13 FP066 - 0.15 FP094 + 0.15 FP099
##            - 0.17 FP172 + 0.14 FP101 - 0.15 NumDblBonds - 0.12 FP081
##            + 0.16 FP053 + 0.1 FP088 - 0.06 FP112
## 
##   Rule 4: [120 cases, mean -2.743, range -6.81 to 1.09, est err 0.624]
## 
##     if
##  FP043 &lt;= 0
##  MolWeight &lt;= 5.890234
##  NumRotBonds &gt; 1.386294
##  NumOxygen &gt; 0.6931472
##     then
##  outcome = 26.218 - 40.17 NumAtoms + 29.76 NumBonds + 3.44 NumHydrogen
##            + 2.24 NumOxygen - 1.17 NumCarbon + 0.248 SurfaceArea1
##            - 1.34 NumRotBonds - 2.01 MolWeight + 1.81 NumNonHAtoms
##            + 0.96 FP142 - 0.33 NumNonHBonds + 0.35 NumRings
##            - 0.006 SurfaceArea2 + 0.06 FP099 - 0.06 NumDblBonds
##            + 0.09 FP043 + 0.02 HydrophilicFactor
## 
##   Rule 5: [14 cases, mean -2.442, range -4.16 to -0.02, est err 0.464]
## 
##     if
##  FP043 &gt; 0
##  MolWeight &lt;= 5.890234
##  NumOxygen &gt; 0.6931472
##  HydrophilicFactor &gt; -0.6934879
##     then
##  outcome = 30.304 - 4.53 MolWeight - 7.87 NumNitrogen - 2.2 NumOxygen
## 
##   Rule 6: [235 cases, mean -2.041, range -6.02 to 1.22, est err 0.474]
## 
##     if
##  FP043 &lt;= 0
##  MolWeight &lt;= 5.890234
##  NumRotBonds &lt;= 1.386294
##  NumOxygen &gt; 0.6931472
##     then
##  outcome = 0.921 - 7.65 NumNonHBonds + 8.4 NumNonHAtoms - 6.07 NumAtoms
##            + 5.52 NumBonds + 2.18 NumRings + 1.33 NumOxygen
##            + 0.148 SurfaceArea1 + 0.39 NumHydrogen - 0.61 MolWeight
##            + 0.62 FP142 - 0.29 FP094 + 0.12 NumCarbon
##            + 0.11 HydrophilicFactor + 0.46 FP043 - 0.26 FP112
##            + 0.36 NumNitrogen - 0.021 SurfaceArea2 + 0.29 FP147
##            - 0.05 NumMultBonds + 0.2 FP099 - 0.2 NumDblBonds + 0.17 FP113
##            - 0.15 FP081 + 0.08 NumRotBonds - 0.14 FP172 - 0.1 FP065
##            - 0.14 FP048 + 0.2 NumHalogen + 0.12 FP028 + 0.07 FP066
##            + 0.07 FP202 + 0.06 FP046 + 0.07 FP109 + 0.08 FP131
##            + 0.05 FP088
## 
##   Rule 7: [215 cases, mean -1.813, range -7.42 to 1.58, est err 0.533]
## 
##     if
##  MolWeight &lt;= 5.890234
##  NumOxygen &lt;= 0.6931472
##  HydrophilicFactor &gt; -0.6934879
##     then
##  outcome = 1.419 - 12.83 NumNonHBonds + 20.16 NumNonHAtoms
##            - 9.47 NumAtoms + 6.53 NumBonds + 3.41 NumRings
##            - 1.79 MolWeight + 0.71 NumHydrogen + 0.96 NumOxygen
##            + 0.113 SurfaceArea1 - 0.14 NumMultBonds + 0.84 NumNitrogen
##            - 0.044 SurfaceArea2 + 0.49 FP099 + 0.62 FP142 - 0.3 FP081
##            - 0.34 NumDblBonds + 0.38 FP043 - 0.25 FP172 - 0.18 FP065
##            - 0.2 FP094 + 0.11 NumRotBonds + 0.06 NumAromaticBonds
##            - 0.16 FP112 - 0.19 FP048 + 0.3 NumHalogen + 0.17 FP147
##            + 0.13 FP113 + 0.05 NumCarbon + 0.11 FP088 + 0.15 FP028
##            + 0.1 FP102 + 0.09 FP202 + 0.09 FP109 - 0.07 FP066
##            + 0.08 FP101 + 0.09 FP173 - 0.03 HydrophilicFactor
##            - 0.09 FP141 + 0.08 FP053
## 
## 
## Evaluation on training data (951 cases):
## 
##     Average  |error|              0.495
##     Relative |error|               0.31
##     Correlation coefficient        0.95
## 
## 
##  Attribute usage:
##    Conds  Model
## 
##    100%   100%    MolWeight
##     58%    91%    HydrophilicFactor
##     56%    98%    NumOxygen
##     36%    55%    FP043
##     34%    91%    NumRotBonds
##      7%    99%    SurfaceArea2
##            99%    FP142
##            99%    NumAtoms
##            99%    NumNonHAtoms
##            99%    NumBonds
##            99%    NumNonHBonds
##            99%    NumHydrogen
##            99%    NumCarbon
##            99%    NumRings
##            99%    SurfaceArea1
##            96%    FP099
##            96%    NumDblBonds
##            87%    NumMultBonds
##            87%    NumHalogen
##            85%    FP065
##            85%    FP066
##            85%    FP081
##            85%    FP088
##            85%    FP094
##            85%    FP172
##            80%    FP112
##            64%    NumAromaticBonds
##            64%    FP046
##            62%    FP053
##            62%    FP101
##            62%    FP102
##            62%    FP173
##            57%    FP141
##            49%    FP147
##            45%    NumNitrogen
##            43%    FP028
##            43%    FP048
##            43%    FP109
##            43%    FP113
##            43%    FP202
##            41%    FP080
##            41%    FP092
##            41%    FP116
##            36%    FP168
##            36%    NumChlorine
##            23%    FP131
##             5%    FP003
##             5%    FP071
##             5%    FP082
##             5%    FP085
## 
## 
## Time: 0.4 secs</code></pre>
</div>
<div id="chapter-10-case-study-compressive-strength-of-concrete-mixtures" class="section level3">
<h3>Chapter 10: Case Study: Compressive Strength of Concrete Mixtures</h3>
<pre class="r"><code>rm(list = ls())
library(AppliedPredictiveModeling)
data(concrete)
str(concrete)</code></pre>
<pre><code>## &#39;data.frame&#39;:    1030 obs. of  9 variables:
##  $ Cement             : num  540 540 332 332 199 ...
##  $ BlastFurnaceSlag   : num  0 0 142 142 132 ...
##  $ FlyAsh             : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ Water              : num  162 162 228 228 192 228 228 228 228 228 ...
##  $ Superplasticizer   : num  2.5 2.5 0 0 0 0 0 0 0 0 ...
##  $ CoarseAggregate    : num  1040 1055 932 932 978 ...
##  $ FineAggregate      : num  676 676 594 594 826 ...
##  $ Age                : int  28 28 270 365 360 90 365 28 28 28 ...
##  $ CompressiveStrength: num  80 61.9 40.3 41 44.3 ...</code></pre>
<pre class="r"><code>str(mixtures)</code></pre>
<pre><code>## &#39;data.frame&#39;:    1030 obs. of  9 variables:
##  $ Cement             : num  0.2231 0.2217 0.1492 0.1492 0.0853 ...
##  $ BlastFurnaceSlag   : num  0 0 0.0639 0.0639 0.0569 ...
##  $ FlyAsh             : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ Water              : num  0.0669 0.0665 0.1023 0.1023 0.0825 ...
##  $ Superplasticizer   : num  0.00103 0.00103 0 0 0 ...
##  $ CoarseAggregate    : num  0.43 0.433 0.418 0.418 0.42 ...
##  $ FineAggregate      : num  0.279 0.278 0.266 0.266 0.355 ...
##  $ Age                : int  28 28 270 365 360 90 365 28 28 28 ...
##  $ CompressiveStrength: num  80 61.9 40.3 41 44.3 ...</code></pre>
<pre class="r"><code># feature plot from caret
library(caret)
featurePlot(x = concrete[,-9],
            y = concrete[,9],
            between = list(x=1, y=1),  # space between panels
            type = c(&quot;g&quot;, &quot;p&quot;, &quot;smooth&quot;))  # add background grid and smoother</code></pre>
<p><img src="regression-models_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code># calc average for replicated mixtures
library(plyr)
averaged &lt;- ddply(mixtures,
                  .(Cement, BlastFurnaceSlag, FlyAsh, Water,
                    Superplasticizer, CoarseAggregate,
                    FineAggregate, Age),
                  function(x) c(CompressiveStrength =
                                  mean(x$CompressiveStrength)))
head(averaged)</code></pre>
<pre><code>##       Cement BlastFurnaceSlag FlyAsh      Water Superplasticizer
## 1 0.22309440       0.00000000      0 0.06692832      0.001032844
## 2 0.22172039       0.00000000      0 0.06651612      0.001026483
## 3 0.14917003       0.06393001      0 0.10228802      0.000000000
## 4 0.14917003       0.06393001      0 0.10228802      0.000000000
## 5 0.08534961       0.05689974      0 0.08251322      0.000000000
## 6 0.12036199       0.05158371      0 0.10316742      0.000000000
##   CoarseAggregate FineAggregate Age CompressiveStrength
## 1       0.4296633     0.2792811  28               79.99
## 2       0.4331759     0.2775611  28               61.89
## 3       0.4181247     0.2664872 270               40.27
## 4       0.4181247     0.2664872 365               41.05
## 5       0.4204736     0.3547638 360               44.30
## 6       0.4217195     0.3031674  90               47.03</code></pre>
<pre class="r"><code># split data into training and test
set.seed(975)
forTraining &lt;- createDataPartition(averaged$CompressiveStrength,
                                   p = 3/4)[[1]]
trainingSet &lt;- averaged[forTraining,]
testSet &lt;- averaged[-forTraining,]

# (.)^2 takes all linear terms and all two-factor interactions
modFormula &lt;- paste(&quot;CompressiveStrength ~ (.)^2 + I(Cement^2) + &quot;,
                    &quot;I(BlastFurnaceSlag^2) + I(FlyAsh^2) + I(Water^2) +&quot;,
                    &quot;I(Superplasticizer^2) + I(Age^2)&quot;)
modFormula &lt;- as.formula(modFormula)
controlObject &lt;- trainControl(method = &quot;repeatedcv&quot;,
                              repeats = 5,
                              number = 10)

set.seed(669)
linearReg &lt;- train(modFormula,
                   data = trainingSet,
                   method = &quot;lm&quot;,
                   trControl = controlObject)
linearReg</code></pre>
<pre><code>## Linear Regression 
## 
## 745 samples
##   8 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 669, 671, 670, 671, 670, 670, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   7.793733  0.7729528  5.918212
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<pre class="r"><code>set.seed(669)
plsModel &lt;- train(modFormula, data = trainingSet,
                  method = &quot;pls&quot;,
                  preProc = c(&quot;center&quot;, &quot;scale&quot;),
                  tuneLength = 15,
                  trControl = controlObject)
plsModel</code></pre>
<pre><code>## Partial Least Squares 
## 
## 745 samples
##   8 predictor
## 
## Pre-processing: centered (42), scaled (42) 
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 669, 671, 670, 671, 670, 670, ... 
## Resampling results across tuning parameters:
## 
##   ncomp  RMSE       Rsquared   MAE     
##    1     10.704021  0.5765400  8.517246
##    2      9.812178  0.6425429  7.606679
##    3      9.187612  0.6839415  7.201155
##    4      8.809442  0.7084070  6.847143
##    5      8.620088  0.7219521  6.683140
##    6      8.523829  0.7291877  6.663404
##    7      8.380743  0.7378897  6.498935
##    8      8.166743  0.7513122  6.316288
##    9      8.007674  0.7598027  6.103489
##   10      7.878724  0.7677181  6.028908
##   11      7.815858  0.7715853  5.936411
##   12      7.803655  0.7723765  5.891998
##   13      7.785938  0.7733984  5.873177
##   14      7.796253  0.7728950  5.910784
##   15      7.789112  0.7728680  5.911448
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was ncomp = 13.</code></pre>
<pre class="r"><code>enetGrid &lt;- expand.grid(.lambda = c(0, 0.001, 0.01, 0.1),
                        .fraction = seq(0.05, 1, length = 20))
set.seed(669)
enetModel &lt;- train(modFormula, data = trainingSet,
                   method = &quot;enet&quot;,
                   preProc = c(&quot;center&quot;, &quot;scale&quot;),
                   tuneGrid = enetGrid,
                   trControl = controlObject)

allResamples &lt;- resamples(list(&quot;Linear Reg&quot; = linearReg,
                               &quot;PLS&quot; = plsModel,
                               &quot;Elastic Net&quot; = enetModel))
parallelplot(allResamples)</code></pre>
<p><img src="regression-models_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
<pre class="r"><code>parallelplot(allResamples, metric = &quot;Rsquared&quot;)</code></pre>
<p><img src="regression-models_files/figure-html/unnamed-chunk-2-3.png" width="672" /></p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
