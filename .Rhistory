date_end
date_start <- "2010-01-01"
tq_transmute_fun_options() %>% str()
?ClCl
stock_prices <- c("MA", "V", "PYPL") %>%
tq_get(get = "stock.prices",
from = "2015-01-01",
to = "2016-12-31") %>%
group_by(symbol)
stock_prices
stock_pairs <- stock_prices %>%
tq_transmute(select = adjusted,
mutate_fun = periodReturn,
period = "daily",
type = "log",
col_rename = "returns") %>%
spread(key = symbol, value = returns)
stock_pairs
stock_prices
stock_prices %>% filter(symbol == "PYPL")
stock_pairs
stock_prices
log(85.3) - log(86.7)
log(85.3-86.7)
log(86.7/85.3)
stock_pairs %>%
ggplot(aes(x = V, y = MA)) +
geom_point(color = palette_light()[[1]], alpha = 0.5) +
geom_smooth(method = "lm") +
labs(title = 'Visualizing returns relationship of Visa and Mastercard') +
theme_tq()
lm(MA ~ V, data = stock_pairs) %>% summary
stock_prices <- c("MA", "V", "PYPL") %>%
tq_get(get = "stock.prices",
from = "2015-01-01",
to = "2016-12-31")
tq_index("SP500")  # company list from marketvolume.com
stock_list <- tq_index("SP500")  # company list from marketvolume.com
stock_prices <- stock_list %>%
tq_get(get = "stock.prices",
from = "2015-01-01",
to = "2016-12-31")
stock_prices
install.packages(c("choroplethr", "choroplethrMaps", "data.table",
"datasets", "devtools", "dlnm", "dplyr", "faraway", "forcats",
"GGally", "ggmap", "ggplot2", "ggthemes", "ghit", "GISTools",
"grid", "gridExtra", "httr", "knitr", "leaflet", "lubridate",
"magrittr", "methods", "microbenchmark", "package", "pander",
"plotly", "profvis", "pryr", "purrr", "rappdirs", "raster",
"RColorBrewer", "readr", "rmarkdown", "scales", "sp", "stats",
"stringr", "testthat", "tidyr", "tidyverse", "tigris", "titanic",
"viridis"))
install.packages(c("choroplethr", "choroplethrMaps", "data.table", "datasets", "devtools", "dlnm", "dplyr", "faraway", "forcats", "GGally", "ggmap", "ggplot2", "ggthemes", "ghit", "GISTools", "grid", "gridExtra", "httr", "knitr", "leaflet", "lubridate", "magrittr", "methods", "microbenchmark", "package", "pander", "plotly", "profvis", "pryr", "purrr", "rappdirs", "raster", "RColorBrewer", "readr", "rmarkdown", "scales", "sp", "stats", "stringr", "testthat", "tidyr", "tidyverse", "tigris", "titanic", "viridis"))
install.packages(c("choroplethr", "choroplethrMaps", "data.table", "datasets", "devtools", "dlnm", "dplyr", "faraway", "forcats", "GGally", "ggmap", "ggplot2", "ggthemes", "ghit", "GISTools", "grid", "gridExtra", "httr", "knitr", "leaflet", "lubridate", "magrittr", "methods", "microbenchmark", "package", "pander", "plotly", "profvis", "pryr", "purrr", "rappdirs", "raster", "RColorBrewer", "readr", "rmarkdown", "scales", "sp", "stats", "stringr", "testthat", "tidyr", "tidyverse", "tigris", "titanic", "viridis"))
install.packages(c("bookdown", "dbplyr", "rlang", "tidyquant", "tidytext", "timeDate"))
vector("numeric", length = 10)
class(0:6)
as.numeric(0:6)
classs(as.numeric(0:6))
class(as.numeric(0:6))
class(as.logical(0:6))  # coerce class
as.logical(0:6)  # coerce class
m <- matrix(nrow = 2, ncol = 3)
m
dim(m)
attribute(m)
attributes(m)
matrix(1:6, nrow = 2)
dim(1:10) <- c(2,5)
x
x <- factor(c("yes", "yes", "no", "yes", "no"))
x
table(x)
unclass(x)
factor(c("yes", "yes", "no", "yes", "no"), levels = c("yes", "no"))
x <- c(1, 2, NaN, NA, 4)
is.na(x)
is.nan(x)
VADeaths
# transforming data into tidy data
require(tidyr)
require(dplyr)
VADeaths
VADeaths %>% tbl_df()
VADeaths %>%
tbl_df() %>%
mutate(age = row.names(VADeaths)) %>%
gather(key, death_rate, -age) %>%
separate(key, c("urban", "gender"), sep = " ") %>%
mutate(age = factor(age), urban = factor(urban), gender = factor(gender))
VADeaths
VADeaths_tidy <- VADeaths %>%
tbl_df() %>%
mutate(age = row.names(VADeaths)) %>%
gather(key, death_rate, -age) %>%
separate(key, c("urban", "gender"), sep = " ") %>%
mutate(age = factor(age), urban = factor(urban), gender = factor(gender))
# transforming data into tidy data
require(tidyr)
require(dplyr)
VADeaths_tidy <- VADeaths %>%
tbl_df() %>%
mutate(age = row.names(VADeaths)) %>%
gather(key, death_rate, -age) %>%
separate(key, c("urban", "gender"), sep = " ") %>%
mutate(age = factor(age), urban = factor(urban), gender = factor(gender))
VADeaths_tidy
VADeaths %>%
summarise(death_rate = n())
VADeaths_tidy %>%
summarise(death_rate = n())
VADeaths_tidy %>%
summarise(death_rate = n(),
lowest_rate = min(death_rate))
VADeaths_tidy %>%
summarise(death_rate = n(),
highest_rate = max(death_rate))
VADeaths_tidy %>%
summarise(n_obs = n(),
highest_rate = max(death_rate))
VADeaths_tidy %>%
summarise(n_obs = n(),
highest_death_rate = max(death_rate))
VADeaths_tidy
VADeaths_tidy %>%
group_by(gender) %>%
head()
install.packages("sourcetools")
install.packages("sf")
install.packages("tmap")
install.packages("sf")
install.packages("sf")
install.packages("tmap")
install.packages(c("DBI", "httpuv", "later", "mapview", "modelr", "processx", "psych", "rgdal", "sf", "sourcetools", "stringi", "stringr", "yaml"))
install.packages(c("callr", "dplyr", "httpuv", "processx", "Rcpp", "sf", "shiny", "stringi", "tidyr"))
install.packages(c("dplyr", "sf", "shiny", "stringi", "tidyr"))
install.packages("dplyr")
install.packages("earth")
knitr::opts_chunk$set(cache=TRUE)
require(earth)
# using train function, method = "nnet" or method = "avNNet"
data(Solubility)
# using train function, method = "nnet" or method = "avNNet"
data(solubility)
require(caret)
require(earth)
require(kernlab)
require(nnet)
# using train function, method = "nnet" or method = "avNNet"
data(solubility)
require(AppliedPredictiveModeling)
# using train function, method = "nnet" or method = "avNNet"
data(solubility)
tooHigh <- findCorrelation(cor(solTrainXtrans), cutoff = 0.75)
tooHigh <- findCorrelation(cor(solTrainXtrans), cutoff = 0.75)
trainXnnet <- solTrainXtrans[, -tooHigh]
testXnnet <- solTestXtrans[, -tooHigh]
nnetGrid <- expand.grid(.decay = c(0, 0.01, 0.1),
.size = c(1:10),
.bag = FALSE)
nnetGrid
# using train function, method = "nnet" or method = "avNNet"
data(solubility)
tooHigh <- findCorrelation(cor(solTrainXtrans), cutoff = 0.75)
trainXnnet <- solTrainXtrans[, -tooHigh]
testXnnet <- solTestXtrans[, -tooHigh]
nnetGrid <- expand.grid(.decay = c(0, 0.01, 0.1),
.size = c(1:10),
.bag = FALSE)
ctrl <- trainControl(method = "cv", number = 10)
set.seed(100)
nnetTune <- train(trainXnnet, solTrainY,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = ctrl,
preProcess = c("center", "scale"),
linout = TRUE,
trace = FALSE,
MaxNWts = 10 * (ncol(trainXnnet) + 1) + 10 + 1,
maxit = 500)
install.packages("MARS")
### Multivariate Adaptive Regression Splines ###
require(MARS)
### Multivariate Adaptive Regression Splines ###
marsFit <- earth(solTrainXtrans, solTrainY)
marsFit
plotmo(marsfit)
plotmo(marsFit)
# using train function
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)
set.seed(100)
marsTuned <- train(solTrainXtrans, solTrainY,
method = "earth",
tuneGrid = marsGrid,
trControl = trainControl(method = "cv"))
evimp(marsFit)
# using kernlab
svmFit <- ksvm(x = solTrainXtrans, y = solTrainY,
kernel = "rbfdot", kpar= "automatic",
C = 1, epsilon = 0.1)
# using kernlab
svmFit <- ksvm(x = solTrainXtrans, y = solTrainY,
kernel = "stringdot", kpar= "automatic",
C = 1, epsilon = 0.1)
?ksvm
# using kernlab
svmFit <- ksvm(x = solTrainXtrans, y = solTrainY,
kernel ="rbfdot", kpar = "automatic",
C = 1, epsilon = 0.1)
# using kernlab
trainingData <- solTrainXtrans
trainingData$Solubility <- solTrainY
svmFit <- ksvm(Solubility ~., data = trainingData,
kernel ="rbfdot", kpar = "automatic",
C = 1, epsilon = 0.1)
svmFit
# using train function
svmRTuned <- train(solTrainXtrans, solTrainY,
method = "svmRadial",
preProcess = c("center", "scale"),
tuneLength = 14,
trControl = trainControl(method = "cv"))
# using the train function from caret
knnDescr <- solTrainXtrans[, -nearZeroVar(solTrainXtrans)]  # remove sparse and unbalanced fingerprints
knnTune <- train(knnDescr,
solTrainY,
method = "knn",
preProcess = c("center", "scale"),
tuneGrid = data.frame(.k = 1:20),
trControl = trainControl(method = "cv"))
# new data will be auto centered and scaled when using knn model object for prediction
knnTune
knitr::opts_chunk$set(cache=TRUE)
rm(list=ls())
observed <-  c(0.22, 0.83,-0.12, 0.89,-0.23,-1.30,-0.15,-1.4,
0.62, 0.99,-0.18, 0.32, 0.34,-0.30, 0.04,-0.87,
0.55,-1.30,-1.15, 0.20)
predicted <- c(0.24, 0.78,-0.66, 0.53, 0.70,-0.75,-0.41,-0.43,
0.49, 0.79,-1.19, 0.06, 0.75,-0.07, 0.43,-0.42,
-0.25,-0.64,-1.26,-0.07)
residualValues <- observed - predicted
summary(residualValues)
# observed versus predicted
axisRange <- extendrange(c(observed, predicted))
plot(observed, predicted, ylim = axisRange, xlim = axisRange)
abline(0, 1, col = "darkgrey", lty = 2)
# predicted versus residuals
plot(predicted, residualValues, ylab = "residual")
abline(h = 0, col = "darkgrey", lty = 2)
# quantitative model performance measures
require(caret)
# quantitative model performance measures
require(caret)
R2(predicted, observed)
?R2
str(predicted)
str(observed)
RMSE(predicted, observed)
cor(predicted, observed)  # base R simple correlation
cor(predicted, observed)^2  # match R^2
cor(predicted, observed, method = "spearman")  # rank correlation
knitr::opts_chunk$set(cache=TRUE)
library(caret)
install.packages("Cubist")
install.packages("gbm")
install.packages("party")
install.packages("partykit")
install.packages("randomForest")
install.packages("RWeka")
library(caret)
library(Cubist)
library(gbm)
library(ipred)
library(party)
library(partykit)
library(randomForest)
library(rpart)
library(RWeka)
data(Solubility)
data(solubility)
library(AppliedPredictiveModeling)
data(solubility)
trainingData <- solTrainXtrans
trainingData$Solubility <- solTrainY
# rpart uses CART
set.seed(100)
# rpart uses CART
set.seed(100)
rpartTune <- train(solTrainXtrans, solTrainY,
method = "rpart2",  # use rpart to tune over complexity parameter, rpart2 tune over maxdepth
tuneLength = 10,
trControl = trainControl(method= "cv"))
plot(rpartTune)
m5Tune <- train(solTrainXtrans, solTrainY,
method = "M5",
trControl = trainControl(method = "cv"),
control =  Weka_control(M = 10))
# randomForest
rfModel <- randomForest(solTrainXtrans, solTrainY)
# randomForest
rfModel <- randomForest(solTrainXtrans, solTrainY,
importance = TRUE,
ntrees = 1000)
importance(rfModel)
varImp(rfModel)
varImp(rfModel)
# gbm
gbmModel <- gbm.fit(solTrainXtrans, solTrainY, distribution = "gaussian")
# tuning with caret
gbmGrid <- expand.grid(.interaction.depth = seq(1, 7, by = 2),
.n.trees = seq(100, 1000, by = 50),
.shrinkage = c(0.01, 0.1))
set.seed(100)
gbmTune <- train(solTrainXtrans, solTrainY,
method = "gbm",
tuneGrid = gbmGrid,
verbose = FALSE)
cubistMod <- cubist(solTrainXtrans, solTrainY)
predict(cubistMod, solTestXtrans)
# predict(cubistMod, solTestXtrans)  # use neighbors agument to pass 0-9 integer to adjust predictions
summary(cubistMod)
head(varImp(rfModel))  # caret wrapper for variable importance
varImp(rfModel)
head(varImp(rfModel))
head(varImp(rfModel))  # caret wrapper for variable importance
head(varImp(rfModel))
rm(list = ls())
library(AppliedPredictiveModeling)
data(concrete)
str(concrete)
str(mixtures)
library(caret)
featurePlot(x = concrete[,-9],
y = concertee[,9],
between = list(x=1, y=1),
type = c("g", "p", "smooth"))
featurePlot(x = concrete[,-9],
y = concerte[,9],
between = list(x=1, y=1),
type = c("g", "p", "smooth"))
featurePlot(x = concrete[,-9],
y = concrete[,9],
between = list(x=1, y=1),
type = c("g", "p", "smooth"))
??ddplyr
?ddply
library(plyr)
averaged <- ddply(mixtures,
.(Cement, BlastFurnaceSlag, FlyAsh, Water,
Superplasticizer, CoarseAggregate,
FineAggregate, Age),
function(x) c(CompressiveStrength =
mean(x$CompressiveStrength)))
averaged
averaged %>% head
head(averaged)
head(mixtures)
forTraining <- createDataPartition(averaged$CompressiveStrength,
p = 3/4)[[1]]
head(forTraining)
head(averaged)
head(averaged)
# split data into training and test
set.seed(975)
forTraining <- createDataPartition(averaged$CompressiveStrength,
p = 3/4)[[1]]
trainingSet <- averaged[forTraining,]
testSet <- averaged[-forTraining,]
knitr::opts_chunk$set(cache=TRUE)
library(AppliedPredictiveModeling)
setwd("C:\\Users\\pistachio\\Projects\\apm")
set.seed(975)
simulatedTrain <- quadBoundaryFunc(500)
simulatedTest <- quadBoundaryFunc(1000)
head(simulatedTrain)
?quadBoundaryFunc
library(randomForest)
rfModel <- randomForest(class ~ X1 + X2,
data = simulatedTrain,
ntree = 2000)
qdaModel <- qda(class ~ X1 + X2, data = simulatedTrain)
library(MASS)
qdaModel <- qda(class ~ X1 + X2, data = simulatedTrain)
qdaTrainPred <- predict(qdaModel, simulatedTrain)
names(qdaTrainPred)
head(qdaTrainPred$class)
head(qdaTrainPred$posterior)
qdaTestPred <- predict(qdaModel, simulatedTest)
simulatedTest$RFprob <- qdaTestPred$posterior
simulatedTest$RFclass <- qdaTestPred$class
head(simulatedTest)
set.seed(975)
simulatedTrain <- quadBoundaryFunc(500)
simulatedTest <- quadBoundaryFunc(1000)
head(simulatedTrain)
simulatedTest$RFprob <- qdaTestPred$posterior
qdaTrainPred <- predict(qdaModel, simulatedTrain)
qdaTestPred <- predict(qdaModel, simulatedTest)
simulatedTest$RFprob <- qdaTestPred$posterior
simulatedTest$RFclass <- qdaTestPred$class
head(simulatedTest)
set.seed(975)
simulatedTrain <- quadBoundaryFunc(500)
simulatedTest <- quadBoundaryFunc(1000)
head(simulatedTrain)
library(randomForest)
rfModel <- randomForest(class ~ X1 + X2,
data = simulatedTrain,
ntree = 2000)
library(MASS)
qdaModel <- qda(class ~ X1 + X2, data = simulatedTrain)  # quadratic discriminant model
qdaTrainPred <- predict(qdaModel, simulatedTrain)
names(qdaTrainPred)  # posterior contains class probs
head(qdaTrainPred$class)
head(qdaTrainPred$posterior)
qdaTestPred <- predict(qdaModel, simulatedTest)
simulatedTest$RFprob <- qdaTestPred$posterior
simulatedTest$RFclass <- qdaTestPred$class
simulatedTest
head(simulatedTest)
library(AppliedPredictiveModeling)
set.seed(975)
simulatedTrain <- quadBoundaryFunc(500)
simulatedTest <- quadBoundaryFunc(1000)
head(simulatedTrain)
library(randomForest)
rfModel <- randomForest(class ~ X1 + X2,
data = simulatedTrain,
ntree = 2000)
library(MASS)
qdaModel <- qda(class ~ X1 + X2, data = simulatedTrain)  # quadratic discriminant model
qdaTrainPred <- predict(qdaModel, simulatedTrain)
names(qdaTrainPred)  # posterior contains class probs
head(qdaTrainPred$class)
head(qdaTrainPred$posterior)
qdaTestPred <- predict(qdaModel, simulatedTest)
simulatedTrain$QDAprob <- qdaTrainPred$posterior[,"Class1"]
simulatedTest$QDAprob <- qdaTestPred$posterior[,"Class1"]
simulatedTest
head(simulatedTest)
rfTestPred <- predict(rfModel, simulatedTest, type = "prob")
head(rfTestPred)
simulatedTest$RFprob <- rfTestPred[,"Class1"]
simulatedTest$RFclass <- predict(rfModel, simulatedTest)  # needs separate predict call
library(caret)
sensitivity(data = simulatedTest$RFclass,
reference = simulatedTest$class,
positive = "Class1")  # assume Class1 is the event of interest
specificity(data = simulatedTest$RFclass,
reference = simulatedTest$class,
positive = "Class2")  # assume Class2 is the non-event
posPredValue(data = simulatedTest$RFclass,
reference = simulatedTest$class,
positive = "Class1")  # PPV, prevalance computed from data
negPredValue(data = simulatedTest$RFclass,
reference = simulatedTest$class,
positive = "Class2")  # NPV
posPredValue(data = simulatedTest$RFclass,
reference = simulatedTest$class,
positive = "Class1",
prevalence = 0.9)  # PPV, manually set prevalence
confusionMatrix(data = simulatedTest$RFclass,
reference = simulatedTest$class,
positive = "Class1")  # PPV, prevalance computed from data)
install.packages("pROC")
library(pROC)
rocCurve <- roc(response = simulatedTest$class,
predictor = simulatedTest$RFprob,
levels = rev(levels(simulatedTest$class)))  # defaults assumes second class is event, need to reverse
auc(rocCurve)
ci.roc(rocCurve)
?auc
ci.auc(rocCurve)
plot(rocCurve, legacy.axes = TRUE)
plot(rocCurve)
plot(rocCurve, legacy.axes = TRUE)
# use lift function from caret packages
labs <- c(RFprob = "Random Forest",
QDAprob = "Quadratic Disriminant Analysis")
liftCurve <- lift(class ~ RFprob + QDAprob, data = simulatedTest, labels = labs)
liftCurve
xyplot(liftCurve,
auto.key = list(columns = 2, lines = TRUE, points = FALSE))
install.packages("PresenceAbsence")
library(PresenceAbsence)
# calibration from caret package
calCurve <- calibration(class ~ RFprob + QDAprob, data = simulatedTest)  # syntax similar to lift
calCurve
xyplot(calCurve, auto.key = list(columns = 2))
# use glm sigmoid to calibrate probs
sigmoidalCal <- glm(relevel(class, ref = "Class2") ~ QDAprob,
data = simulatedTrain,
family = binomial)
coef(summary(sigmoidalCal))
sigmoidProbs <- predict(sigmoidalCal,
newdata = simulatedTest[, "QDAprob", drop = FALSE],
type = "response")
simulatedTest$QDAsigmoid <- sigmoidProbs
head(simulatedTest)
install.packages("klaR")
library(klaR)
BayesCal <- NaiveBayes(class ~ QDAprob, data = simulatedTrain,
usekernal = TRUE)
BayesProbs <- predict(BayesCal,
newdata = simulatedTest[, "QDAprob", drop = FALSE])
simulatedTest$QDABayes <- BayesProbs$posterior[,"Class1"]
head(simulatedTest[, c(5,6,8,9)])
calCurve2 <- calibration(class ~ QDAprob + QDABayes + QDAsigmoid
, data = simulatedTest)
calCurve2 <- calibration(class ~ QDAprob + QDABayes + QDAsigmoid,
data = simulatedTest)
xyplot(calCurve2)
