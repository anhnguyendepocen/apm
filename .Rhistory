cor(Cl(SPY)["2009-01-30/"], Cl(VXX))
plot(diff(log(Cl(VXX))))
cor(Ad(SPY)["2009-01-30/"], Ad(VXX))
plot(as.numeric(diff(log(Ad(SPY)["2009-01-30/"]))[-1]),
as.numeric(diff(log(Ad(VXX)))[-1]))
sv <- as.xts(merge(diff(log(Ad(SPY)["2009-01-30/"]))[-1],
diff(log(Cl(SPY)["2009-01-30/"]))[-1],
diff(log(Ad(VXX)))[-1],
diff(log(Cl(VXX)))[-1]))
head(sv)
reg <- lm(VXX.Adjusted ~ SPY.Adjusted, data=sv)
summary(reg)
par(mfrow = c(2, 2))
plot(reg$residuals,
main = "Residuals through time",
xlab = "Days", ylab = "Residuals")
hist(reg$residuals, breaks=100,
main = "Distribution of residuals",
xlab = "Residuals")
qqnorm(reg$residuals)
qqline(reg$residuals)
acf(reg$residuals, main="Autocorrelation")
vxx_lag_1 <- lag(Cl(VXX), k=1)
sv <- merge(sv, lag(sv))
plot(as.numeric(sv$SPY.Adjusted.1), as.numeric(sv$VXX.Adjusted),
main = "Scatter plot SPY lagged vs. VXX",
cex.main = 0.8,
cex.axis = 0.8,
cex.lab = 0.8)
grid()  # no discernible relationship
reg2 <- lm(VXX.Close ~ SPY.Close.1, data=sv)
summary(reg2)
ccf(as.numeric(sv$SPY.Adjusted), as.numeric(sv$VXX.Adjusted),
main = "Cross correlation between SPY and VXX")
z <- rnorm(1000)
par(mfrow=c(2,1))  # corr does not exist with iid var
acf(z); grid(); acf(z^2); grid()
par(mfrow=c(1,1))  # corr does exist with real returns
acf(sv$SPY.Adjusted ^ 2); grid()
par(mfrow=c(1,2))  # corr exists with other fns too
acf(sv$SPY.Adjusted ^ 3)
acf(abs(sv$SPY.Adjusted))
rm(list=ls())
install.packages("twitteR")
consumer_key <- "	0cqk2Wu5mtmLJymPnwYi44v4o"
consumer_secret <- "uCdAT0kuZATBnKXAk8MW2BknIFFcTgl2iW3mrQx35En3CvmXww"
access_token <- "14456397-V0ADcYqp2bWBXoDpohCMEcvFVamP0GBeKhKeagmRg"
access_secret <- "YcC0EKv9nUu5l6OFeolGQcoDCQ5Pbyk44hX1D0NXGcahd"
library(twitteR)
origop <- options("httr_oauth_cache")
options(httr_oauth_cache = TRUE)
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
consumer_key <- "0cqk2Wu5mtmLJymPnwYi44v4o"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
options(httr_oauth_cache = origop)
stock_tweets <- searchTwitter("AAPL")
stock_tweets <- twListToDF(stock_tweets)
stock_tweets <- searchTwitter("AAPL") %>% twListToDF()
library(tidyverse)
stock_tweets <- searchTwitter("AAPL") %>% twListToDF()
head(stock_tweets)
head(stock_tweets)
library(tidytext)
install.packages("tidytext")
library(tidytext)
install.packages("quantstrat")
install.packages("quantstrat", repos="http://R-Forge.R-project.org")
install.packages("quantstrat", repos="http://R-Forge.R-project.org")
library(quantstrat)
install.packages("C:\\Users\\pistachio\\Downloads\\quantstrat_0.9.1739.zip", repos=NULL, type="source")
install.packages("C:\\Users\\pistachio\\Downloads\\blotter_0.9.1741.zip", repos=NULL, type="source")
library(quantstrat)
library(blotter)
library(quantstrat)
library(blotter)
rm(list=ls(.blotter), envir=.blotter)
currency("USD")
Sys.setenv(TZ="UTC")
symbols <- c("XLB", #SPDR Materials sector
"XLE", #SPDR Energy sector
"XLF", #SPDR Financial sector
"XLP", #SPDR Consumer staples sector
"XLI", #SPDR Industrial sector
"XLU", #SPDR Utilities sector
"XLV", #SPDR Healthcare sector
"XLK", #SPDR Tech sector
"XLY", #SPDR Consumer discretionary sector
"RWR", #SPDR Dow Jones REIT ETF
"EWJ", #iShares Japan
"EWG", #iShares Germany
"EWU", #iShares UK
"EWC", #iShares Canada
"EWY", #iShares South Korea
"EWA", #iShares Australia
"EWH", #iShares Hong Kong
"EWS", #iShares Singapore
"IYZ", #iShares U.S. Telecom
"EZU", #iShares MSCI EMU ETF
"IYR", #iShares U.S. Real Estate
"EWT", #iShares Taiwan
"EWZ", #iShares Brazil
"EFA", #iShares EAFE
"IGE", #iShares North American Natural Resources
"EPP", #iShares Pacific Ex Japan
"LQD", #iShares Investment Grade Corporate Bonds
"SHY", #iShares 1-3 year TBonds
"IEF", #iShares 3-7 year TBonds
"TLT" #iShares 20+ year Bonds
)
require(quantstrat)
require(PerformanceAnalytics)
initDate <- "1990-01-01"
from <- "2003-01-01"
to <- "2013-12-31"
options(width = 70)
if(!"XLB" %in% ls()) {
suppressMessages(getSymbols(symbols,
from=from, to=to,
src="yahoo", adjust=TRUE))
}
install.packages(c("jsonlite", "tibble", "tseries"))
install.packages("testthat")
plot(cars)
library(mailR)
taskscheduleR:::taskschedulerAddin()
install.packages(c("backports", "broom", "curl", "data.table", "dendextend", "devtools", "digest", "FinancialInstrument", "foreach", "fpc", "git2r", "glue", "hexbin", "hms", "hunspell", "iterators", "knitr", "lazyeval", "lubridate", "mclust", "openssl", "padr", "quantmod", "R.utils", "Rcpp", "RcppArmadillo", "RCurl", "registry", "reshape2", "rlang", "rmarkdown", "robustbase", "rprojroot", "stringi", "taskscheduleR", "testthat", "tibble", "tidyselect", "tidytext", "tidyverse", "timeDate", "timeSeries", "viridis", "withr", "xts", "yaml", "zoo"))
install.packages(c("callr", "digest", "knitr", "lubridate", "mvtnorm", "openssl", "seriation", "tidyr", "viridis", "viridisLite", "xml2"))
library(quantmod)
library(tidyverse)
library(magrittr)
library(quantmod)
library(rvest)
library(tidyverse)
library(stringr)
library(lubridate)
setwd("C:\\Users\\pistachio\\Projects\\stock-analysis\\P001-large-changes-2")
library(magrittr)
library(quantmod)
library(rvest)
library(tidyverse)
library(stringr)
library(lubridate)
library(magrittr)
library(tidyquant)
install.packages(c("BH", "callr", "dendextend", "tseries"))
tq_index_options()
tq_index("SP500")
tq_exchange_options()
tq_get_options()
tq_index("SP500")  # company list from marketvolume.com
date_end <- Sys.Date()
date_end
date_start <- "2010-01-01"
tq_transmute_fun_options() %>% str()
?ClCl
stock_prices <- c("MA", "V", "PYPL") %>%
tq_get(get = "stock.prices",
from = "2015-01-01",
to = "2016-12-31") %>%
group_by(symbol)
stock_prices
stock_pairs <- stock_prices %>%
tq_transmute(select = adjusted,
mutate_fun = periodReturn,
period = "daily",
type = "log",
col_rename = "returns") %>%
spread(key = symbol, value = returns)
stock_pairs
stock_prices
stock_prices %>% filter(symbol == "PYPL")
stock_pairs
stock_prices
log(85.3) - log(86.7)
log(85.3-86.7)
log(86.7/85.3)
stock_pairs %>%
ggplot(aes(x = V, y = MA)) +
geom_point(color = palette_light()[[1]], alpha = 0.5) +
geom_smooth(method = "lm") +
labs(title = 'Visualizing returns relationship of Visa and Mastercard') +
theme_tq()
lm(MA ~ V, data = stock_pairs) %>% summary
stock_prices <- c("MA", "V", "PYPL") %>%
tq_get(get = "stock.prices",
from = "2015-01-01",
to = "2016-12-31")
tq_index("SP500")  # company list from marketvolume.com
stock_list <- tq_index("SP500")  # company list from marketvolume.com
stock_prices <- stock_list %>%
tq_get(get = "stock.prices",
from = "2015-01-01",
to = "2016-12-31")
stock_prices
install.packages(c("choroplethr", "choroplethrMaps", "data.table",
"datasets", "devtools", "dlnm", "dplyr", "faraway", "forcats",
"GGally", "ggmap", "ggplot2", "ggthemes", "ghit", "GISTools",
"grid", "gridExtra", "httr", "knitr", "leaflet", "lubridate",
"magrittr", "methods", "microbenchmark", "package", "pander",
"plotly", "profvis", "pryr", "purrr", "rappdirs", "raster",
"RColorBrewer", "readr", "rmarkdown", "scales", "sp", "stats",
"stringr", "testthat", "tidyr", "tidyverse", "tigris", "titanic",
"viridis"))
install.packages(c("choroplethr", "choroplethrMaps", "data.table", "datasets", "devtools", "dlnm", "dplyr", "faraway", "forcats", "GGally", "ggmap", "ggplot2", "ggthemes", "ghit", "GISTools", "grid", "gridExtra", "httr", "knitr", "leaflet", "lubridate", "magrittr", "methods", "microbenchmark", "package", "pander", "plotly", "profvis", "pryr", "purrr", "rappdirs", "raster", "RColorBrewer", "readr", "rmarkdown", "scales", "sp", "stats", "stringr", "testthat", "tidyr", "tidyverse", "tigris", "titanic", "viridis"))
install.packages(c("choroplethr", "choroplethrMaps", "data.table", "datasets", "devtools", "dlnm", "dplyr", "faraway", "forcats", "GGally", "ggmap", "ggplot2", "ggthemes", "ghit", "GISTools", "grid", "gridExtra", "httr", "knitr", "leaflet", "lubridate", "magrittr", "methods", "microbenchmark", "package", "pander", "plotly", "profvis", "pryr", "purrr", "rappdirs", "raster", "RColorBrewer", "readr", "rmarkdown", "scales", "sp", "stats", "stringr", "testthat", "tidyr", "tidyverse", "tigris", "titanic", "viridis"))
install.packages(c("bookdown", "dbplyr", "rlang", "tidyquant", "tidytext", "timeDate"))
vector("numeric", length = 10)
class(0:6)
as.numeric(0:6)
classs(as.numeric(0:6))
class(as.numeric(0:6))
class(as.logical(0:6))  # coerce class
as.logical(0:6)  # coerce class
m <- matrix(nrow = 2, ncol = 3)
m
dim(m)
attribute(m)
attributes(m)
matrix(1:6, nrow = 2)
dim(1:10) <- c(2,5)
x
x <- factor(c("yes", "yes", "no", "yes", "no"))
x
table(x)
unclass(x)
factor(c("yes", "yes", "no", "yes", "no"), levels = c("yes", "no"))
x <- c(1, 2, NaN, NA, 4)
is.na(x)
is.nan(x)
VADeaths
# transforming data into tidy data
require(tidyr)
require(dplyr)
VADeaths
VADeaths %>% tbl_df()
VADeaths %>%
tbl_df() %>%
mutate(age = row.names(VADeaths)) %>%
gather(key, death_rate, -age) %>%
separate(key, c("urban", "gender"), sep = " ") %>%
mutate(age = factor(age), urban = factor(urban), gender = factor(gender))
VADeaths
VADeaths_tidy <- VADeaths %>%
tbl_df() %>%
mutate(age = row.names(VADeaths)) %>%
gather(key, death_rate, -age) %>%
separate(key, c("urban", "gender"), sep = " ") %>%
mutate(age = factor(age), urban = factor(urban), gender = factor(gender))
# transforming data into tidy data
require(tidyr)
require(dplyr)
VADeaths_tidy <- VADeaths %>%
tbl_df() %>%
mutate(age = row.names(VADeaths)) %>%
gather(key, death_rate, -age) %>%
separate(key, c("urban", "gender"), sep = " ") %>%
mutate(age = factor(age), urban = factor(urban), gender = factor(gender))
VADeaths_tidy
VADeaths %>%
summarise(death_rate = n())
VADeaths_tidy %>%
summarise(death_rate = n())
VADeaths_tidy %>%
summarise(death_rate = n(),
lowest_rate = min(death_rate))
VADeaths_tidy %>%
summarise(death_rate = n(),
highest_rate = max(death_rate))
VADeaths_tidy %>%
summarise(n_obs = n(),
highest_rate = max(death_rate))
VADeaths_tidy %>%
summarise(n_obs = n(),
highest_death_rate = max(death_rate))
VADeaths_tidy
VADeaths_tidy %>%
group_by(gender) %>%
head()
install.packages("sourcetools")
install.packages("sf")
install.packages("tmap")
install.packages("sf")
install.packages("sf")
install.packages("tmap")
install.packages(c("DBI", "httpuv", "later", "mapview", "modelr", "processx", "psych", "rgdal", "sf", "sourcetools", "stringi", "stringr", "yaml"))
install.packages(c("callr", "dplyr", "httpuv", "processx", "Rcpp", "sf", "shiny", "stringi", "tidyr"))
install.packages(c("dplyr", "sf", "shiny", "stringi", "tidyr"))
install.packages("dplyr")
knitr::opts_chunk$set(cache=TRUE)
require(AppliedPredictiveModeling)
require(elasticnet)
require(lars)
require(caret)
require(caret)
require(MASS)
require(pls)
require(stats)
install.packages(elasticnet)
install.packages("elasticnet")
install.packages("lars")
install.packages("pls")
require(AppliedPredictiveModeling)
require(elasticnet)
require(lars)
require(caret)
require(MASS)
require(pls)
require(stats)
data(solubility)
rm(list=ls())
data(solubility)
ls(pattern = "^sol")
trainingData <- solTrainXtrans
trainingData$Solubility <- solTrainY
lmFitAllPredictors <- lm(Solubility ~., data = trainingData)
summary(lmFitAllPredictors)
lmPred1 <- predict(lmFitAllPredictors, solTestXtrans)
head(lmPred1)
head(summary(lmFitAllPredictors))
str(summary(lmFitAllPredictors))
lmValues1 <- data.frame(obs = solTestY, pred = lmPred1)
defaultSummary(lmValues1)  # caret metrics
# try robust lm from MASS (uses Huber approach)
ctrl <- trainControl(method = "cv", number = 10)
set.seed(100)
lmFit1 <- train(x = solTrainXtrans, y = solTrainY,
method = "lm", trControl = ctrl)
lmFit1
xyplot(solTrainY ~ predict(lmFit1),
# plot points and use background grid
type = c("p", "g"),
xlab = "Predicted", ylab = "Observed")
xyplot(resid(lmFit1 ~ predict(lmFit1),
type = c("p", "g"),
xlab = "Predicted", ylab = "Residuals"))
xyplot(resid(lmFit1) ~ predict(lmFit1),
type = c("p", "g"),
xlab = "Predicted", ylab = "Residuals")
# removing highly correlated vars first
corThresh <- 0.9
tooHigh <- findCorrelation(cor(solTrainXtrans), corThresh)
corrPred <- names(solTrainXtrans)[tooHigh]
trainXfiltered <- solTrainXtrans[, -tooHigh]
testXfiltered <- solTestXtrans[, -tooHigh]
lmFiltered <- train(trainXfiltered, solTrainY, method = "lm", trControl = ctrl)
summary(lmFiltered)
str(summary(lmFiltered))
lmFiltered
rlmPCA <- train(trainXfiltered, solTrainY, method = "rlm", preProcess = "pca", trControl = ctrl)
rlmPCA
### Partial Least Squares ###
plsFit <- plsr(Solubility ~., data = trainingData)
predict(plsFit, solTestXtrans[1:5,], ncomp = 1:2)
# using train function
plsTune <- train(solTrainXtrans, solTrainY,
method = "pls",
tuneLength = 20,  # default tuning grid evals 1:tuneLength
trControl = ctrl,
preProcess = c("center", "scale"))
plsTune
### Penalized Regression Models ###
# lm.ridge from MASS, or enet from elasticnet
ridgeModel <- enet(x = as.matrix(solTrainXtrans), y = solTrainY,
lambda = 0.001)  # this is ridge penalty
ridgePred <- predict(ridgeModel, newx = as.matrix(solTestXtrans),
s = 1, mode = "fraction",  # s=1 is full solution. lasso lamba=0 so this is ridge regression
type = "fit")
head(ridgePred$fit)
# defining tuning grid
ridgeGrid <- data.frame(.lambda = seq(0, 0.1, length = 15))
ridgeRegFit <- train(solTrainXtrans, solTrainY,
method = "ridge",
tuneGrid = ridgeGrid,
trControl = ctrl,
preProcess = c("center", "scale"))
ridgeRegFit
# lasso: lars from lars or enet from elasticnet or glmnet
enetModel <- enet(x = as.matrix(solTrainXtrans), y = solTrainY,
lambda = 0.01, normalize = TRUE)
enetPred <- predict(enetModel, newx = as.matrix(solTestXtrans),
s = 0.1, mode = "fraction",
type = "fit")
names(enetPred)
enetPred$fit
head(enetPred$fit0
head(enetPred$fit)
head(enetPred$fit)
enetCoef <- predict(enetModel, newx = as.matrix(solTestXtrans),
s = 0.1, mode = "fraction",
type = "coefficients")
tail(enetCoef$coefficients)
# using train function for lasso
enetGrid <- expand.grid(.lambda = c(0, 0.01, 0.1),
.fraction = seq(0.05, 1, length = 20))
enetGrid
enetGrid
enetTune <- train(solTrainXtrans, solTrainY,
method = "enet",
tuneGrid = enetGrid,
trControl = ctrl,
preProcess = c("center", "scale"))
plot(enetTune)
knitr::opts_chunk$set(cache=TRUE)
rm(list = ls())
require(caret)
data(tecator)
setwd("C:\\Users\\pistachio\\Projects\\apm")
rmarkdown::render_site()
require(mlbench)
data(Glass)
str(Glass)
# (a) visualizations
pairs(Glass[,-10])
require(tidyverse)
require(ggplot2)
Glass[, -10] %>% gather() %>% head()  # check
ggplot(gather(Glass[, -10]), aes(value)) +
geom_histogram(bins = 10) +
facet_wrap(~key, scales = "free_x")
require(corrplot)
correlations <- cor(Glass[, -10])
correlations
corrplot(correlations, order = "hclust")
# (b) outliers / skewness
# there seems to be an outlier for K and possibly for Fe
require(e1071)
apply(Glass[, -10], 2, skewness)  # definitely skewed
# (c) relevant transformations
# apply box cox to skewed predictors
require(caret)
RITrans <- BoxCoxTrans(Glass$RI)
RITrans
predict(RITrans, head(Glass$RI))
require(mlbench)
data("Soybean")
str(Soybean)
# (a) category frequencies
require(caret)
nearZeroVar(Soybean, saveMetrics = TRUE)
sapply(Soybean[,nearZeroVar(Soybean)], table)
# (b) missing data patterns
require(mice)
md.pattern(Soybean)  # 562 of 683 have values for all cols
# missing values seem to happen for entire categories, i.e. all leaf cols
md.pattern(Soybean) %>% colnames  # col names from most to least filled out
# (c) handle missing values
soy_data <- Soybean[, -nearZeroVar(Soybean)]  # drop 3 cols
preProcValues <- preProcess(soy_data, method = c("knnImpute"))  # doesn't work bc of categorical vars?
soy_data_imputed <- mice(soy_data, method = "pmm", seed = 500)
# (a) category frequencies
require(caret)
data(BloodBrain)
# (b) degenerate distributions
# assume this means dist with near zero variance
require(magrittr)
sapply(bbbDescr[,nearZeroVar(bbbDescr)], table)
sapply(bbbDescr[,nearZeroVar(bbbDescr)], table) %>% names()
# (c) correlations
correlations <- cor(bbbDescr)
highCorr <- findCorrelation(correlations, cutoff = 0.75)
length(highCorr)  # yes, strong correlations
# use PCA or remove highly correlated predictors
# almost half the vars are highly correlated
# yes, will impact number of predictors for modeling
mean_r2 <- c(0.444, 0.5, 0.533, 0.545, 0.542, 0.537, 0.534, 0.534, 0.52, 0.507)
tolerance <- (mean_r2 - max(mean_r2))/max(mean_r2)
tolerance
require(AppliedPredictiveModeling)
require(caret)
data(oil)
str(oilType)
table(oilType)
# (a) sample in base R
set.seed(1)
table(sample(oilType, 60))
set.seed(2)
table(sample(oilType, 60))
set.seed(3)
table(sample(oilType, 60))
set.seed(4)
table(sample(oilType, 60))
set.seed(5)
table(sample(oilType, 60))
# (b) stratified sampling with caret
sample_caret_ID <- createDataPartition(oilType, p = 60/96, list = FALSE)
table(oilType[sample_caret_ID])  # pretty similar to base sample
# (c) resampling for small sample size
# could try a 10-fold CV
# (d) binom.test confidence interval
binom.test(16, 20)  # if 16 out of 20 correct on test set (80% accuracy)
# confidence interval can give a sense of uncertainty
0.943 - 0.563  # width of confidence interval
test_accuracy <- 0.8
diff(binom.test(20 * test_accuracy, 20)[["conf.int"]])
diff(binom.test(40 * test_accuracy, 40)[["conf.int"]])
diff(binom.test(60 * test_accuracy, 60)[["conf.int"]])
diff(binom.test(80 * test_accuracy, 80)[["conf.int"]])
diff(binom.test(100 * test_accuracy, 100)[["conf.int"]])
diff(binom.test(1000 * test_accuracy, 1000)[["conf.int"]])
knitr::opts_chunk$set(cache=TRUE)
rm(list = ls())
install.packages("earth")
require(caret)
require(earth)
require(kernlab)
require(nnet)
libary(caret)
library(caret)
library(kernlab)
library(nnet)
library(earth)
install.packages("earth")
