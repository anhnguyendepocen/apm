<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>APM Notes</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 45px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 50px;
  margin-top: -50px;
}

.section h2 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h3 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h4 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h5 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h6 {
  padding-top: 50px;
  margin-top: -50px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Applied Predictive Modeling (2013)</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Notes</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Computation
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="general-strategies.html">General Strategies</a>
    </li>
    <li>
      <a href="regression-models.html">Regression Models</a>
    </li>
    <li>
      <a href="classification-models.html">Classification Models</a>
    </li>
    <li>
      <a href="other-considerations.html">Other Considerations</a>
    </li>
  </ul>
</li>
<li>
  <a href="solutions.html">Solutions</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">APM Notes</h1>

</div>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<div id="chapter-1-introduction" class="section level3">
<h3>Chapter 1: Introduction</h3>
<p>Data characteristics to evaluate</p>
<ul>
<li>Response
<ul>
<li>categorical vs. continuous</li>
<li>balanced/symmetric</li>
<li>unbalanced/skewed</li>
<li>independent</li>
</ul></li>
<li>Predictors
<ul>
<li>continuous</li>
<li>count</li>
<li>categorical</li>
<li>correlated/associated</li>
<li>different scales</li>
<li>missing values</li>
<li>sparse</li>
</ul></li>
</ul>
</div>
</div>
<div id="general-strategies" class="section level2">
<h2>General Strategies</h2>
<div id="chapter-2-a-short-tour-of-the-predictive-modeling-process" class="section level3">
<h3>Chapter 2: A Short Tour of the Predictive Modeling Process</h3>
<p>n/a</p>
</div>
<div id="chapter-3-data-pre-processing" class="section level3">
<h3>Chapter 3: Data Pre-Processing</h3>
<ul>
<li><strong>Centering</strong>: subtract average value so that predictors have 0 mean</li>
<li><strong>Scaling</strong>: divide by standard deviation so predictors all have sd of 1</li>
</ul>
<p>For <strong>skewness</strong>, general rule of thumb is if ratio of highest value to lowest value is greater than 20, there is significant skewness. If using the skewness statistic, a symmetric distribution will have a skewness value close to 0, right skewed distributions have a positive statistic and left skewed distributions have a negative statistic.</p>
<p>Transforming data with log, square root, or inverse could remove skewness.</p>
<p><strong>Box-Cox Transformation</strong></p>
<p>This can be applied independently to each predictor that contains values &gt; 0.</p>
<p><span class="math display">\[x^{*} = \left\{\begin{matrix}
(x^{\lambda-1})/\lambda, &amp;  \lambda \neq 0 \\ 
log(x), &amp; \lambda = 0 \end{matrix}\right.\]</span></p>
<p><strong>Outliers</strong></p>
<ul>
<li>Some methods like tree-based and SVM are not sensitive to outliers.</li>
<li>Spatial sign transformation projects predictors onto a multidimensional sphere (making all samples the same distance from the center). Need to center and scale data first, and has to be applied on entire group of predictors.</li>
</ul>
<p><strong>PCA</strong></p>
<ul>
<li>principal components are uncorrelated</li>
<li>scree plot shows total variance explained by # of components</li>
<li>scale of components tend to get smaller bc they account for less and less of the variation</li>
<li>PCA can be used to look at loadings by “channels”.</li>
</ul>
<p><strong>Missing Values</strong></p>
<ul>
<li>understand why data is missing first</li>
<li>is pattern of missing data related to outcome?</li>
<li>censored data is not missing data since something is known about its value</li>
<li>can impute missing values with K nearest neighbors</li>
</ul>
<p><strong>Removing Variables</strong></p>
<ul>
<li>rule of thumb for near-zero variance predictors
<ul>
<li>franction of unique values over sample size is low (&lt;10%)</li>
<li>ratio of most frequent to second most frequent value is large (&gt;20)</li>
</ul></li>
<li>variance inflation factor (VIF) can be used to identify collinearity
<ul>
<li>limited to linear regression and requires more samples than predictors</li>
</ul></li>
<li>could remove min # of predictors so that pairwise correlations are below some threshold (e.g. 0.75)</li>
</ul>
</div>
<div id="chapter-4-over-fitting-and-model-tuning" class="section level3">
<h3>Chapter 4: Over-Fitting and Model Tuning</h3>
<ul>
<li><strong>apparent</strong> performance is using training set to predict model performance</li>
<li><strong>tuning parameters</strong> are model params that can’t be estimated with formulas (number of neighbors in a KNN classification model)</li>
</ul>
<p><strong>Data Splitting</strong></p>
<ul>
<li>when samples are not large, may need to avoid test set
<ul>
<li>test set needs to be sufficiently large to have power/precision</li>
</ul></li>
<li>stratified random sampling applies random sampling within groups to account for the outcome</li>
<li>maximum dissimilarity sampling splits based on predictors</li>
</ul>
<p><strong>Resampling Techniques</strong></p>
<ul>
<li>k-fold cross-validation
<ul>
<li>usually k is set to 5 or 10, but no formal rule</li>
<li>larger k has smaller bias (difference b/w estimated and true model performance), but more computationally demanding</li>
<li>k-fold CV generally has higher variance compared to other methods (problem for small datasets)</li>
<li>bias for small values of k (2 or 3) is about the same as the bootstrap, but with larger variance</li>
</ul></li>
<li>stratified random sampling applied to k-fold CV selects k partitions so that folds are balanced with respect to the outcome</li>
<li>leave-one-out cross-validation (LOOCV) can be thought of as a special case of k-fold where k equals to number of samples
<ul>
<li>has a closed form solution for linear regression models</li>
</ul></li>
<li>repeated training/test splits, also called leave-group-out CV or monte carlo CV
<ul>
<li>rule of thumb for group size is 75-80%</li>
<li>unlike k-fold CV, samples can be in multiple hold out sets, and repitions are usually larger (50-200)</li>
</ul></li>
<li>bootstrap samples data with replacement to create subsets that are the same size as total samples
<ul>
<li>samples that are not selected are the “out of bag” samples, used like a hold out test set</li>
<li>tend to have less uncertainty than k-fold CV</li>
<li>bias can be problematic for small datasets</li>
<li>632 method: 0.632 x bootstrap estimate + 0.368 x apparent error rate</li>
<li>63.2% of data points are represented at least once in the bootstrap sample</li>
<li>this method reduces bias, but can be unstable for smaller datasets</li>
<li>there’s also a 632+ method</li>
</ul></li>
</ul>
<p><strong>Choosing Tuning Parameters</strong></p>
<ul>
<li>in general, favor simpler models
<ul>
<li>choosing tuning params based on numically optimal value can still lead to overfitting</li>
</ul></li>
<li>one-standard error method starts with numerically optimal param and picks the simplest model within one sd</li>
<li>tolerance is (x-optimal)/optimal and can be used if deciding a certain % loss in performance is acceptable</li>
</ul>
<p><strong>Choosing Resampling Methods</strong></p>
<ul>
<li>if sample size is small, recommend 10-fold CV
<ul>
<li>good bias/variance properties</li>
<li>low computational costs</li>
</ul></li>
<li>if goal is model selection instead of best indicator of performance, recommend a bootstrap method
<ul>
<li>low variance</li>
</ul></li>
<li>if sample size is large, differences between resampling methods are smaller
<ul>
<li>computational efficiency is more important</li>
</ul></li>
</ul>
<p><strong>Choosing Between Models</strong></p>
<ol style="list-style-type: decimal">
<li>start with several models that are less interpretable and most flexible (boosted trees, SVM)</li>
<li>investigate simpler models that are less opaque (MARS, PLS, GAM, naive Bayes)</li>
<li>use the simplest model that reasonably approximates performance of complex models</li>
</ol>
<p>A <strong>paired t-test</strong> can be used to evaluate if there are statistically significant differences in model performance.</p>
</div>
</div>
<div id="regression-models" class="section level2">
<h2>Regression Models</h2>
<div id="chapter-5-measuring-performance-in-regression-models" class="section level3">
<h3>Chapter 5: Measuring Performance in Regression Models</h3>
<p><strong>Quantitative Performance Measures</strong></p>
<ul>
<li>RMSE is most common</li>
<li><span class="math inline">\(R^{2}\)</span> is the coefficient of determination
<ul>
<li>proportion of information in data explained by model</li>
<li>simplest version squares the correlation coefficient b/w observed and predicted</li>
<li>cautions
<ul>
<li>this reflects correlation, not accuracy</li>
<li>is dependent on variation in outcome (in denominator)
<ul>
<li><span class="math inline">\(R^{2}=1-RMSE/(Sample Variance)\)</span> so result is worse if variance is low and responses with large variance may have very good <span class="math inline">\(R^{2}\)</span> results</li>
</ul></li>
</ul></li>
</ul></li>
<li>rank correlation (Spearman) can be used for models used to rank new samples
<ul>
<li>calculated as the correlation coefficent between the ranks of observed and predicted</li>
</ul></li>
</ul>
<p><strong>Variance-Bias Trade-off</strong></p>
<p>Expected value of MSE is equal to irreducible noise plus squared bias plus model variance.</p>
<ul>
<li>Model bias is how close model form can reflect true relationship between predictors and outcome</li>
<li>Model variance reflects how much model parameters will change if predictors change
<ul>
<li>collinearity can increase variance</li>
</ul></li>
</ul>
<p><span class="math display">\[E[MSE] = \sigma^{2} + (Model Bias)^{2} + Model Variance\]</span></p>
</div>
<div id="chapter-6-linear-regression-and-its-cousins" class="section level3">
<h3>Chapter 6: Linear Regression and Its Cousins</h3>
<ul>
<li>ordinary linear regression finds parameter estimates that minimize bias
<ul>
<li>objective is to minimize sum of squared errors</li>
</ul></li>
<li>ridge, lasso, and elastic net find estimates that have lower variance</li>
</ul>
<p>Benefits of linear regression type models</p>
<ul>
<li>predictor changes are easiliy interpretable</li>
<li>relationship among predictors is easily interpretable</li>
<li>mathematical nature allows for standard error computations
<ul>
<li>can assess statistical significance of each predictor, but need to make assumptions about residual distribution</li>
</ul></li>
</ul>
<p>Drawbacks of linear regression type models</p>
<ul>
<li>solution may not be linear in parameters</li>
<li>sensitive to outliers bc SSE is minimized
<ul>
<li>can use MAE or Huber function instead (Huber uses squared residuals under a threshold and simple difference above threshold)</li>
</ul></li>
</ul>
<p><strong>Linear Regression</strong></p>
<p><span class="math display">\[\hat{\beta} = (X^{T}X)^{-1}X^{T}y\]</span></p>
<ul>
<li>inverse of <span class="math inline">\((X^{T}X)\)</span> exists when:
<ul>
<li>no predictor can be determined from a combo of other predictors
<ul>
<li>removing pairwise correlated predictors does not eliminate predictors that are functions of 2+ predictors</li>
<li>use variance inflation factor to diagnose multicollinearity</li>
</ul></li>
<li>number of samples is greater than number of predictors</li>
</ul></li>
</ul>
<p><strong>Partial Least Squares</strong></p>
<ul>
<li>pre-processing predictors with PCA before regression is called PCR
<ul>
<li>drawback is that PCA does not consider response, so if variability of predictors is unrelated to variability of response then regression will not be good
<ul>
<li>PLS is recommended when there are correlated predictors and want a linear regression type model</li>
</ul></li>
</ul></li>
<li>PLS finds components that maximally summarize variation of predictors while also requiring components to have max correlation with response
<ul>
<li>iterative process of calculating weights (w), scores (t), and loadings (p)
<ul>
<li>w: relationship b/w predictor and response, signals predictor importance in VIP (variable importance in the projection)</li>
<li>t: predictor orthongonally projected onto direction</li>
<li>p: correlation between scores (t) and original predictors</li>
<li>at end of iteration, current estimates are subtracted from predictors and responses</li>
</ul></li>
<li>can be thought of like supervised dimension reduction</li>
<li>predictors should be centered and scaled before PLS, similar to PCA</li>
<li>has one tuning parameter: number of components</li>
<li>squared VIP values sum to number of predictors
<ul>
<li>general rule of thumb is if VIP &gt; 1 then predictor has predictive info for response</li>
<li>predictors with small PLS regression coef and small VIP are likely not important for model</li>
</ul></li>
<li>algo has been improved upon many times, though other models are still recommended for nonlinear relationships b/w predictors and response instead of augmenting PLS</li>
</ul></li>
<li>in practice, PCR and PLS produce models with similar performance but PCR needs to retain more components</li>
</ul>
<p><strong>Penalized Models</strong></p>
<ul>
<li>OLS coef are unbiased and has lowest variance of unbiased linear models</li>
<li>common that small increase in bias will lower var significantly, thus reducing MSE
<ul>
<li>correlation can lead to large variance</li>
<li>biased models that deal w collinearity better can have better MSE than OLS</li>
</ul></li>
<li>one way to create biased model is by using penalty
<ul>
<li>since penalty is applied to coefficients, they should be on the same scale so predictors should be centered and scaled</li>
<li>when model overfits or coef are inflated bc of collinearity, can add penalty to SSE if estimates are large</li>
<li>ridge regression adds penalty to the sum of squared regression parameters
<ul>
<li>L2: second order penalty</li>
</ul></li>
<li>lasso (least absolute shrinkage and selection) adds penalty to abs value of regression parameters
<ul>
<li>L1: first order penalty</li>
<li>also does feature selection bc some betas are set to 0, unliked ridge</li>
</ul></li>
<li>LARS (least angle regression) includes lasso and similar models</li>
<li>elastic net combines ridge and lasso penalties (different lambdas)
<ul>
<li>combines effective regularization of ridge and feature selection of lasso</li>
<li>might more effectively deal w groups of correlated vars</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="chapter-7-nonlinear-regression-models" class="section level3">
<h3>Chapter 7: Nonlinear Regression Models</h3>
<ul>
<li>linear models can handle non linearity but must know nature of nonlinearity to manually adjust</li>
</ul>
<p><strong>Neural Networks</strong></p>
<ul>
<li>hidden layer is a linear combo of predictors, but typically transformed by nonlinear function (i.e. sigmoidal)</li>
<li>output is a linear combo of hidden layer units</li>
<li>coef are unlikely to present coherent info bc there are no constraints to defining the linear combo (unlike PLS)</li>
<li>total params estimates are <span class="math inline">\(H(P+1)+H+1\)</span></li>
<li>challenging optimization problem bc of number of params
<ul>
<li>back propagation is an efficient method of using derivatives, but soln is often not a global one</li>
<li>tendency to overfit
<ul>
<li>can address with early stopping method to halt when error rate increases, however, errors have uncertainty</li>
<li>can also use regularization (weight decay) and add penalty to coefficients (<strong>lambda is usually b/w 0 and 0.1</strong>)</li>
<li>bc models can be unstable (finds local optimal solution), several models can be averaged, this has a large positive fefect on neural networks</li>
</ul></li>
</ul></li>
</ul>
<p><strong>Multivariate Adaptive Regression Splines (MARS)</strong></p>
<ul>
<li>creates new features from one or two predictors by finding a breakpoint for which one side is 0 until breakpoint and the other is 0 after breakpoint
<ul>
<li>each predictor is evaluated for breakpoint by minimized error
<ul>
<li>additional features are evaluated on top of prior features until a stopping point is reached</li>
<li>hinge functions are typically written as <span class="math inline">\(h(x-a)\)</span> (nonzero when <span class="math inline">\(x&gt;a\)</span>) and <span class="math inline">\(h(a-x)\)</span> (nonzero when <span class="math inline">\(x&lt;a\)</span>)</li>
<li>this wouldn’t be done for binary predictors</li>
</ul></li>
<li>becomes a piecewise linear model</li>
</ul></li>
<li>after creating full set of features, features are pruned if they don’t significantly reduce error rate using leave one out CV approximation
<ul>
<li>process does not proceed backwards from feature creation</li>
</ul></li>
<li>second degree MARS does the same search for single predictor breakpoints and then searches for new cuts to couple with original features
<ul>
<li>in practice, have seen second degree or higher MARS produce have instabilities where a few samples predictions are wildly inaccurate
<ul>
<li>this has not been observed with additive MARS models</li>
</ul></li>
</ul></li>
<li>two tuning parameters: degree of features added, number of retained terms</li>
<li>advantages of using MARS
<ul>
<li>automatically does feature selection
<ul>
<li>model equation is independent of predictors that are not involved with any final model features</li>
</ul></li>
<li>very interpretable</li>
<li>requires little pre-processing
<ul>
<li>MARS is okay if there are correlated variables, but strong correlations will hurt interpretability</li>
</ul></li>
</ul></li>
</ul>
<p><strong>Support Vector Machines</strong></p>
<ul>
<li>drawback of minimizing SSE is that models are sensitive to outliers
<ul>
<li>alternative is to use Huber function, where residuals are squared for small values else use absolute values</li>
<li>SVM is similar in that user selects a threshold and residuals within threshold do not contribute to error and above threshold contribute a linear scale amount
<ul>
<li>squared residuals are not used, so less sensitive to outliers</li>
<li>if threshold is high, outliers determine the regression</li>
</ul></li>
</ul></li>
<li>SVM has to fit number of parameters on scale of sample size
<ul>
<li>regularization helps over-parameterization</li>
<li>training data is required to make new predictions
<ul>
<li>when training data is large, SVM can be time consuming, unless many samples have <span class="math inline">\(\alpha=0\)</span> due to cost penalty</li>
</ul></li>
</ul></li>
<li>cost penalty is applied to residuals, lower cost leads to underfitting</li>
<li>SVM can have different kernels: linear, polynomial, radial, hyperbolic
<ul>
<li>using kernel functions can introduce new parameters
<ul>
<li>for radial kernel, can estimate scale parameter with shortcut</li>
</ul></li>
<li>in practice radial kernel does quite well, unless true relationship is linear</li>
</ul></li>
<li>in practice, cost parameter provides more flexibility for model tuning, so recommend fixing epsilon and tuning over kernel parameters</li>
<li>predictors should be centered and scaled bc they enter model as sum of cross products</li>
<li>relevance vector machine is a bayesian analog to SVM</li>
</ul>
<p><strong>K-Nearest Neighbors</strong></p>
<ul>
<li>user defines distance measure
<ul>
<li>Euclidean distance (straight line) is most common</li>
<li>Minkowski distance is a generalization of Euclidean distance (handles higher dimensions)</li>
</ul></li>
<li>predictors should be centered and scaled to prevent predictors with larger scales biasing results</li>
<li>distance computation requires non-missing values
<ul>
<li>can exclude missing values or impute with naive estimator (e.g. mean)</li>
</ul></li>
<li>ideal K can be selection through resampling
<ul>
<li>small K’s overfit and large K’s underfit</li>
</ul></li>
<li>problems include computation time (distance calc requires entire sample to be in memory) and disconnect between local structure and predictive ability of KNN (irrelevant or noisy predictors need to be removed in pre-processing)</li>
</ul>
</div>
<div id="chapter-8-regression-trees-and-rule-based-models" class="section level3">
<h3>Chapter 8: Regression Trees and Rule-Based Models</h3>
<ul>
<li>advantages
<ul>
<li>do not need as much pre-processing
<ul>
<li>can effectively handle predictors that are sparse, skewed, continuous, categorical, etc.</li>
</ul></li>
<li>do not need to specify model form</li>
<li>can handle missing data</li>
<li>implicitly conducts feature selection</li>
<li>when tree is small, easy to interpret and compute</li>
</ul></li>
<li>disadvantages
<ul>
<li>model instability - slight changes in data can drastically change model so model is not as interpretable (for single trees)
<ul>
<li>strongly correlated vars are also an issue</li>
</ul></li>
<li>not optimal predictive performance bc of defined rectangular space</li>
<li>finite terminal nodes results in finite predicted results</li>
<li>predictors with more distinct values are implicitly favored (selection bias)</li>
</ul></li>
<li>ensemble models mitigate disadvantages</li>
</ul>
<p><strong>Basic Regression Trees</strong></p>
<ul>
<li>CART (classification and regression tree)
<ul>
<li>starts with entire data set, finds split for each predictor to minimize sum of squares error</li>
<li>repeats for each split group</li>
<li>also called recursive partitioning</li>
<li>tree growing step happens until number of samples in the final splits fall below some threshold
<ul>
<li>missing data is ignored, surrogate splits are evaluated</li>
</ul></li>
<li>tree is then pruned back to a smaller depth
<ul>
<li>one method is cost-complexity tuning
<ul>
<li>finds tree with smallest error rate with penalization for number of nodes</li>
<li>number of nodes is multiplied by complexity parameter
<ul>
<li>can select tuning parameter with cross validation and one-standard error rule</li>
</ul></li>
</ul></li>
</ul></li>
<li>variable importance can be measured by keeping track of reduction of optimization criteria for each predictor</li>
</ul></li>
<li>GUIDE (generalized, unbiased, interaction detection and estimation)
<ul>
<li>addresses bias issue by decoupling predictor selection (uses hypotheses testing) and split value</li>
</ul></li>
<li>Conditional Inference Trees
<ul>
<li>addresses bias by using hypothesis testing for predection selection and determining split values</li>
<li>uses 1 minus p-value as significance threshold
<ul>
<li>allows for comparison of predictors on different scales</li>
<li>multiple comparison corrections can be applied to reduce bias and reduce false-positives from conduction a large number of hypothesis tests</li>
</ul></li>
<li>no need to prune bc more splits result in smaller samples and thus higher p-values</li>
<li>p-val is not directly related to model performance so still need to choose tree based on performance</li>
</ul></li>
</ul>
<p><strong>Regression Model Trees</strong></p>
<ul>
<li>terminal nodes can use a function besides average in group (models in the leaves)</li>
<li>Weka package includes a “rational reconstruction” implementation of M5
<ul>
<li>initial split starts the same as simple regression tree
<ul>
<li>split is found through exhaustive search across all predictors and training samples, but expected reduction in node error rate is used (measures reduction in standard deviation)</li>
</ul></li>
<li>linear model is fit within partition using the split variable (and preceding ones) to minimize error instead of SD</li>
<li>repeat tree growing process until error rate does not improve or not enough samples to continue
<ul>
<li>fully grown tree will have a linear model for every node</li>
</ul></li>
<li>prune tree using adjusted error rate
<ul>
<li>observed vs predicted error is multiplied by penalty <span class="math inline">\((n^{*}+p)/(n^{*}-p)\)</span>
<ul>
<li><span class="math inline">\(n^{*}\)</span> is number of training set data points used in linear model</li>
<li><span class="math inline">\(p\)</span> is number of parameters</li>
</ul></li>
</ul></li>
<li>smoothing or recursive shrinking metho is used when predicting new samples
<ul>
<li>linear models along path are combined from the bottom up
<ul>
<li>child model prediction is weighted using training sample size and parent uses a default weighting of 15</li>
</ul></li>
</ul></li>
</ul></li>
<li>collinearity can still lead to model instability, smoothing helps, but removing correlated vars can have a measurable drop in performance</li>
</ul>
<p><strong>Rule-Based Models</strong></p>
<ul>
<li>a rule is a distinct path through a tree (all the way to the end)</li>
<li>number of samples affected by a rule is called its coverage</li>
<li>nodes can be redundancies (same split used in different places)</li>
<li>separate and conquer strategy creates many trees (unsmoothed) and keeps rule with largest coverage, then repeats process on samples that are not in coverage area until all samples are covered</li>
</ul>
<p><strong>Bagged Trees</strong></p>
<ul>
<li>bagging, short for bootstrap aggregation, is a general approach that uses bootstrapping with regression/classification models to construct an ensemble
<ul>
<li>generate m bootstrapped samples of original training data to train unpruned trees of set max depth</li>
<li>individual m model predictions are averaged to give the bagged model prediction</li>
<li>often see exponential decrease in prediction improvement from increasing m
<ul>
<li>most improvement is seen from small number of trees (m&lt;10)</li>
<li>if performance is still not good at m=50, should try a different method (random forest, boosting)</li>
</ul></li>
</ul></li>
<li>advantages of bagged models
<ul>
<li>reduces prediction variance through aggregation process (more important for unstable models)
<ul>
<li>changing the sequence of bootstrapped samples should not result in a significantly different end prediction</li>
</ul></li>
<li>better prediction accuracy (related to predictions being more stable), esp for trees
<ul>
<li>bagging stable, low variance models (linear regression, MARS) will result is smaller accuracy improvements</li>
</ul></li>
<li>provides internal estimate of predictive performance that correlates well with CV or test set estimates
<ul>
<li>because some (out-of-bag) samples are left out when constructing each tree, sort of like CV</li>
<li>average of out-of-bag performance metrics (out-of-bag estimates) can be used to gauge ensemble performance</li>
</ul></li>
</ul></li>
<li>disadvantages
<ul>
<li>computational costs and memory requirements (as m increases)
<ul>
<li>parallel computing can help with both computation time and memory (each tree is fit independently)</li>
</ul></li>
<li>much less interpretable than unbagged model
<ul>
<li>can’t obtain any convenient rules like from single tree</li>
<li>can still get variable importance (see random forest section)</li>
</ul></li>
</ul></li>
</ul>
<p><strong>Random Forests</strong></p>
<ul>
<li>bagged trees are not completely independent bc all predictors are used for each tree
<ul>
<li>if sample size is large enough, trees might not be that different (tree correlation)</li>
<li>to reduce this correlation, randomness can be added by sampling top predictors</li>
</ul></li>
<li>algo for random forests
<ul>
<li>create m bootstrapped samples</li>
<li>train tree on each sample
<ul>
<li>at each split randomly select k (or m_try) from the original predictors to evaluate for splits</li>
</ul></li>
<li>predictions are averages across trees</li>
</ul></li>
<li>m_try parameter
<ul>
<li>default m_try is recommended to be 1/3 of total predictors</li>
<li>bc of computational intensity, recommend tuning grid to have 5 values of m_try evenly spaced between 2 and P (total)</li>
</ul></li>
<li>number of trees
<ul>
<li>rf is protected from overfitting so increasing trees just increases computation time</li>
<li>recommend using at least 1000 trees, add more trees until CV performance levels off</li>
</ul></li>
<li>advantages
<ul>
<li>more efficient than bagging (bc a subset of predictors are evaluated for each tree), though more trees are required for rf than bagging</li>
<li>computation can be parallel, so more efficient than boosting</li>
<li>protected from overfitting</li>
</ul></li>
<li>disadvantages
<ul>
<li>rf can underfit when data is not noisy</li>
<li>impossible to gain understading of relationship b/w predictors and response</li>
</ul></li>
<li>rf tuning parameters in practice usually does not have too much impact on performance</li>
<li>using out of bag error rate for evaluation will drastic reduce time needed, compared to CV</li>
<li>variable importance values
<ul>
<li>between predictor correlations can have significant impact on values
<ul>
<li>uninformative predictors with high correlation to informative predictors can have higher ranking</li>
<li>dilution effect (including two important variables that are highly correlated will halve their importance)</li>
</ul></li>
<li>m_try parameter also has a serious effect on importance values</li>
</ul></li>
</ul>
<p><strong>Boosting</strong></p>
<ul>
<li>gradient boosting machines (gbm): given a loss function (squared error) and weak learner (regression trees), find an additive model that minimizes loss function
<ul>
<li>initialize with best guess (mean of response), another model is fit to residuals, repeat until user specified number of iterations is hit</li>
<li>trees make a great base learner bc
<ul>
<li>can be made weak learners by restricting depth</li>
<li>can be additive</li>
<li>can be generated quickly</li>
</ul></li>
<li>if trees are used, tuning parameters will be depth of tree (interaction depth) and number of iterations</li>
</ul></li>
<li>susceptible to overfitting by finding local optima instead of global best split, despit weak learner
<ul>
<li>remedy is to apply regularization (shrinkage) by only adding a fraction (learning rate <span class="math inline">\(\lambda\)</span>) of current predicted value
<ul>
<li>small learning rates increase computation time and memory demands</li>
<li>usually use learning rate of 0.01 or 0.1</li>
</ul></li>
<li>can also add a bagging step by only using a random sample (bagging fraction) for each tree (stochastic gradient boosting)
<ul>
<li>increases accuracy and reduces computation time</li>
<li>suggest default bagging fraction of 0.5, but this can also be tuned</li>
</ul></li>
</ul></li>
<li>computation time for boosting is usually greater than rf bc it can’t be made into a parallel process (each tree depends on prior trees)</li>
<li>variable importance value rankings can drop off more steeply than rf because of tree dependency (structure correlation can lead to same predictors being selected across trees)</li>
</ul>
<p><strong>Cubist</strong></p>
<ul>
<li>differs from previous models in three ways
<ul>
<li>technique used for linear model smoothing, creating rules, pruning</li>
<li>optional boosting (committees)</li>
<li>predictions can be adjusted using nearby training data</li>
</ul></li>
</ul>
</div>
</div>
<div id="classification-models" class="section level2">
<h2>Classification Models</h2>
<div id="chapter-11-measuring-performance-in-classification-models" class="section level3">
<h3>Chapter 11: Measuring Performance in Classification Models</h3>
</div>
<div id="chapter-12-discriminant-analysis-and-other-linear-classification-models" class="section level3">
<h3>Chapter 12: Discriminant Analysis and Other Linear Classification Models</h3>
</div>
<div id="chapter-13-nonlinear-classification-models" class="section level3">
<h3>Chapter 13: Nonlinear Classification Models</h3>
</div>
<div id="chapter-14-classification-trees-and-rule-based-models" class="section level3">
<h3>Chapter 14: Classification Trees and Rule-Based Models</h3>
</div>
<div id="chapter-15-a-summary-of-grant-application-models" class="section level3">
<h3>Chapter 15: A Summary of Grant Application Models</h3>
</div>
<div id="chapter-16-remedies-of-severe-class-imbalance" class="section level3">
<h3>Chapter 16: Remedies of Severe Class Imbalance</h3>
</div>
<div id="chapter-17-case-study-job-scheduling" class="section level3">
<h3>Chapter 17: Case Study: Job Scheduling</h3>
</div>
</div>
<div id="other-considerations" class="section level2">
<h2>Other Considerations</h2>
<div id="chapter-18-measuring-predictor-importance" class="section level3">
<h3>Chapter 18: Measuring Predictor Importance</h3>
</div>
<div id="chapter-19-an-introduction-to-feature-selection" class="section level3">
<h3>Chapter 19: An Introduction to Feature Selection</h3>
</div>
<div id="chapter-20-factors-that-can-affect-model-performance" class="section level3">
<h3>Chapter 20: Factors That Can Affect Model Performance</h3>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
