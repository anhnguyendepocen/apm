<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Applied Predictive Modeling (2013) by Kjell Johnson and Max Kuhn – Notes</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 45px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 50px;
  margin-top: -50px;
}

.section h2 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h3 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h4 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h5 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h6 {
  padding-top: 50px;
  margin-top: -50px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Applied Predictive Modeling (2013)</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Notes</a>
</li>
<li>
  <a href="solutions.html">Solutions</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Applied Predictive Modeling (2013) by Kjell Johnson and Max Kuhn – Notes</h1>

</div>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<div id="chapter-1" class="section level3">
<h3>Chapter 1</h3>
<p>Data characteristics to evaluate</p>
<ul>
<li>Response
<ul>
<li>categorical vs. continuous</li>
<li>balanced/symmetric</li>
<li>unbalanced/skewed</li>
<li>independent</li>
</ul></li>
<li>Predictors
<ul>
<li>continuous</li>
<li>count</li>
<li>categorical</li>
<li>correlated/associated</li>
<li>different scales</li>
<li>missing values</li>
<li>sparse</li>
</ul></li>
</ul>
</div>
</div>
<div id="general-strategies" class="section level2">
<h2>General Strategies</h2>
<div id="chapter-2" class="section level3">
<h3>Chapter 2</h3>
<p>n/a</p>
</div>
<div id="chapter-3" class="section level3">
<h3>Chapter 3</h3>
<ul>
<li><strong>Centering</strong>: subtract average value so that predictors have 0 mean</li>
<li><strong>Scaling</strong>: divide by standard deviation so predictors all have sd of 1</li>
</ul>
<p>For <strong>skewness</strong>, general rule of thumb is if ratio of highest value to lowest value is greater than 20, there is significant skewness. If using the skewness statistic, a symmetric distribution will have a skewness value close to 0, right skewed distributions have a positive statistic and left skewed distributions have a negative statistic.</p>
<p>Transforming data with log, square root, or inverse could remove skewness.</p>
<pre class="r"><code>library(AppliedPredictiveModeling)
library(e1071)
data(&quot;segmentationOriginal&quot;)
#apply(segmentationOriginal[, -(1:3)], 2, function(x) skewness(x))
# can&#39;t match the 2.39 from book</code></pre>
<p><strong>Box-Cox Transformation</strong></p>
<p>This can be applied independently to each predictor that contains values &gt; 0.</p>
<p><span class="math display">\[x^{*} = \left\{\begin{matrix}
(x^{\lambda-1})/\lambda, &amp;  \lambda \neq 0 \\ 
log(x), &amp; \lambda = 0 \end{matrix}\right.\]</span></p>
<p><strong>Outliers</strong></p>
<ul>
<li>Some methods like tree-based and SVM are not sensitive to outliers.</li>
<li>Spatial sign transformation projects predictors onto a multidimensional sphere (making all samples the same distance from the center). Need to center and scale data first, and has to be applied on entire group of predictors.</li>
</ul>
<p><strong>PCA</strong></p>
<ul>
<li>principal components are uncorrelated</li>
<li>scree plot shows total variance explained by # of components</li>
<li>scale of components tend to get smaller bc they account for less and less of the variation</li>
<li>PCA can be used to look at loadings by “channels”.</li>
</ul>
<p><strong>Missing Values</strong></p>
<ul>
<li>understand why data is missing first</li>
<li>is pattern of missing data related to outcome?</li>
<li>censored data is not missing data since something is known about its value</li>
<li>can impute missing values with K nearest neighbors</li>
</ul>
<p><strong>Removing Variables</strong></p>
<ul>
<li>rule of thumb for near-zero variance predictors
<ul>
<li>franction of unique values over sample size is low (&lt;10%)</li>
<li>ratio of most frequent to second most frequent value is large (&gt;20)</li>
</ul></li>
<li>variance inflation factor (VIF) can be used to identify collinearity
<ul>
<li>limited to linear regression and requires more samples than predictors</li>
</ul></li>
<li>could remove min # of predictors so that pairwise correlations are below some threshold (e.g. 0.75)</li>
</ul>
<pre class="r"><code>library(AppliedPredictiveModeling)
library(e1071)
library(caret)
library(corrplot)
data(&quot;segmentationOriginal&quot;)
segData &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;)
cellID &lt;- segData$Cell
class &lt;- segData$Class
case &lt;- segData$Case
segData &lt;- segData[, -(1:3)]  # drop cols

statusColNum &lt;- grep(&quot;Status&quot;, names(segData))  # binary versions of predictors
segData &lt;- segData[, -statusColNum]
skewValues &lt;- apply(segData, 2, skewness)
head(skewValues)</code></pre>
<pre><code>##    AngleCh1     AreaCh1 AvgIntenCh1 AvgIntenCh2 AvgIntenCh3 AvgIntenCh4 
## -0.02426252  3.52510745  2.95918524  0.84816033  2.20234214  1.90047128</code></pre>
<pre class="r"><code># Box Cox Transform
Ch1AreaTrans &lt;- BoxCoxTrans(segData$AreaCh1)
Ch1AreaTrans</code></pre>
<pre><code>## Box-Cox Transformation
## 
## 1009 data points used to estimate Lambda
## 
## Input data summary:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   150.0   194.0   256.0   325.1   376.0  2186.0 
## 
## Largest/Smallest: 14.6 
## Sample Skewness: 3.53 
## 
## Estimated Lambda: -0.9</code></pre>
<pre class="r"><code>head(segData$AreaCh1)</code></pre>
<pre><code>## [1] 819 431 298 256 258 358</code></pre>
<pre class="r"><code>predict(Ch1AreaTrans, head(segData$AreaCh1))</code></pre>
<pre><code>## [1] 1.108458 1.106383 1.104520 1.103554 1.103607 1.105523</code></pre>
<pre class="r"><code># PCA using base R
pcaObject &lt;- prcomp(segData,
                    center = TRUE, scale. = TRUE)
percentVariance &lt;- pcaObject$sdev^2/sum(pcaObject$sdev^2)*100
percentVariance[1:3]  # pct var explained</code></pre>
<pre><code>## [1] 20.91236 17.01330 11.88689</code></pre>
<pre class="r"><code>head(pcaObject$x[, 1:5])  # transformed object</code></pre>
<pre><code>##           PC1        PC2         PC3       PC4        PC5
## 2   5.0985749  4.5513804 -0.03345155 -2.640339  1.2783212
## 3  -0.2546261  1.1980326 -1.02059569 -3.731079  0.9994635
## 4   1.2928941 -1.8639348 -1.25110461 -2.414857 -1.4914838
## 12 -1.4646613 -1.5658327  0.46962088 -3.388716 -0.3302324
## 15 -0.8762771 -1.2790055 -1.33794261 -3.516794  0.3936099
## 16 -0.8615416 -0.3286842 -0.15546723 -2.206636  1.4731658</code></pre>
<pre class="r"><code>head(pcaObject$rotation[, 1:3])  # loadings</code></pre>
<pre><code>##                      PC1         PC2          PC3
## AngleCh1     0.001213758 -0.01284461  0.006816473
## AreaCh1      0.229171873  0.16061734  0.089811727
## AvgIntenCh1 -0.102708778  0.17971332  0.067696745
## AvgIntenCh2 -0.154828672  0.16376018  0.073534399
## AvgIntenCh3 -0.058042158  0.11197704 -0.185473286
## AvgIntenCh4 -0.117343465  0.21039086 -0.105060977</code></pre>
<pre class="r"><code># pre-processing with caret
trans &lt;- preProcess(segData,
                    method = c(&quot;BoxCox&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;))
trans</code></pre>
<pre><code>## Created from 1009 samples and 58 variables
## 
## Pre-processing:
##   - Box-Cox transformation (47)
##   - centered (58)
##   - ignored (0)
##   - principal component signal extraction (58)
##   - scaled (58)
## 
## Lambda estimates for Box-Cox transformation:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -2.00000 -0.50000 -0.10000  0.05106  0.30000  2.00000 
## 
## PCA needed 19 components to capture 95 percent of the variance</code></pre>
<pre class="r"><code>transformed &lt;- predict(trans, segData)
head(transformed)</code></pre>
<pre><code>##           PC1        PC2        PC3       PC4        PC5        PC6
## 2   1.5684742  6.2907855 -0.3333299 -3.063327 -1.3415782  0.3933609
## 3  -0.6664055  2.0455375 -1.4416841 -4.701183 -1.7422020  0.4313114
## 4   3.7500055 -0.3915610 -0.6690260 -4.020753  1.7927777 -0.8542507
## 12  0.3768509 -2.1897554  1.4380167 -5.327116 -0.4066757  1.1092318
## 15  1.0644951 -1.4646516 -0.9900478 -5.627351 -0.8650174  0.1070749
## 16 -0.3798629  0.2173028  0.4387980 -2.069880 -1.9363920 -0.3696683
##            PC7        PC8       PC9        PC10        PC11       PC12
## 2  -1.31779481 -1.8965728 0.7111801  0.16193272  1.44061816 -0.6647078
## 3   1.28450248 -3.0829008 1.9973303  0.58665039  0.80080447  1.4480935
## 4  -0.07092724 -0.5997223 0.9873784 -0.47230884  1.22229470  1.1277348
## 12  0.70231874 -0.9667673 0.4970412 -0.10925035  1.59963522 -0.6665738
## 15  0.49640237 -0.6569112 0.4917103 -0.01652356  0.01154796 -0.7153114
## 16  0.01207458 -1.2991267 0.8748767 -0.28802701 -0.32026720  1.3544711
##           PC13       PC14         PC15         PC16         PC17
## 2  -0.50341167 -0.5251037  0.209541528  0.001408739  0.783699478
## 3   0.44875803 -0.4299460 -0.610433146 -1.058349329 -0.791062674
## 4  -1.37477652 -1.4884756 -0.712689953 -0.359746977 -0.002913506
## 12 -1.26751477 -0.2010528  0.135890457 -1.125603114 -0.025376331
## 15 -0.06833084 -0.2925842  0.003031988 -0.046983964 -0.678036827
## 16 -0.68148834  0.3984937 -1.948861159 -0.746533391  1.413093003
##           PC18        PC19
## 2  -0.55515083  0.68129112
## 3   0.06569274  0.14157724
## 4   1.35736326  0.10187098
## 12 -0.63185874 -0.96813663
## 15 -0.58461317  0.05084857
## 16 -1.96291699  0.73874900</code></pre>
<pre class="r"><code>nearZeroVar(segData)</code></pre>
<pre><code>## integer(0)</code></pre>
<pre class="r"><code>correlations &lt;- cor(segData)
correlations[1:4, 1:4]</code></pre>
<pre><code>##                 AngleCh1      AreaCh1 AvgIntenCh1 AvgIntenCh2
## AngleCh1     1.000000000 -0.002627172 -0.04300776 -0.01944681
## AreaCh1     -0.002627172  1.000000000 -0.02529739 -0.15330301
## AvgIntenCh1 -0.043007757 -0.025297394  1.00000000  0.52521711
## AvgIntenCh2 -0.019446810 -0.153303007  0.52521711  1.00000000</code></pre>
<pre class="r"><code>#corrplot(correlations, order = &quot;hclust&quot;)

highCorr &lt;- findCorrelation(correlations, cutoff = 0.75)
length(highCorr)</code></pre>
<pre><code>## [1] 32</code></pre>
<pre class="r"><code>head(highCorr)</code></pre>
<pre><code>## [1] 23 40 43 36  7 15</code></pre>
<pre class="r"><code>filteredSegData &lt;- segData[, -highCorr]</code></pre>
</div>
<div id="chapter-4" class="section level3">
<h3>Chapter 4</h3>
<ul>
<li><strong>apparent</strong> performance is using training set to predict model performance</li>
<li><strong>tuning parameters</strong> are model params that can’t be estimated with formulas (number of neighbors in a KNN classification model)</li>
</ul>
<p><strong>Data Splitting</strong></p>
<ul>
<li>when samples are not large, may need to avoid test set
<ul>
<li>test set needs to be sufficiently large to have power/precision</li>
</ul></li>
<li>stratified random sampling applies random sampling within groups to account for the outcome</li>
<li>maximum dissimilarity sampling splits based on predictors</li>
</ul>
<p><strong>Resampling Techniques</strong></p>
<ul>
<li>k-fold cross-validation
<ul>
<li>usually k is set to 5 or 10, but no formal rule</li>
<li>larger k has smaller bias (difference b/w estimated and true model performance), but more computationally demanding</li>
<li>k-fold CV generally has higher variance compared to other methods (problem for small datasets)</li>
<li>bias for small values of k (2 or 3) is about the same as the bootstrap, but with larger variance</li>
</ul></li>
<li>stratified random sampling applied to k-fold CV selects k partitions so that folds are balanced with respect to the outcome</li>
<li>leave-one-out cross-validation (LOOCV) can be thought of as a special case of k-fold where k equals to number of samples
<ul>
<li>has a closed form solution for linear regression models</li>
</ul></li>
<li>repeated training/test splits, also called leave-group-out CV or monte carlo CV
<ul>
<li>rule of thumb for group size is 75-80%</li>
<li>unlike k-fold CV, samples can be in multiple hold out sets, and repitions are usually larger (50-200)</li>
</ul></li>
<li>bootstrap samples data with replacement to create subsets that are the same size as total samples
<ul>
<li>samples that are not selected are the “out of bag” samples, used like a hold out test set</li>
<li>tend to have less uncertainty than k-fold CV</li>
<li>bias can be problematic for small datasets</li>
<li>632 method: 0.632 x bootstrap estimate + 0.368 x apparent error rate</li>
<li>63.2% of data points are represented at least once in the bootstrap sample</li>
<li>this method reduces bias, but can be unstable for smaller datasets</li>
<li>there’s also a 632+ method</li>
</ul></li>
</ul>
<p><strong>Choosing Tuning Parameters</strong></p>
<ul>
<li>in general, favor simpler models
<ul>
<li>choosing tuning params based on numically optimal value can still lead to overfitting</li>
</ul></li>
<li>one-standard error method starts with numerically optimal param and picks the simplest model within one sd</li>
<li>tolerance is (x-optimal)/optimal and can be used if deciding a certain % loss in performance is acceptable</li>
</ul>
<p><strong>Choosing Resampling Methods</strong></p>
<ul>
<li>if sample size is small, recommend 10-fold CV
<ul>
<li>good bias/variance properties</li>
<li>low computational costs</li>
</ul></li>
<li>if goal is model selection instead of best indicator of performance, recommend a bootstrap method
<ul>
<li>low variance</li>
</ul></li>
<li>if sample size is large, differences between resampling methods are smaller
<ul>
<li>computational efficiency is more important</li>
</ul></li>
</ul>
<p><strong>Choosing Between Models</strong></p>
<ol style="list-style-type: decimal">
<li>start with several models that are less interpretable and most flexible (boosted trees, SVM)</li>
<li>investigate simpler models that are less opaque (MARS, PLS, GAM, naive Bayes)</li>
<li>use the simplest model that reasonably approximates performance of complex models</li>
</ol>
<p>A <strong>paired t-test</strong> can be used to evaluate if there are statistically significant differences in model performance.</p>
<pre class="r"><code>rm(list=ls())
library(AppliedPredictiveModeling)
library(caret)
#library(Design)  # replaced with rms
library(rms)
library(e1071)
library(ipred)
library(MASS)

data(twoClassData)
str(predictors)</code></pre>
<pre><code>## &#39;data.frame&#39;:    208 obs. of  2 variables:
##  $ PredictorA: num  0.158 0.655 0.706 0.199 0.395 ...
##  $ PredictorB: num  0.1609 0.4918 0.6333 0.0881 0.4152 ...</code></pre>
<pre class="r"><code>str(classes)</code></pre>
<pre><code>##  Factor w/ 2 levels &quot;Class1&quot;,&quot;Class2&quot;: 2 2 2 2 2 2 2 2 2 2 ...</code></pre>
<pre class="r"><code># data splitting
set.seed(1)
trainingRows &lt;- createDataPartition(classes, p = 0.8, list = FALSE)
head(trainingRows)</code></pre>
<pre><code>##      Resample1
## [1,]         1
## [2,]         2
## [3,]         3
## [4,]         4
## [5,]         5
## [6,]         6</code></pre>
<pre class="r"><code>trainPredictors &lt;- predictors[trainingRows,]
trainClasses &lt;- classes[trainingRows]  # not a dataframe, just vector
testPredictors &lt;- predictors[-trainingRows,]
testClasses &lt;- classes[-trainingRows]

str(trainPredictors)</code></pre>
<pre><code>## &#39;data.frame&#39;:    167 obs. of  2 variables:
##  $ PredictorA: num  0.158 0.655 0.706 0.199 0.395 ...
##  $ PredictorB: num  0.1609 0.4918 0.6333 0.0881 0.4152 ...</code></pre>
<pre class="r"><code>str(testPredictors)</code></pre>
<pre><code>## &#39;data.frame&#39;:    41 obs. of  2 variables:
##  $ PredictorA: num  0.0658 0.1056 0.2909 0.4129 0.0472 ...
##  $ PredictorB: num  0.1786 0.0801 0.3021 0.2869 0.0414 ...</code></pre>
<pre class="r"><code># caret fn &#39;maxdissim&#39; generates test set using max dissimilarity sampling

# resampling
# - repeated training/test splits
set.seed(1)
repeatedSplits &lt;- createDataPartition(trainClasses, p = 0.8, times = 3)
str(repeatedSplits)</code></pre>
<pre><code>## List of 3
##  $ Resample1: int [1:135] 1 2 4 5 6 8 9 10 11 12 ...
##  $ Resample2: int [1:135] 2 3 4 6 7 8 9 11 14 15 ...
##  $ Resample3: int [1:135] 4 5 6 7 8 9 11 13 14 15 ...</code></pre>
<pre class="r"><code># - k-fold CV
set.seed(1)
cvSplits &lt;- createFolds(trainClasses, k = 10, returnTrain = TRUE)
str(cvSplits)</code></pre>
<pre><code>## List of 10
##  $ Fold01: int [1:151] 2 3 4 5 6 7 11 12 13 14 ...
##  $ Fold02: int [1:150] 1 2 3 4 5 6 7 8 9 10 ...
##  $ Fold03: int [1:150] 1 2 3 4 5 6 7 8 9 10 ...
##  $ Fold04: int [1:151] 1 2 3 4 5 7 8 9 10 11 ...
##  $ Fold05: int [1:150] 1 2 3 5 6 7 8 9 10 11 ...
##  $ Fold06: int [1:150] 1 2 3 4 5 6 8 9 10 11 ...
##  $ Fold07: int [1:150] 1 3 4 5 6 7 8 9 10 11 ...
##  $ Fold08: int [1:151] 1 2 3 4 5 6 7 8 9 10 ...
##  $ Fold09: int [1:150] 1 2 4 5 6 7 8 9 10 12 ...
##  $ Fold10: int [1:150] 1 2 3 4 6 7 8 9 10 11 ...</code></pre>
<pre class="r"><code>cvPredictors1 &lt;- trainPredictors[cvSplits[[1]],]
cvClasses1 &lt;- trainClasses[cvSplits[[1]]]
nrow(cvPredictors1)/nrow(trainPredictors)</code></pre>
<pre><code>## [1] 0.9041916</code></pre>
<pre class="r"><code># - bootstrapping: createResamples
# - repeated CV: createMultiFolds

# model building
# using formula interface w large data can by unnecessarily slow
# - using knn3 from caret
trainPredictors &lt;- as.matrix(trainPredictors)
knnFit &lt;- knn3(x = trainPredictors, y = trainClasses, k = 5)
knnFit</code></pre>
<pre><code>## 5-nearest neighbor model
## Training set outcome distribution:
## 
## Class1 Class2 
##     89     78</code></pre>
<pre class="r"><code>testPredictions &lt;- predict(knnFit, newdata = testPredictors, type = &quot;class&quot;)
head(testPredictions)</code></pre>
<pre><code>## [1] Class2 Class2 Class1 Class1 Class2 Class2
## Levels: Class1 Class2</code></pre>
<pre class="r"><code># determining tuning parameters
rm(list=ls())
data(&quot;GermanCredit&quot;)
set.seed(1056)
trainingRows &lt;- createDataPartition(GermanCredit$Class, p = 0.8, list = FALSE)
GermanCreditTrain &lt;- GermanCredit[trainingRows, -(1:9)]  # drop first 9 cols
dropZeroVar &lt;- nearZeroVar(GermanCreditTrain)
GermanCreditTrain &lt;- GermanCreditTrain[, -dropZeroVar]
GermanCreditTest &lt;- GermanCredit[-trainingRows, -(1:9)]  # drop first 9 cols
GermanCreditTest &lt;- GermanCreditTest[, -dropZeroVar]
svmFit &lt;- train(Class ~.,
                data = GermanCreditTrain,
                method = &quot;svmRadial&quot;,
                preProc = c(&quot;center&quot;, &quot;scale&quot;),
                tuneLength = 10,  # evals 2^(-2) to 2^7
                trControl = trainControl(method = &quot;repeatedcv&quot;, repeats = 5)) 
svmFit</code></pre>
<pre><code>## Support Vector Machines with Radial Basis Function Kernel 
## 
## 800 samples
##  41 predictor
##   2 classes: &#39;Bad&#39;, &#39;Good&#39; 
## 
## Pre-processing: centered (41), scaled (41) 
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 720, 720, 720, 720, 720, 720, ... 
## Resampling results across tuning parameters:
## 
##   C       Accuracy  Kappa    
##     0.25  0.70000   0.0000000
##     0.50  0.71325   0.0978653
##     1.00  0.74350   0.2819257
##     2.00  0.75875   0.3601013
##     4.00  0.75550   0.3685613
##     8.00  0.73575   0.3395050
##    16.00  0.71375   0.2980955
##    32.00  0.69175   0.2562410
##    64.00  0.67950   0.2313896
##   128.00  0.67225   0.2150734
## 
## Tuning parameter &#39;sigma&#39; was held constant at a value of 0.0141941
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were sigma = 0.0141941 and C = 2.</code></pre>
<pre class="r"><code>plot(svmFit, scales = list(x = list(log = 2)))</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-91-1.png" width="672" /></p>
<pre class="r"><code>#predictedProbs &lt;- predict(svmFit, newdata = GermanCreditTest, type = &quot;prob&quot;)  # see errata

# between model comparison
set.seed(1056)
logisticReg &lt;- train(Class ~.,
                     data = GermanCreditTrain,
                     method = &quot;glm&quot;,
                     trControl = trainControl(method = &quot;repeatedcv&quot;, repeats =5))
logisticReg  # need to remove extra cols for one hot encoding? not going to bother for notes</code></pre>
<pre><code>## Generalized Linear Model 
## 
## 800 samples
##  41 predictor
##   2 classes: &#39;Bad&#39;, &#39;Good&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 720, 720, 720, 720, 720, 720, ... 
## Resampling results:
## 
##   Accuracy  Kappa    
##   0.73025   0.3064704</code></pre>
<pre class="r"><code>resamp &lt;- resamples(list(SVM = svmFit, Logistic = logisticReg))
summary(resamp)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = resamp)
## 
## Models: SVM, Logistic 
## Number of resamples: 50 
## 
## Accuracy 
##           Min.  1st Qu. Median    Mean  3rd Qu.  Max. NA&#39;s
## SVM      0.675 0.728125 0.7625 0.75875 0.787500 0.825    0
## Logistic 0.625 0.700000 0.7250 0.73025 0.759375 0.800    0
## 
## Kappa 
##               Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## SVM      0.1666667 0.2843344 0.3457102 0.3601013 0.4594595 0.5512821    0
## Logistic 0.0625000 0.2500000 0.3002137 0.3064704 0.3845504 0.5121951    0</code></pre>
<pre class="r"><code>?xyplot.resamples
modelDifferences &lt;- diff(resamp)
summary(modelDifferences)</code></pre>
<pre><code>## 
## Call:
## summary.diff.resamples(object = modelDifferences)
## 
## p-value adjustment: bonferroni 
## Upper diagonal: estimates of the difference
## Lower diagonal: p-value for H0: difference = 0
## 
## Accuracy 
##          SVM      Logistic
## SVM               0.0285  
## Logistic 0.001231         
## 
## Kappa 
##          SVM     Logistic
## SVM              0.05363 
## Logistic 0.02288</code></pre>
<pre class="r"><code># small values for SVM mean small p vals so possible that models are really different</code></pre>
</div>
</div>
<div id="regression-models" class="section level2">
<h2>Regression Models</h2>
<div id="chapter-5" class="section level3">
<h3>Chapter 5</h3>
</div>
<div id="chapter-6" class="section level3">
<h3>Chapter 6</h3>
</div>
<div id="chapter-7" class="section level3">
<h3>Chapter 7</h3>
</div>
<div id="chapter-8" class="section level3">
<h3>Chapter 8</h3>
</div>
<div id="chapter-9" class="section level3">
<h3>Chapter 9</h3>
</div>
<div id="chapter-10" class="section level3">
<h3>Chapter 10</h3>
</div>
</div>
<div id="classification-models" class="section level2">
<h2>Classification Models</h2>
<div id="chapter-11" class="section level3">
<h3>Chapter 11</h3>
</div>
<div id="chapter-12" class="section level3">
<h3>Chapter 12</h3>
</div>
<div id="chapter-13" class="section level3">
<h3>Chapter 13</h3>
</div>
<div id="chapter-14" class="section level3">
<h3>Chapter 14</h3>
</div>
<div id="chapter-15" class="section level3">
<h3>Chapter 15</h3>
</div>
<div id="chapter-16" class="section level3">
<h3>Chapter 16</h3>
</div>
<div id="chapter-17" class="section level3">
<h3>Chapter 17</h3>
</div>
</div>
<div id="other-considerations" class="section level2">
<h2>Other Considerations</h2>
<div id="chapter-18" class="section level3">
<h3>Chapter 18</h3>
</div>
<div id="chapter-19" class="section level3">
<h3>Chapter 19</h3>
</div>
<div id="chapter-20" class="section level3">
<h3>Chapter 20</h3>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
