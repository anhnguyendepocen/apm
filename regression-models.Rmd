---
title: "APM Computation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

## Regression Models

### Chapter 5: Measuring Performance in Regression Models

```{r ch5, message=FALSE, warning=FALSE}
rm(list=ls())
observed <-  c(0.22, 0.83,-0.12, 0.89,-0.23,-1.30,-0.15,-1.4,
               0.62, 0.99,-0.18, 0.32, 0.34,-0.30, 0.04,-0.87,
               0.55,-1.30,-1.15, 0.20)
predicted <- c(0.24, 0.78,-0.66, 0.53, 0.70,-0.75,-0.41,-0.43,
               0.49, 0.79,-1.19, 0.06, 0.75,-0.07, 0.43,-0.42,
              -0.25,-0.64,-1.26,-0.07)
residualValues <- observed - predicted
summary(residualValues)

# observed versus predicted
axisRange <- extendrange(c(observed, predicted))
plot(observed, predicted, ylim = axisRange, xlim = axisRange)
abline(0, 1, col = "darkgrey", lty = 2)

# predicted versus residuals
plot(predicted, residualValues, ylab = "residual")
abline(h = 0, col = "darkgrey", lty = 2)

# quantitative model performance measures
library(caret)
R2(predicted, observed)
RMSE(predicted, observed)
cor(predicted, observed)  # base R simple correlation
cor(predicted, observed)^2  # match R^2
cor(predicted, observed, method = "spearman")  # rank correlation
```


### Chapter 6: Linear Regression and Its Cousins

```{r ch6, message=FALSE, warning=FALSE}
rm(list = ls())
require(AppliedPredictiveModeling)
require(elasticnet)
require(lars)
require(caret)
require(MASS)
require(pls)
require(stats)
data(solubility)
ls(pattern = "^sol")  # obj beginning w "sol"
trainingData <- solTrainXtrans
trainingData$Solubility <- solTrainY

### Ordinary Linear Regression ###

lmFitAllPredictors <- lm(Solubility ~., data = trainingData)
str(summary(lmFitAllPredictors))  # training results
lmPred1 <- predict(lmFitAllPredictors, solTestXtrans)
head(lmPred1)
lmValues1 <- data.frame(obs = solTestY, pred = lmPred1)
defaultSummary(lmValues1)  # caret metrics

# using robust lm from MASS (uses Huber approach)
ctrl <- trainControl(method = "cv", number = 10)
set.seed(100)  
lmFit1 <- train(x = solTrainXtrans, y = solTrainY,
                method = "lm", trControl = ctrl)
lmFit1
xyplot(solTrainY ~ predict(lmFit1),
       # plot points and use background grid
       type = c("p", "g"),
       xlab = "Predicted", ylab = "Observed")
xyplot(resid(lmFit1) ~ predict(lmFit1),
       type = c("p", "g"),
       xlab = "Predicted", ylab = "Residuals")

# using train function
corThresh <- 0.9
tooHigh <- findCorrelation(cor(solTrainXtrans), corThresh)
corrPred <- names(solTrainXtrans)[tooHigh]
trainXfiltered <- solTrainXtrans[, -tooHigh]
testXfiltered <- solTestXtrans[, -tooHigh]
set.seed(100)
lmFiltered <- train(trainXfiltered, solTrainY, method = "lm", trControl = ctrl)
lmFiltered
rlmPCA <- train(trainXfiltered, solTrainY, method = "rlm", preProcess = "pca", trControl = ctrl)
rlmPCA

### Partial Least Squares ###
plsFit <- plsr(Solubility ~., data = trainingData)
predict(plsFit, solTestXtrans[1:5,], ncomp = 1:2)

# using train function
plsTune <- train(solTrainXtrans, solTrainY,
                 method = "pls", 
                 tuneLength = 20,  # default tuning grid evals 1:tuneLength
                 trControl = ctrl,
                 preProcess = c("center", "scale"))
plsTune

### Penalized Regression Models ###

# ridge; lm.ridge from MASS or enet from elasticnet
ridgeModel <- enet(x = as.matrix(solTrainXtrans), y = solTrainY, 
                   lambda = 0.001)  # this is ridge penalty
ridgePred <- predict(ridgeModel, newx = as.matrix(solTestXtrans),
                     s = 1, mode = "fraction",  # s=1 is full solution. lasso lamba=0 so this is ridge regression
                     type = "fit")
head(ridgePred$fit)

# defining tuning grid
ridgeGrid <- data.frame(.lambda = seq(0, 0.1, length = 15))
set.seed(100)
ridgeRegFit <- train(solTrainXtrans, solTrainY,
                     method = "ridge",
                     tuneGrid = ridgeGrid,
                     trControl = ctrl,
                     preProcess = c("center", "scale"))
ridgeRegFit

# lasso: lars from lars or enet from elasticnet or glmnet
enetModel <- enet(x = as.matrix(solTrainXtrans), y = solTrainY,
                  lambda = 0.01, normalize = TRUE)  # normalize does center and scale
enetPred <- predict(enetModel, newx = as.matrix(solTestXtrans),
                    s = 0.1, mode = "fraction",
                    type = "fit")
names(enetPred)
head(enetPred$fit)
enetCoef <- predict(enetModel, newx = as.matrix(solTestXtrans),
                    s = 0.1, mode = "fraction",
                    type = "coefficients")

tail(enetCoef$coefficients)

# using train function for lasso
enetGrid <- expand.grid(.lambda = c(0, 0.01, 0.1),
                        .fraction = seq(0.05, 1, length = 20))
set.seed(100)
enetTune <- train(solTrainXtrans, solTrainY,
                  method = "enet",
                  tuneGrid = enetGrid,
                  trControl = ctrl,
                  preProcess = c("center", "scale"))
plot(enetTune)
```

### Chapter 7: Nonlinear Regression Models

```{r ch7, message=FALSE, warning=FALSE}
rm(list = ls())
require(AppliedPredictiveModeling)
require(caret)
require(earth)
require(kernlab)
require(nnet)

### Neural Networks ###

# using nnet
# nnetFit <- nnet(predictors, outcome, 
#                 size = 5,
#                 decay = 0.01,
#                 linout = TRUE,
#                 trace = FALSE, 
#                 maxit = 500,
#                 MaxNwts = 5 * (ncol(predictors) + 1) + 5 + 1)
# nnetAvg <- avNNet(predictors, outcome, 
#                 size = 5,
#                 decay = 0.01,
#                 linout = TRUE,
#                 trace = FALSE, 
#                 maxit = 500,
#                 MaxNwts = 5 * (ncol(predictors) + 1) + 5 + 1)
# predict(nnetFit, newData)
# predict(nnetAvg, newData)

# using train function, method = "nnet" or method = "avNNet"
data(solubility)
tooHigh <- findCorrelation(cor(solTrainXtrans), cutoff = 0.75)
trainXnnet <- solTrainXtrans[, -tooHigh]
testXnnet <- solTestXtrans[, -tooHigh]
nnetGrid <- expand.grid(.decay = c(0, 0.01, 0.1),
                        .size = c(1:10),
                        .bag = FALSE)
ctrl <- trainControl(method = "cv", number = 10)
set.seed(100)
# nnetTune <- train(trainXnnet, solTrainY,  # takes a long time to run
#                   method = "avNNet",
#                   tuneGrid = nnetGrid,
#                   trControl = ctrl,
#                   preProcess = c("center", "scale"),
#                   linout = TRUE,
#                   trace = FALSE,
#                   MaxNWts = 10 * (ncol(trainXnnet) + 1) + 10 + 1,
#                   maxit = 500)

### Multivariate Adaptive Regression Splines ###

# using earth package
marsFit <- earth(solTrainXtrans, solTrainY)
marsFit
summary(marsFit)  # more details
plotmo(marsFit)
evimp(marsFit)

# using train function
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)
set.seed(100)
# marsTuned <- train(solTrainXtrans, solTrainY,  # also takes a long time to run
#                    method = "earth",
#                    tuneGrid = marsGrid,
#                    trControl = trainControl(method = "cv"))
# marsSTuned
# head(predict(marsTuned, solTestXtrans))
# varImp(marsTuned)

### Support Vector Machines ###

# using kernlab
trainingData <- solTrainXtrans
trainingData$Solubility <- solTrainY
svmFit <- ksvm(Solubility ~., data = trainingData,
               kernel ="rbfdot", kpar = "automatic",
               C = 1, epsilon = 0.1)
svmFit

# using train function
# svmRTuned <- train(solTrainXtrans, solTrainY,
#                    method = "svmRadial",
#                    preProcess = c("center", "scale"),
#                    tuneLength = 14,  # 2^-2 to 2^11 cost values
#                    trControl = trainControl(method = "cv"))
# svmRTuned
# svmRTuned$finalModel  

### K-Nearest Neighbors ###

# using the train function from caret
knnDescr <- solTrainXtrans[, -nearZeroVar(solTrainXtrans)]  # remove sparse and unbalanced fingerprints
set.seed(100)
knnTune <- train(knnDescr,
                 solTrainY,
                 method = "knn",
                 preProcess = c("center", "scale"),
                 tuneGrid = data.frame(.k = 1:20),
                 trControl = trainControl(method = "cv"))
# new data will be auto centered and scaled when using knn model object for prediction
knnTune
```

### Chapter 8: Regression Trees and Rule-Based Models

```{r, message=FALSE, warning=FALSE}
rm(list = ls())
library(caret)
library(Cubist)
library(gbm)
library(ipred)
library(party)
library(partykit)
library(randomForest)
library(rpart)
library(RWeka)
library(AppliedPredictiveModeling)
data(solubility)
trainingData <- solTrainXtrans
trainingData$Solubility <- solTrainY

### Single Trees ###

# rpart uses CART
set.seed(100)
rpartTune <- train(solTrainXtrans, solTrainY,
                   method = "rpart2",  # use rpart to tune over complexity parameter, rpart2 tune over maxdepth
                   tuneLength = 10,
                   trControl = trainControl(method= "cv"))
plot(rpartTune)

# ctree in party uses conditional inference
# use ctree_control to tune parameters

### Model Trees ###

# use method = "M5" in caret or RWeka
# set.seed(100)
# m5Tune <- train(solTrainXtrans, solTrainY,
#                 method = "M5",
#                 trControl = trainControl(method = "cv"),
#                 control =  Weka_control(M = 10))
# plot(m5Tune)

### Bagged Trees ###

# ipred: bagging, ipredbagg
# RWeka: Baggin
# caret: method = bag
# party: cforest set mtry equal to total number of predictors

### Random Forest ###

# randomForest
# can't use with missing data
rfModel <- randomForest(solTrainXtrans, solTrainY,
                        importance = TRUE,
                        ntrees = 1000)
head(varImp(rfModel))  # caret wrapper for variable importance

### Boosted Trees ###

# gbm 
gbmModel <- gbm.fit(solTrainXtrans, solTrainY, distribution = "gaussian")
# use gaussian distribution for continuous response

# tuning with caret
# gbmGrid <- expand.grid(.interaction.depth = seq(1, 7, by = 2),
#                        .n.trees = seq(100, 1000, by = 50),
#                        .shrinkage = c(0.01, 0.1))  # need to add n.minobsinnode
# set.seed(100)
# gbmTune <- train(solTrainXtrans, solTrainY,
#                  method = "gbm",
#                  tuneGrid = gbmGrid,
#                  verbose = FALSE)

### Cubist ###

cubistMod <- cubist(solTrainXtrans, solTrainY)
# predict(cubistMod, solTestXtrans)  # use neighbors agument to pass 0-9 integer to adjust predictions
summary(cubistMod)
```


### Chapter 10: Case Study: Compressive Strength of Concrete Mixtures

```{r, message=FALSE, warning=FALSE}
rm(list = ls())
library(AppliedPredictiveModeling)
data(concrete)
str(concrete)
str(mixtures)

# feature plot from caret
library(caret)
featurePlot(x = concrete[,-9],
            y = concrete[,9],
            between = list(x=1, y=1),  # space between panels
            type = c("g", "p", "smooth"))  # add background grid and smoother

# calc average for replicated mixtures
library(plyr)
averaged <- ddply(mixtures,
                  .(Cement, BlastFurnaceSlag, FlyAsh, Water,
                    Superplasticizer, CoarseAggregate,
                    FineAggregate, Age),
                  function(x) c(CompressiveStrength =
                                  mean(x$CompressiveStrength)))
head(averaged)

# split data into training and test
set.seed(975)
forTraining <- createDataPartition(averaged$CompressiveStrength,
                                   p = 3/4)[[1]]
trainingSet <- averaged[forTraining,]
testSet <- averaged[-forTraining,]

# (.)^2 takes all linear terms and all two-factor interactions
modFormula <- paste("CompressiveStrength ~ (.)^2 + I(Cement^2) + ",
                    "I(BlastFurnaceSlag^2) + I(FlyAsh^2) + I(Water^2) +",
                    "I(Superplasticizer^2) + I(Age^2)")
modFormula <- as.formula(modFormula)
controlObject <- trainControl(method = "repeatedcv",
                              repeats = 5,
                              number = 10)

set.seed(669)
linearReg <- train(modFormula,
                   data = trainingSet,
                   method = "lm",
                   trControl = controlObject)
linearReg

set.seed(669)
plsModel <- train(modFormula, data = trainingSet,
                  method = "pls",
                  preProc = c("center", "scale"),
                  tuneLength = 15,
                  trControl = controlObject)
plsModel

enetGrid <- expand.grid(.lambda = c(0, 0.001, 0.01, 0.1),
                        .fraction = seq(0.05, 1, length = 20))
set.seed(669)
enetModel <- train(modFormula, data = trainingSet,
                   method = "enet",
                   preProc = c("center", "scale"),
                   tuneGrid = enetGrid,
                   trControl = controlObject)

allResamples <- resamples(list("Linear Reg" = linearReg,
                               "PLS" = plsModel,
                               "Elastic Net" = enetModel))
parallelplot(allResamples)
parallelplot(allResamples, metric = "Rsquared")
```

